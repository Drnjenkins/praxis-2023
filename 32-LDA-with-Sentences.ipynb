{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19d7a77d-2710-4113-a4e6-95f0fe329bb0",
   "metadata": {},
   "source": [
    "# LDA with Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0291927f-b2be-48e1-aef7-7fd7855bde8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add coherence matrix (k by score: k = 1, Coherence score: 0.8242367870330447)\n",
    "# And a graph of k by score (see 30-NMF-v2.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06244427-b4c1-4d71-b458-0dc66b1dba92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import nltk # If this step fails, rerun 07-Install-NLTK.ipynb\n",
    "import string\n",
    "\n",
    "# Identify the working directory and data files\n",
    "working_directory = './32-LDA-with-Sentences'\n",
    "\n",
    "# Create the working directory if needed\n",
    "try:\n",
    "    os.makedirs(working_directory, exist_ok=True)\n",
    "except OSError as error:\n",
    "    print(f\"Error creating {working_directory}: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd9f6b86-c525-4894-9cdb-1e1f6ac49d7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Topics range\n",
    "min_topics = 1\n",
    "max_topics = 25\n",
    "\n",
    "# Number of training passes for LDA\n",
    "passes = 10\n",
    "\n",
    "# The number of top words per topic\n",
    "num_top_words = 10\n",
    "\n",
    "# A row to use for verification of processing\n",
    "verification_row = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "181ae050-e89d-495e-a868-af9323d48a31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet --upgrade nltk gensim spacy pyldavis sentence-transformers hdbscan mpld3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96bb5b79-b64b-43b4-8664-272dd9f6a0fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "!python -m nltk.downloader --quiet 'all'\n",
    "!python -m spacy download en_core_web_sm > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8a0828c-e6c6-47e2-95d8-65d25ddf699a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the preprocessed data into a dataframe\n",
    "import pickle\n",
    "\n",
    "with open('./21-Preprocess-Combined-Data-v2/dataframe.pickle', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "699caf8a-3dba-41a2-b639-ec5e74a31012",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5736, 20)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "526595ac-b49e-4cc7-b5e4-e85a32aa6c90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>FOI_TEXT</th>\n",
       "      <th>DEVICE_PROBLEM_CODE</th>\n",
       "      <th>DEVICE_PROBLEM_TEXT</th>\n",
       "      <th>GENERIC_NAME</th>\n",
       "      <th>DEVICE_REPORT_PRODUCT_CODE</th>\n",
       "      <th>UDI-DI</th>\n",
       "      <th>UDI-PUBLIC</th>\n",
       "      <th>DATE_OF_EVENT</th>\n",
       "      <th>REPORTER_OCCUPATION_CODE</th>\n",
       "      <th>REPORT_DATE</th>\n",
       "      <th>EVENT_LOCATION</th>\n",
       "      <th>SOURCE_TYPE</th>\n",
       "      <th>TOKENIZED_TEXT</th>\n",
       "      <th>NOPUNCT_TEXT</th>\n",
       "      <th>NOSTOPWORDS_TEXT</th>\n",
       "      <th>NODIGITS_TEXT</th>\n",
       "      <th>POS_TEXT</th>\n",
       "      <th>LEMMATIZED_TEXT</th>\n",
       "      <th>STEMMED_TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1969025</td>\n",
       "      <td>IT WAS REPORTED THAT THE TRANSMITTER LOST CONN...</td>\n",
       "      <td>3283</td>\n",
       "      <td>Wireless Communication Problem</td>\n",
       "      <td>CONTINUOUS GLUCOSE MONITOR</td>\n",
       "      <td>QBJ</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>07/30/2020</td>\n",
       "      <td>000</td>\n",
       "      <td></td>\n",
       "      <td>I</td>\n",
       "      <td>CONSUMER</td>\n",
       "      <td>[it, was, reported, that, the, transmitter, lo...</td>\n",
       "      <td>[it, was, reported, that, the, transmitter, lo...</td>\n",
       "      <td>[reported, transmitter, lost, connection, pump...</td>\n",
       "      <td>[reported, transmitter, lost, connection, pump...</td>\n",
       "      <td>[(reported, VBN), (transmitter, NN), (lost, VB...</td>\n",
       "      <td>[report, transmitter, lose, connection, pump, ...</td>\n",
       "      <td>[report, transmitt, lost, connect, pump, great...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ROW_ID                                           FOI_TEXT  \\\n",
       "0  1969025  IT WAS REPORTED THAT THE TRANSMITTER LOST CONN...   \n",
       "\n",
       "  DEVICE_PROBLEM_CODE             DEVICE_PROBLEM_TEXT  \\\n",
       "0                3283  Wireless Communication Problem   \n",
       "\n",
       "                 GENERIC_NAME DEVICE_REPORT_PRODUCT_CODE UDI-DI UDI-PUBLIC  \\\n",
       "0  CONTINUOUS GLUCOSE MONITOR                        QBJ                     \n",
       "\n",
       "  DATE_OF_EVENT REPORTER_OCCUPATION_CODE REPORT_DATE EVENT_LOCATION  \\\n",
       "0    07/30/2020                      000                          I   \n",
       "\n",
       "  SOURCE_TYPE                                     TOKENIZED_TEXT  \\\n",
       "0    CONSUMER  [it, was, reported, that, the, transmitter, lo...   \n",
       "\n",
       "                                        NOPUNCT_TEXT  \\\n",
       "0  [it, was, reported, that, the, transmitter, lo...   \n",
       "\n",
       "                                    NOSTOPWORDS_TEXT  \\\n",
       "0  [reported, transmitter, lost, connection, pump...   \n",
       "\n",
       "                                       NODIGITS_TEXT  \\\n",
       "0  [reported, transmitter, lost, connection, pump...   \n",
       "\n",
       "                                            POS_TEXT  \\\n",
       "0  [(reported, VBN), (transmitter, NN), (lost, VB...   \n",
       "\n",
       "                                     LEMMATIZED_TEXT  \\\n",
       "0  [report, transmitter, lose, connection, pump, ...   \n",
       "\n",
       "                                        STEMMED_TEXT  \n",
       "0  [report, transmitt, lost, connect, pump, great...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d449a7a0-c954-4199-aa5b-b964ca534ccb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a DataFrame for the sentences\n",
    "sentences_df = pd.DataFrame(\n",
    "    columns=[\n",
    "        'SENTENCE', \n",
    "        'ROW_ID', \n",
    "        'FOI_TEXT', \n",
    "        'DEVICE_PROBLEM_CODE',\n",
    "        'DEVICE_PROBLEM_TEXT'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Iterate over every row in the FOI_TEXT DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    for sentence in nltk.sent_tokenize(row['FOI_TEXT']):\n",
    "        sentence_data = {\n",
    "            'SENTENCE': sentence, \n",
    "            'ROW_ID': row['ROW_ID'], \n",
    "            'FOI_TEXT': row['FOI_TEXT'], \n",
    "            'DEVICE_PROBLEM_CODE': row['DEVICE_PROBLEM_CODE'],\n",
    "            'DEVICE_PROBLEM_TEXT': row['DEVICE_PROBLEM_TEXT']\n",
    "        }\n",
    "        \n",
    "        sentences_df = sentences_df.append(sentence_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d91153a-53ba-4892-bbf4-8fd4ca134790",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25686, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "896d8c8b-e4d0-430c-8fa1-5bb9be1b8902",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SENTENCE</th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>FOI_TEXT</th>\n",
       "      <th>DEVICE_PROBLEM_CODE</th>\n",
       "      <th>DEVICE_PROBLEM_TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IT WAS REPORTED THAT THE TRANSMITTER LOST CONN...</td>\n",
       "      <td>1969025</td>\n",
       "      <td>IT WAS REPORTED THAT THE TRANSMITTER LOST CONN...</td>\n",
       "      <td>3283</td>\n",
       "      <td>Wireless Communication Problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>THE TRANSMITTER ULTIMATELY REGAINED CONNECTION...</td>\n",
       "      <td>1969025</td>\n",
       "      <td>IT WAS REPORTED THAT THE TRANSMITTER LOST CONN...</td>\n",
       "      <td>3283</td>\n",
       "      <td>Wireless Communication Problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NO ADDITIONAL PATIENT OR EVENT INFORMATION WAS...</td>\n",
       "      <td>1969025</td>\n",
       "      <td>IT WAS REPORTED THAT THE TRANSMITTER LOST CONN...</td>\n",
       "      <td>3283</td>\n",
       "      <td>Wireless Communication Problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IT WAS REPORTED THAT SIGNAL LOSS OVER ONE HOUR...</td>\n",
       "      <td>1426265</td>\n",
       "      <td>IT WAS REPORTED THAT SIGNAL LOSS OVER ONE HOUR...</td>\n",
       "      <td>3283</td>\n",
       "      <td>Wireless Communication Problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NO PRODUCT OR DATA WAS PROVIDED FOR EVALUATION.</td>\n",
       "      <td>1426265</td>\n",
       "      <td>IT WAS REPORTED THAT SIGNAL LOSS OVER ONE HOUR...</td>\n",
       "      <td>3283</td>\n",
       "      <td>Wireless Communication Problem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            SENTENCE   ROW_ID  \\\n",
       "0  IT WAS REPORTED THAT THE TRANSMITTER LOST CONN...  1969025   \n",
       "1  THE TRANSMITTER ULTIMATELY REGAINED CONNECTION...  1969025   \n",
       "2  NO ADDITIONAL PATIENT OR EVENT INFORMATION WAS...  1969025   \n",
       "3  IT WAS REPORTED THAT SIGNAL LOSS OVER ONE HOUR...  1426265   \n",
       "4    NO PRODUCT OR DATA WAS PROVIDED FOR EVALUATION.  1426265   \n",
       "\n",
       "                                            FOI_TEXT DEVICE_PROBLEM_CODE  \\\n",
       "0  IT WAS REPORTED THAT THE TRANSMITTER LOST CONN...                3283   \n",
       "1  IT WAS REPORTED THAT THE TRANSMITTER LOST CONN...                3283   \n",
       "2  IT WAS REPORTED THAT THE TRANSMITTER LOST CONN...                3283   \n",
       "3  IT WAS REPORTED THAT SIGNAL LOSS OVER ONE HOUR...                3283   \n",
       "4  IT WAS REPORTED THAT SIGNAL LOSS OVER ONE HOUR...                3283   \n",
       "\n",
       "              DEVICE_PROBLEM_TEXT  \n",
       "0  Wireless Communication Problem  \n",
       "1  Wireless Communication Problem  \n",
       "2  Wireless Communication Problem  \n",
       "3  Wireless Communication Problem  \n",
       "4  Wireless Communication Problem  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddb9f03e-d3d5-4535-a371-ea69d7c255c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Process the sentences\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "punctuations = set(string.punctuation)\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    # Lowercase the sentence\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    # Remove any words that start with a digit\n",
    "    sentence = re.sub(r'\\b\\d\\w*\\b', '', sentence)\n",
    "\n",
    "    # Remove punctuation\n",
    "    sentence_tokens = sentence.split()\n",
    "    sentence_tokens = [token.translate(str.maketrans(\"\", \"\", string.punctuation)) for token in sentence_tokens]\n",
    "\n",
    "    # Remove stopwords\n",
    "    sentence_tokens = [token for token in sentence_tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatize\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, pos='v') for token in sentence_tokens]\n",
    "    \n",
    "    # Rebuild the sentence\n",
    "    sentence = ' '.join(lemmatized_tokens)\n",
    "\n",
    "    return sentence\n",
    "\n",
    "sentences_df['PROCESSED_SENTENCE'] = sentences_df['SENTENCE'].apply(process_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9d8c28e-0557-4f3a-a83d-1780c1ca18d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'THE PROBABLE CAUSE COULD NOT BE DETERMINED.' ==> 'probable cause could determine'\n"
     ]
    }
   ],
   "source": [
    "print(f\"'{sentences_df['SENTENCE'][verification_row]}' ==> '{sentences_df['PROCESSED_SENTENCE'][verification_row]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b452660-efa3-40d4-9b55-13b02763277a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1\n",
      "k = 2\n",
      "k = 3\n",
      "k = 4\n",
      "k = 5\n",
      "k = 6\n",
      "k = 7\n",
      "k = 8\n",
      "k = 9\n",
      "k = 10\n",
      "k = 11\n",
      "k = 12\n",
      "k = 13\n",
      "k = 14\n",
      "k = 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/gensim/topic_coherence/direct_confirmation_measure.py:204: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  m_lr_i = np.log(numerator / denominator)\n",
      "/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/gensim/topic_coherence/indirect_confirmation_measure.py:323: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cv1.T.dot(cv2)[0, 0] / (_magnitude(cv1) * _magnitude(cv2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 16\n",
      "k = 17\n",
      "k = 18\n",
      "k = 19\n",
      "k = 20\n",
      "k = 21\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "# split each sentence into a list of words\n",
    "texts = sentences_df['PROCESSED_SENTENCE'].apply(lambda x: nltk.word_tokenize(x)).tolist()\n",
    "\n",
    "# Create a dictionary from the words\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Create a bag-of-words representation of the corpus\n",
    "corpus = [dictionary.doc2bow(word) for word in texts]\n",
    "\n",
    "# Initialize the scoreboard\n",
    "coherence_scores_df = pd.DataFrame(columns=['k', 'Coherence Score'])\n",
    "\n",
    "pbar = tqdm.tqdm(total=len(topics_range))\n",
    "\n",
    "for k in range(min_topics, max_topics + 1):\n",
    "    \n",
    "    # Train the LDA model\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(\n",
    "        corpus=corpus, \n",
    "        id2word=dictionary, \n",
    "        num_topics=k, \n",
    "        passes=passes\n",
    "    )\n",
    "\n",
    "    coherence_model = CoherenceModel(\n",
    "        model=lda_model, \n",
    "        texts=texts, \n",
    "        dictionary=dictionary, \n",
    "        coherence='c_v'\n",
    "    )\n",
    "\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "    coherence_scores_df = coherence_scores_df.append(\n",
    "        pd.Series(\n",
    "            [k, coherence_score], \n",
    "            index=coherence_scores_df.columns\n",
    "        ), \n",
    "        ignore_index=True\n",
    "    )\n",
    "    \n",
    "    pbar.update(1)\n",
    "\n",
    "pbar.close() \n",
    "coherence_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa9086a-42b8-4d51-9908-4758c038d881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the coherence_scores_df code to a file as HTML\n",
    "with open(f\"{working_directory}/lda_using_sentences_coherence_scores_table.html\", 'w') as f:\n",
    "    f.write(coherence_scores_df.to_html(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719cd33f-0e83-40f7-b01b-07949c16e937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig_1 = px.line(\n",
    "    coherence_scores_df, \n",
    "    x='k', \n",
    "    y='Coherence Score', \n",
    "    title=f\"NMF using Sentences - Coherence Score | {working_directory}\\n\"\n",
    ")\n",
    "\n",
    "# add markers for each point\n",
    "fig_1.update_traces(mode='lines+markers')\n",
    "\n",
    "# extend the limits of the x-axis from 0 to 16\n",
    "fig_1.update_xaxes(range=[0, max_topics + 1])\n",
    "\n",
    "# show all numbers on the x-axis\n",
    "fig_1.update_layout(xaxis=dict(tickmode='linear'))\n",
    "\n",
    "# write the graph to a file in the working directory\n",
    "fig_1.write_html(f\"{working_directory}/nmf_using_sentences_coherence_scores_chart.html\")\n",
    "\n",
    "# show the plot\n",
    "fig_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0d223b-3b1a-4638-960b-bd7c8c60eb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the highest score\n",
    "\n",
    "# Sort the dataframe by Coherence Score in descending order\n",
    "sorted_df = coherence_scores_df.sort_values(by='Coherence Score', ascending=False)\n",
    "\n",
    "# Pick the value of k from the first row of the sorted dataframe\n",
    "selected_k = sorted_df.iloc[0]['k']\n",
    "\n",
    "print(f\"Highest Coherence Score: {selected_k} = {sorted_df.iloc[0]['Coherence Score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d03946-4548-426f-9368-9b24088ef145",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = pd.DataFrame(columns=['Topic', f\"Top {num_top_words} Words\"])\n",
    "\n",
    "# Create an LDA model using the selected k\n",
    "lda_model = gensim.models.ldamodel.LdaModel(\n",
    "    corpus=corpus, \n",
    "    id2word=dictionary,\n",
    "    num_topics=selected_k, \n",
    "    passes=passes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d5456c7-40f8-422a-90cd-5276dc52de88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: 31-NMF-with-Sentences/nmf_using_sentences_coherence_scores_table.html to s3://praxis-2023-html-output/31-NMF-with-Sentences/nmf_using_sentences_coherence_scores_table.html\n",
      "upload: 31-NMF-with-Sentences/nmf_using_sentences_topics_and_words_table.html to s3://praxis-2023-html-output/31-NMF-with-Sentences/nmf_using_sentences_topics_and_words_table.html\n",
      "upload: ./index.html to s3://praxis-2023-html-output/index.html\n",
      "\n",
      "fin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import subprocess\n",
    "\n",
    "bucket = \"praxis-2023-html-output\"\n",
    "website = f\"http://{bucket}.s3-website-us-west-2.amazonaws.com\"\n",
    "\n",
    "# Use the fnmatch module to find all files in the current directory that end in \".html\"\n",
    "file_list = []\n",
    "for root, dirnames, filenames in os.walk(\".\"):\n",
    "    for filename in fnmatch.filter(filenames, '*.html'):\n",
    "        file_list.append(os.path.join(root, filename))\n",
    "\n",
    "# Sort the file list alphabetically\n",
    "file_list.sort()\n",
    "\n",
    "# Create the HTML file and write the header\n",
    "with open(os.path.join(\".\", 'index.html'), 'w') as f:\n",
    "    f.write('''<html>\n",
    "        <head>\n",
    "            <title>Praxis 2023 HTML Output</title>\n",
    "            <style>\n",
    "                table {\n",
    "                    border-collapse: collapse;\n",
    "                    width: 100%;\n",
    "                }\n",
    "                th, td {\n",
    "                    text-align: left;\n",
    "                    padding: 8px;\n",
    "                }\n",
    "                th {\n",
    "                    background-color: #007bff;\n",
    "                    color: #fff;\n",
    "                    font-weight: bold;\n",
    "                }\n",
    "                tr:nth-child(even) {\n",
    "                    background-color: #f2f2f2;\n",
    "                }\n",
    "                tr:hover {\n",
    "                    background-color: #ddd;\n",
    "                }\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <table>\n",
    "                <tr><th>Name</th><th>Size</th></tr>\\n\n",
    "    ''')\n",
    "\n",
    "    # Loop through each file and add a row to the table\n",
    "    for file_name in file_list:\n",
    "        if file_name in ['./index.html']:\n",
    "            continue\n",
    "            \n",
    "        file_size = os.path.getsize(file_name)\n",
    "        f.write(f'<tr><td><a href=\"{website}/{file_name}\" target=\"_blank\" rel=\"noopener noreferrer\">{file_name}</a></td><td>{int(file_size / 1048576)} MB</td></tr>\\n')\n",
    "\n",
    "    # Write the footer and close the file\n",
    "    f.write('</table></body></html>')\n",
    "\n",
    "command = [\"aws\", \"s3\", \"sync\", \".\", f\"s3://{bucket}\", \"--exclude\", \"*\", \"--include\", \"*.html\", \"--no-progress\"]\n",
    "\n",
    "# Run the command and wait for it to complete\n",
    "output = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "# Print the output\n",
    "print(output.stdout)\n",
    "print('fin')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p37",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
