{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process 2015 Data\n",
    "\n",
    "Goals:\n",
    "- Use the 2015 MAUDE data to create a sample dataset for experimentation\n",
    "- I really need to finish this soon.\n",
    "\n",
    "Steps:\n",
    "1. Identify all common data files\n",
    "\n",
    "|File                    |Description|Required|\n",
    "|------------------------|-----------|--------|\n",
    "|`mdrfoithru2021.zip`    |Master Record through 2021|X|\n",
    "|`patientthru2021.zip`   |Patient Record through 2021|X|\n",
    "|`foitextchange.zip`     |Narrative data updates: changes to existing narrative data and additional narrative data for existing base records|X|\n",
    "|`patientproblemcode.zip`|Device Data for patientproblemcode||\n",
    "|`patientproblemdata.zip`|Patient Problem Data||\n",
    "|`patientchange.zip`     |MAUDE Patient data updates: changes to existing Base data||\n",
    "|`mdrfoichange.zip`      |MAUDE Base data updates: changes to existing Base data||\n",
    "|`devicechange.zip`      |Device data updates: changes to existing Device data and additional Device data for existing Base records||\n",
    "|`deviceproblemcodes.zip`|Device Problem Data||\n",
    "|`foidevproblem.zip`     |Device Data for foidevproblem||\n",
    "\n",
    "2. Identify all 2015 data files\n",
    "\n",
    "|File                    |Description|Required|\n",
    "|------------------------|-----------|--------|\n",
    "|`device2015.zip`        |Device Data for 2015|X|\n",
    "|`foitext2015.zip`       |Narrative Data for 2015|X|\n",
    "\n",
    "3. Create databases for each data type\n",
    "4. Create a merged dataset using joins for each Master Data Record ID in the 2015 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzip data files\n",
    "Goals:\n",
    "- _*(COMPLETE)*_ Unzip downloaded data files to extract the archived `.txt` file\n",
    "\n",
    "The following Python code completes these steps:\n",
    "1. Identify the data directory, working directory, and data files\n",
    "1. Create the working directory if needed\n",
    "1. Unzip the data files into the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "# Identify the data directory, working directory, and data files\n",
    "data_directory = './data'\n",
    "working_directory = './2015'\n",
    "data_files = ['device2015.zip', 'foitextchange.zip', 'patientthru2021.zip',\n",
    "              'foitext2015.zip', 'mdrfoithru2021.zip']\n",
    "\n",
    "# Create the working directory if needed\n",
    "try:\n",
    "    os.makedirs(working_directory, exist_ok=True)\n",
    "except OSError as error:\n",
    "    print(f\"Error creating {working_directory}: {error}\")\n",
    "\n",
    "# Unzip the data files into the working directory\n",
    "for i in data_files:\n",
    "    print(f\"Unzipping {i}\")\n",
    "    with ZipFile(f\"{data_directory}/{i}\", \"r\") as zip:\n",
    "        zip.extractall(f\"{working_directory}\")\n",
    "\n",
    "print(\"Unzip complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a database with tables for each data file\n",
    "Goals:\n",
    "- _*(COMPLETE)*_ Read the data files into dataframes\n",
    "- _*(COMPLETE)*_ Write the dataframes to tables in an SQLite database\n",
    "\n",
    "The following Python code completes these steps:\n",
    "1. Create a database\n",
    "1. Process each file in the working directory\n",
    "1. Get the file path and the base name of the file by removing '.txt'\n",
    "1. Create a dataframe for each file using Pandas, reading each file as a comma seperated values file, using the pipe seperator\n",
    "1. Set the MDR_REPORT_KEY as the index for the dataframe\n",
    "1. Write the dataframe to a new SQLite table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import sqlite3\n",
    "\n",
    "# Identify the working directory\n",
    "working_directory = './2015'\n",
    "\n",
    "# Create an SQLite database\n",
    "db = sqlite3.connect(f\"{working_directory}/database.sqlite3\")\n",
    "\n",
    "# Process each file in the working directory\n",
    "for root, dirs, files in os.walk(working_directory):\n",
    "   for file_name in files:\n",
    "\n",
    "      # Only process '.txt' files...\n",
    "      if file_name.endswith(\".txt\"):\n",
    "         print(f\"Processing {file_name}\")\n",
    "         \n",
    "         # Get the file path and the base name of the file by removing '.txt'\n",
    "         file_path = os.path.join(root, file_name)\n",
    "         base_name = file_name.replace('.txt','')\n",
    "\n",
    "         # Create a dataframe for the file\n",
    "         df = pd.read_csv(file_path, \n",
    "            sep=\"|\", \n",
    "            encoding=\"ISO-8859-1\", \n",
    "            on_bad_lines='warn', \n",
    "            quoting=csv.QUOTE_NONE)\n",
    "\n",
    "         # Set the MDR_REPORT_KEY as the index for the dataframe\n",
    "         df.set_index('MDR_REPORT_KEY', inplace=True)\n",
    "         \n",
    "         # Write the dataframe to a new SQLite table\n",
    "         df.to_sql(base_name, db, if_exists=\"replace\")\n",
    "\n",
    "         # Remove the dataframe in an attempt to free up memory\n",
    "         del df\n",
    "\n",
    "print(\"Database creation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the 2015 Data\n",
    "\n",
    "Goals:\n",
    "- _*(PARTIALLY COMPLETE)*_ Given that the database has tables for all the data elements, combine the data to form a complete data set for 2015\n",
    "\n",
    "The following Python code completes these steps:\n",
    "1. Create tables for MDR and Patient data from 2015\n",
    "1. Use the 2015 tables to join the DEVICE2015, foitext2015, and foitextChange data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Identify the working directory\n",
    "working_directory = './2015'\n",
    "\n",
    "# Connect to the SQLite database\n",
    "db = sqlite3.connect(f\"{working_directory}/database.sqlite3\")\n",
    "\n",
    "# Create a table for the MDR 2015 data\n",
    "db.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS mdrfoi2015 AS\n",
    "    SELECT *\n",
    "      FROM mdrfoiThru2021\n",
    "     WHERE mdrfoiThru2021.DATE_RECEIVED LIKE '%2015%';\n",
    "''')\n",
    "\n",
    "# Create a table for the patient 2015 data\n",
    "db.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS patient2015 AS\n",
    "    SELECT *\n",
    "    FROM patientThru2021\n",
    "    WHERE patientThru2021.DATE_RECEIVED LIKE '%2015%';\n",
    "''')\n",
    "\n",
    "# Create a temp table for 2015 Device and FOI Text data\n",
    "db.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS devicefoitext2015 AS\n",
    "    SELECT  \n",
    "        foitextChange.*,\n",
    "        foitext2015.*,\n",
    "        DEVICE2015.*\n",
    "    FROM DEVICE2015\n",
    "        LEFT JOIN foitextChange ON DEVICE2015.MDR_REPORT_KEY = foitextChange.MDR_REPORT_KEY\n",
    "        LEFT JOIN foitext2015 ON DEVICE2015.MDR_REPORT_KEY = foitext2015.MDR_REPORT_KEY;\n",
    "''')\n",
    "\n",
    "print(\"2015 Device and FOI Text data combination complete\")\n",
    "\n",
    "# Create a temp table for 2015 Device, FOI Text, and Patient data\n",
    "db.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS devicefoitextpatient2015 AS\n",
    "    SELECT  \n",
    "        patient2015.*,\n",
    "        devicefoitext2015.*\n",
    "    FROM devicefoitext2015\n",
    "        LEFT JOIN patient2015 ON devicefoitext2015.MDR_REPORT_KEY = patient2015.MDR_REPORT_KEY;\n",
    "''')\n",
    "\n",
    "print(\"2015 Device, FOI Text, and Patient data combination complete\")\n",
    "\n",
    "# Create a temp table for 2015 Device, FOI Text, and Patient data, and MDR data\n",
    "# db.execute('''\n",
    "#     CREATE TABLE IF NOT EXISTS all2015 AS\n",
    "#     SELECT  \n",
    "#         mdrfoi2015.*,\n",
    "#         devicefoitextpatient2015.*\n",
    "#     FROM devicefoitextpatient2015\n",
    "#         LEFT JOIN mdrfoi2015 ON devicefoitextpatient2015.MDR_REPORT_KEY = mdrfoi2015.MDR_REPORT_KEY;\n",
    "# ''')\n",
    "\n",
    "# print(\"2015 Device, FOI Text, Patient data, and MDR data combination complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations and Research\n",
    "To complete this step in the data processing, a two dedicated servers were created in AWS to run the Jupyter notebooks.  The first environment was a [SageMaker Studio Lab](https://studiolab.sagemaker.aws/) environment with 15GB disk and 4GB RAM.  The second was a [SageMaker Notebook server](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html) with 4GB RAM and 100GB disk.\n",
    "\n",
    "The SageMaker Studio Lab server was substantially faster than the Google Colab environment initially used and allowed for persistent data storage.  This improved the processing for the download notebook, [1-Download-MAUDE-Data](./1-Download-MAUDE-Data.ipynb).  However, the disk space allocated to the SageMaker Studio Lab server was not enough to hold the downloaded MAUDE data (which is compressed) _and_ the uncompressed MAUDE data.  This prompted the move to a dedicated environment with more disk space.\n",
    "\n",
    "The SageMaker Notebook Server environment allowed the server disk space to be specified directly.  The server was created with enough disk space to hold the compressed and uncompressed MAUDE data set.  However, while processing the uncompressed data to creata a database, the Jupyter notebook runtime experienced crashes, indicating that not enough RAM was available to process the largest of the MAUDE datafiles which is about 4 GB uncompressed.  The code was updated in an attempt to try to free memory when possible but that did not prevent the notebook from crashing.\n",
    "\n",
    "To progress with the data processing, a Jupyter environment was created on a MacBook Pro with 64GB of RAM and plenty of disk space for the compressed and uncompressed MAUDE data.  This allowed the [2-Process-2015-Data](./2-Process-2015-Data.ipynb) notebook to successfully create all databases as needed.\n",
    "\n",
    "With databases in place for each file type of the MAUDE data for 2015, the process of combining the data was initiated.  An initial attempt was made to join all the data using the MDR_REPORT_KEY as the index for joining rows.  However, this resulted in a long runtime; the process was allowed to run for just over 11 hours before being interrupted.\n",
    "\n",
    "In an effort to speed up the data combination, new tables were made to hold the 2015 data from the MDR and Patient files, which contain data from the start of the project to the present date.  Then, the data was joined two tables at a time, starting with the Device data and the report data.  The result of the device and text join was used to create an intermediate table.  This new table was then joined with the Patient data to create a second intermediate table.  These two joins were able to complete in less than 6 minutes.  However, attempting to join the second intermediate table with the MDR data resulted in the process running for more than two hours before being interrupted.\n",
    "\n",
    "The document [A Primer to the Structure, Content and Linkage of the FDA’s Manufacturer and User Facility Device Experience (MAUDE) Files](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5994953/) was reference again to see if anything could be done to expidite the combination of the MAUDE data to produce a complete data set for 2015.\n",
    "\n",
    "Upon reviewing the space for the problem related to this student's practice, an observation was made that the MDR data may not be needed to complete the project as intended.\n",
    "\n",
    "# Next Steps\n",
    "1. Review the merged data for completeness\n",
    "1. If the data is complete, filter for device type and remove duplicates\n",
    "1. Review the data in the device and report data files to determine if the data they contain can be used without the MDR data\n",
    "\n",
    "# Summary\n",
    "1. Two dedicated Jupyter noteboook envionments were created in AWS to allow for persistent data storage and improved runtime speeds.\n",
    "1. In both dedicated environments, the size of the MAUDE data taxed the disk and RAM capacity.  Ultimately, the data was processed on a local workstation with 1TB disk and 64GB RAM.\n",
    "1. Combining the data for 2015 proved difficult using an \"all-at-once\" approach.  Using intermediate tables that joined two data files together at a time allowed the data to be merged efficiently and to completion.\n",
    "1. The size of the MDR data set is proving to be problematic for merging.\n",
    "1. The merged data will be reviewed to determine if the MDR data is required or if the device, report, and patient data can be used without the MDR data.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('VSCode')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0df23e0b8587652e8947bf433502af00ecfb3462d59e71e7bba71c0280b5fb19"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
