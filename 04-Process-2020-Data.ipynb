{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Review and Reprocess 2020 Data\n",
    "\n",
    "## Overview\n",
    "\n",
    "## Data review by file\n",
    "The column names and data in each file were reviewed.  Following are the reviews for each file.  \n",
    "\n",
    "Columns that are needed are marked with `.`.  Columns that are _not_ needed are marked with `x`. \n",
    "\n",
    "### `foidevproblem.txt`:\n",
    "- All Columns needed from this file\n",
    "- _*THERE ARE NO COLUMN HEADERS PRESENT IN THE FILE*_\n",
    "- The following column headers need to be added manually\n",
    "```\n",
    "\t. MDR_REPORT_KEY\n",
    "\t. DEVICE_PROBLEM_CODE\n",
    "```\n",
    "\n",
    "### `deviceproblemcodes.txt`:\n",
    "- All Columns needed from this file\n",
    "- _*THERE ARE NO COLUMN HEADERS PRESENT IN THE FILE*_\n",
    "- The following column headers need to be added manually\n",
    "```\n",
    "\t. DEVICE_PROBLEM_CODE\n",
    "\t. DEVICE_PROBLEM_TEXT\n",
    "```\n",
    "\n",
    "### `foitext2015.txt`:\n",
    "- All Columns are needed\n",
    "```\n",
    "\t. MDR_REPORT_KEY\n",
    "\t. MDR_TEXT_KEY\n",
    "\t. TEXT_TYPE_CODE\n",
    "\t. PATIENT_SEQUENCE_NUMBER\n",
    "\t. DATE_REPORT\n",
    "\t. FOI_TEXT\n",
    "```\n",
    "\n",
    "### `DEVICE2015.txt`:\n",
    "- Columns that are needed are marked with `.`.  Columns that are _not_ needed are marked with `x`. \n",
    "```\n",
    "\t. MDR_REPORT_KEY\n",
    "\tx DEVICE_EVENT_KEY\n",
    "\tx IMPLANT_FLAG\n",
    "\tx DATE_REMOVED_FLAG\n",
    "\t. DEVICE_SEQUENCE_NO\n",
    "\tx DATE_RECEIVED\n",
    "\t. BRAND_NAME\n",
    "\t. GENERIC_NAME\n",
    "\t. MANUFACTURER_D_NAME\n",
    "\tx MANUFACTURER_D_ADDRESS_1\n",
    "\tx MANUFACTURER_D_ADDRESS_2\n",
    "\tx MANUFACTURER_D_CITY\n",
    "\tx MANUFACTURER_D_STATE_CODE\n",
    "\tx MANUFACTURER_D_ZIP_CODE\n",
    "\tx MANUFACTURER_D_ZIP_CODE_EXT\n",
    "\tx MANUFACTURER_D_COUNTRY_CODE\n",
    "\tx MANUFACTURER_D_POSTAL_CODE\n",
    "\tx DEVICE_OPERATOR\n",
    "\tx EXPIRATION_DATE_OF_DEVICE\n",
    "\t. MODEL_NUMBER\n",
    "\tx CATALOG_NUMBER\n",
    "\tx LOT_NUMBER\n",
    "\tx OTHER_ID_NUMBER\n",
    "\t. DEVICE_AVAILABILITY\n",
    "\tx DATE_RETURNED_TO_MANUFACTURER\n",
    "\t. DEVICE_REPORT_PRODUCT_CODE\n",
    "\tx DEVICE_AGE_TEXT\n",
    "\tx DEVICE_EVALUATED_BY_MANUFACTUR\n",
    "\tx COMBINATION_PRODUCT_FLAG\n",
    "```\n",
    "\n",
    "### `mdrfoiThru2021.txt`:\n",
    "- Columns that are needed are marked with `.`.  Columns that are _not_ needed are marked with `x`. \n",
    "- Consider modifying this file using Pandas to reduce the size of the data before merging with other files.\n",
    "```\n",
    "\t. MDR_REPORT_KEY\n",
    "\tx EVENT_KEY\n",
    "\t. REPORT_NUMBER\n",
    "\t. REPORT_SOURCE_CODE\n",
    "\tx MANUFACTURER_LINK_FLAG_\n",
    "\t. NUMBER_DEVICES_IN_EVENT\n",
    "\tx NUMBER_PATIENTS_IN_EVENT\n",
    "\t. DATE_RECEIVED\n",
    "\tx ADVERSE_EVENT_FLAG\n",
    "\tx PRODUCT_PROBLEM_FLAG\n",
    "\tx DATE_REPORT\n",
    "\t. DATE_OF_EVENT\n",
    "\tx REPROCESSED_AND_REUSED_FLAG\n",
    "\t. REPORTER_OCCUPATION_CODE\n",
    "\tx HEALTH_PROFESSIONAL\n",
    "\t. INITIAL_REPORT_TO_FDA\n",
    "\tx DATE_FACILITY_AWARE\n",
    "\t. REPORT_DATE\n",
    "\tx REPORT_TO_FDA\n",
    "\tx DATE_REPORT_TO_FDA\n",
    "\t. EVENT_LOCATION\n",
    "\tx DATE_REPORT_TO_MANUFACTURER\n",
    "\tx MANUFACTURER_CONTACT_T_NAME\n",
    "\tx MANUFACTURER_CONTACT_F_NAME\n",
    "\tx MANUFACTURER_CONTACT_L_NAME\n",
    "\tx MANUFACTURER_CONTACT_STREET_1\n",
    "\tx MANUFACTURER_CONTACT_STREET_2\n",
    "\tx MANUFACTURER_CONTACT_CITY\n",
    "\tx MANUFACTURER_CONTACT_STATE\n",
    "\tx MANUFACTURER_CONTACT_ZIP_CODE\n",
    "\tx MANUFACTURER_CONTACT_ZIP_EXT\n",
    "\tx MANUFACTURER_CONTACT_COUNTRY\n",
    "\tx MANUFACTURER_CONTACT_POSTAL\n",
    "\tx MANUFACTURER_CONTACT_AREA_CODE\n",
    "\tx MANUFACTURER_CONTACT_EXCHANGE\n",
    "\tx MANUFACTURER_CONTACT_PHONE_NO\n",
    "\tx MANUFACTURER_CONTACT_EXTENSION\n",
    "\tx MANUFACTURER_CONTACT_PCOUNTRY\n",
    "\tx MANUFACTURER_CONTACT_PCITY\n",
    "\tx MANUFACTURER_CONTACT_PLOCAL\n",
    "\t. MANUFACTURER_G1_NAME\n",
    "\tx MANUFACTURER_G1_STREET_1\n",
    "\tx MANUFACTURER_G1_STREET_2\n",
    "\tx MANUFACTURER_G1_CITY\n",
    "\tx MANUFACTURER_G1_STATE_CODE\n",
    "\tx MANUFACTURER_G1_ZIP_CODE\n",
    "\tx MANUFACTURER_G1_ZIP_CODE_EXT\n",
    "\tx MANUFACTURER_G1_COUNTRY_CODE\n",
    "\tx MANUFACTURER_G1_POSTAL_CODE\n",
    "\tx DATE_MANUFACTURER_RECEIVED\n",
    "\tx DEVICE_DATE_OF_MANUFACTURE\n",
    "\tx SINGLE_USE_FLAG\n",
    "\t. REMEDIAL_ACTION\n",
    "\tx PREVIOUS_USE_CODE\n",
    "\tx REMOVAL_CORRECTION_NUMBER\n",
    "\t. EVENT_TYPE\n",
    "\tx DISTRIBUTOR_NAME\n",
    "\tx DISTRIBUTOR_ADDRESS_1\n",
    "\tx DISTRIBUTOR_ADDRESS_2\n",
    "\tx DISTRIBUTOR_CITY\n",
    "\tx DISTRIBUTOR_STATE_CODE\n",
    "\tx DISTRIBUTOR_ZIP_CODE\n",
    "\tx DISTRIBUTOR_ZIP_CODE_EXT\n",
    "\tx REPORT_TO_MANUFACTURER\n",
    "\t. MANUFACTURER_NAME\n",
    "\tx MANUFACTURER_ADDRESS_1\n",
    "\tx MANUFACTURER_ADDRESS_2\n",
    "\tx MANUFACTURER_CITY\n",
    "\tx MANUFACTURER_STATE_CODE\n",
    "\tx MANUFACTURER_ZIP_CODE\n",
    "\tx MANUFACTURER_ZIP_CODE_EXT\n",
    "\tx MANUFACTURER_COUNTRY_CODE\n",
    "\tx MANUFACTURER_POSTAL_CODE\n",
    "\t. TYPE_OF_REPORT\n",
    "\t. SOURCE_TYPE\n",
    "\tx DATE_ADDED\n",
    "\tx DATE_CHANGED\n",
    "\tx REPORTER_COUNTRY_CODE\n",
    "\tx PMA_PMN_NUM\n",
    "\tx EXEMPTION_NUMBER\n",
    "\t. SUMMARY_REPORT\n",
    "\t. NOE_SUMMARIZED\n",
    "```\n",
    "\n",
    "### `patientThru2021.txt`:\n",
    "- Columns marked with 'x' are not needed;\n",
    "- _*No Columns needed from this file*_\n",
    "```\n",
    "\tx MDR_REPORT_KEY\n",
    "\tx PATIENT_SEQUENCE_NUMBER\n",
    "\tx DATE_RECEIVED\n",
    "\tx SEQUENCE_NUMBER_TREATMENT\n",
    "\tx SEQUENCE_NUMBER_OUTCOME\n",
    "```\n",
    "\n",
    "### `foitextChange.txt`:\n",
    "- Columns marked with 'x' are not needed;\n",
    "- _*No Columns needed from this file*_\n",
    "```\n",
    "\tx MDR_REPORT_KEY\n",
    "\tx MDR_TEXT_KEY\n",
    "\tx TEXT_TYPE_CODE\n",
    "\tx PATIENT_SEQUENCE_NUMBER\n",
    "\tx DATE_REPORT\n",
    "\tx FOI_TEXT\n",
    "```\n",
    "\n",
    "## Summary of data needed\n",
    "After reviewing the files for 2015 and files common to the MAUDE data set, it was concluded that the following columns were needed from each file:\n",
    "\n",
    "```\n",
    "1. foidevproblem.txt:\n",
    "\t. MDR_REPORT_KEY\n",
    "\t. DEVICE_PROBLEM_CODE\n",
    "\n",
    "2. deviceproblemcodes.txt:\n",
    "\t. DEVICE_PROBLEM_CODE\n",
    "\t. DEVICE_PROBLEM_TEXT\n",
    "\n",
    "3. foitext2015.txt:\n",
    "\t. MDR_REPORT_KEY\n",
    "\t. MDR_TEXT_KEY\n",
    "\t. TEXT_TYPE_CODE\n",
    "\t. PATIENT_SEQUENCE_NUMBER\n",
    "\t. DATE_REPORT\n",
    "\t. FOI_TEXT\n",
    "\n",
    "4. DEVICE2015.txt:\n",
    "\t. MDR_REPORT_KEY\n",
    "\t. DEVICE_SEQUENCE_NO\n",
    "\t. BRAND_NAME\n",
    "\t. GENERIC_NAME\n",
    "\t. MANUFACTURER_D_NAME\n",
    "\t. MODEL_NUMBER\n",
    "\t. DEVICE_AVAILABILITY\n",
    "\t. DEVICE_REPORT_PRODUCT_CODE\n",
    "\t\n",
    "5. mdrfoiThru2021.txt:\n",
    "\t. MDR_REPORT_KEY\n",
    "\t. REPORT_NUMBER\n",
    "\t. REPORT_SOURCE_CODE\n",
    "\t. NUMBER_DEVICES_IN_EVENT\n",
    "\t. DATE_RECEIVED\n",
    "\t. INITIAL_REPORT_TO_FDA\n",
    "\t. MANUFACTURER_G1_NAME\n",
    "\t. REMEDIAL_ACTION\n",
    "\t. EVENT_TYPE\n",
    "\t. MANUFACTURER_NAME\n",
    "\t. TYPE_OF_REPORT\n",
    "\t. SUMMARY_REPORT\n",
    "\t. NOE_SUMMARIZED\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment to process the data files\n",
    "Goals:\n",
    "- _*(COMPLETE)*_ Identify the data directory and working directory, creating the working directory as needed\n",
    "\n",
    "The following Python code completes these steps:\n",
    "1. Identify the data directory and working directory\n",
    "1. Create the working directory if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Identify the data directory, working directory, and data files\n",
    "data_directory = './01-Download-MAUDE-Data'\n",
    "working_directory = './04-Process-2020-Data'\n",
    "\n",
    "# Create the working directory if needed\n",
    "try:\n",
    "    os.makedirs(working_directory, exist_ok=True)\n",
    "except OSError as error:\n",
    "    print(f\"Error creating {working_directory}: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the data for 2020 and any common data\n",
    "Goals:\n",
    "- _*(COMPLETE)*_ Reference the data files in the data directory and unzip them into the working directory\n",
    "\n",
    "The following Python code completes these steps:\n",
    "1. Identify the data directory and working directory\n",
    "1. Unzip the data files into the working directory\n",
    "1. Change file names in working directory to lower case. This is needed because the device data use a lowercase name for the zip archives but the data filename is in all caps.  Setting everything to lowercase will make it easy to work with files going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unzipping foidevproblem.zip\n",
      "Unzipping deviceproblemcodes.zip\n",
      "Unzipping foitext2020.zip\n",
      "Unzipping device2020.zip\n",
      "Unzipping mdrfoithru2021.zip\n",
      "Unzip complete.\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "data_files = [\n",
    "    'foidevproblem.zip',       # 1\n",
    "    'deviceproblemcodes.zip',  # 2\n",
    "    'foitext2020.zip',         # 3\n",
    "    'device2020.zip',          # 4\n",
    "    'mdrfoithru2021.zip'       # 5\n",
    "]\n",
    "\n",
    "# Unzip the data files into the working directory\n",
    "for i in data_files:\n",
    "    print(f\"Unzipping {i}\")\n",
    "    with ZipFile(f\"{data_directory}/{i}\", \"r\") as zip:\n",
    "        zip.extractall(f\"{working_directory}\")\n",
    "\n",
    "print(\"Unzip complete.\")\n",
    "\n",
    "# Change file names in working directory to lower case\n",
    "for file in os.listdir(working_directory):\n",
    "        os.rename(f\"{working_directory}/{file}\", f\"{working_directory}/{file.lower()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine `foidevproblem.txt` and `deviceproblemcodes.txt`\n",
    "The `foidevproblem.txt` data file contains problem codes referenced by `MDR_REPORT_KEY`.  Another file, `deviceproblemcodes.txt` contains the full description for the code. \n",
    "\n",
    "Goals:\n",
    "- _*(COMPLETE)*_ Combine the data from `foidevproblem` and `deviceproblemcodes` into a single dataframe that can be referenced by `MDR_REPORT_KEY`.\n",
    "\n",
    "The following Python code completes these steps:\n",
    "1. Read the `foidevproblem.txt` and `deviceproblemcodes.txt` files into data frames\n",
    "1. Use an inner join to combine the device code data frames on the 'DEVICE_PROBLEM_CODE' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined_foidevproblem data frame creation complete (rows, columns): (15982151, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Read the foidevproblem and deviceproblemcodes files into data frames\n",
    "foidevproblem = pd.read_csv(f\"{working_directory}/foidevproblem.txt\", \n",
    "        sep=\"|\",                                        # The data is seperated by the '|' character\n",
    "        encoding=\"ISO-8859-1\",                          # This helps with reading the file format\n",
    "        on_bad_lines='warn',                            # This tells Pandas to only warn on bad lines vs causing an error\n",
    "        quoting=csv.QUOTE_NONE,                         # This helps Pandas process records that have odd quotes in them\n",
    "        names=[\"MDR_REPORT_KEY\",\"DEVICE_PROBLEM_CODE\"], # This names the columns manually\n",
    "        header=None,                                    # This tells Pandas that the data does not have column headers\n",
    "        dtype = 'str')                                  # This tells Pandas to treat all numbers as words\n",
    "        \n",
    "deviceproblemcodes = pd.read_csv(f\"{working_directory}/deviceproblemcodes.txt\", \n",
    "        sep=\"|\", \n",
    "        encoding=\"ISO-8859-1\", \n",
    "        on_bad_lines='warn', \n",
    "        quoting=csv.QUOTE_NONE,\n",
    "        names=[\"DEVICE_PROBLEM_CODE\",\"DEVICE_PROBLEM_TEXT\"], # This names the columns manually\n",
    "        header=None, \n",
    "        dtype = 'str')\n",
    "\n",
    "# Use an inner join to combine the device code data frames on the 'DEVICE_PROBLEM_CODE' column\n",
    "# The new data frame will include: \n",
    "#       MDR_REPORT_KEY\n",
    "#       DEVICE_PROBLEM_CODE\n",
    "#       DEVICE_PROBLEM_TEXT\n",
    "combined_foidevproblem = pd.merge(\n",
    "        foidevproblem, \n",
    "        deviceproblemcodes, \n",
    "        on=\"DEVICE_PROBLEM_CODE\", \n",
    "        how=\"inner\")\n",
    "\n",
    "# Tell Pandas to use the MDR_REPORT_KEY column as the index for the dataframe without creating a new dataframe\n",
    "combined_foidevproblem.set_index('MDR_REPORT_KEY', inplace=True)\n",
    "\n",
    "print(f\"combined_foidevproblem data frame creation complete (rows, columns): {combined_foidevproblem.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the 2020 narative data\n",
    "Goals:\n",
    "- _*(COMPLETE)*_ Read the 2020 narrative data into a dataframe\n",
    "\n",
    "The following Python code completes these steps:\n",
    "1. Create a dataframe for the `foitext2020.txt` file\n",
    "1. Tell Pandas to use the MDR_REPORT_KEY column as the index for the dataframe without creating a new dataframe\n",
    "1. Replace any records that Pandas converted to 'N/A' with an empty string.  This is needed for further processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foitext2020 data frame creation complete: (3041922, 5)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Create a dataframe for the foitext2020 file\n",
    "foitext2020 = pd.read_csv(f\"{working_directory}/foitext2020.txt\",\n",
    "        sep=\"|\", \n",
    "        encoding=\"ISO-8859-1\", \n",
    "        on_bad_lines='warn', \n",
    "        quoting=csv.QUOTE_NONE, \n",
    "        dtype = 'str')\n",
    "\n",
    "# Use the MDR_REPORT_KEY column as the index for the dataframe without creating a new dataframe\n",
    "foitext2020.set_index('MDR_REPORT_KEY', inplace=True)\n",
    "\n",
    "# Replace any records that Pandas converted to 'N/A' with an empty string.  This is needed for further processing\n",
    "foitext2020.fillna('', inplace=True)\n",
    "\n",
    "print(f\"foitext2020 data frame creation complete: {foitext2020.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device2020 data frame creation complete (rows, columns): (1567670, 9)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Create a dataframe for the device file\n",
    "device2020 = pd.read_csv(f\"{working_directory}/device2020.txt\",\n",
    "        sep=\"|\", \n",
    "        encoding=\"ISO-8859-1\", \n",
    "        on_bad_lines='warn', \n",
    "        quoting=csv.QUOTE_NONE, \n",
    "        dtype = 'str')\n",
    "\n",
    "# Use the MDR_REPORT_KEY column as the index for the dataframe without creating a new dataframe\n",
    "device2020.set_index('MDR_REPORT_KEY', inplace=True)\n",
    "\n",
    "# Identify the unwanted columns\n",
    "unwanted_columns = [\n",
    "    'DEVICE_EVENT_KEY',\n",
    "    'IMPLANT_FLAG',\n",
    "    'DATE_REMOVED_FLAG',\n",
    "    'DATE_RECEIVED',\n",
    "    'MANUFACTURER_D_ADDRESS_1',\n",
    "    'MANUFACTURER_D_ADDRESS_2',\n",
    "    'MANUFACTURER_D_CITY',\n",
    "    'MANUFACTURER_D_STATE_CODE',\n",
    "    'MANUFACTURER_D_ZIP_CODE',\n",
    "    'MANUFACTURER_D_ZIP_CODE_EXT',\n",
    "    'MANUFACTURER_D_COUNTRY_CODE',\n",
    "    'MANUFACTURER_D_POSTAL_CODE',\n",
    "    'DEVICE_OPERATOR',\n",
    "    'EXPIRATION_DATE_OF_DEVICE',\n",
    "    'CATALOG_NUMBER',\n",
    "    'LOT_NUMBER',\n",
    "    'OTHER_ID_NUMBER',\n",
    "    'DATE_RETURNED_TO_MANUFACTURER',\n",
    "    'DEVICE_AGE_TEXT',\n",
    "    'DEVICE_EVALUATED_BY_MANUFACTUR',\n",
    "    'COMBINATION_PRODUCT_FLAG'\n",
    "]\n",
    "\n",
    "# Remove the unwanted columns from the device dataframe\n",
    "device2020.drop(unwanted_columns, axis=1, inplace=True)\n",
    "\n",
    "# Replace any records that Pandas converted to 'N/A' with an empty string.  This is needed for further processing\n",
    "device2020.fillna('', inplace=True)\n",
    "\n",
    "print(f\"device2020 data frame creation complete (rows, columns): {device2020.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the MDR data\n",
    "Goals:\n",
    "- _*(COMPLETE)*_ Read the mdrfoithru2021 data into a dataframe\n",
    "\n",
    "The following Python code completes these steps:\n",
    "1. Create a dataframe for the `mdrfoithru2021.txt` file\n",
    "1. Use the MDR_REPORT_KEY column as the index for the dataframe without creating a new dataframe\n",
    "1. Identify the unwanted columns\n",
    "1. Remove the unwanted columns from the device dataframe\n",
    "1. Replace any records that Pandas converted to 'N/A' with an empty string.  This is needed for further processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 11971429: expected 82 fields, saw 83\\n'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Create a dataframe for the mdrfoithru2021 file\n",
    "mdrfoithru2021 = pd.read_csv(f\"{working_directory}/mdrfoithru2021.txt\",\n",
    "        sep=\"|\", \n",
    "        encoding=\"ISO-8859-1\", \n",
    "        on_bad_lines='warn', \n",
    "        quoting=csv.QUOTE_NONE,\n",
    "        dtype = 'str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdrfoiThru2021 data frame creation complete (rows, columns): (12830731, 17)\n"
     ]
    }
   ],
   "source": [
    "# Use the MDR_REPORT_KEY column as the index for the dataframe without creating a new dataframe\n",
    "mdrfoithru2021.set_index('MDR_REPORT_KEY', inplace=True)\n",
    "\n",
    "# Identify the unwanted columns\n",
    "unwanted_columns = [\n",
    "    'EVENT_KEY',\n",
    "    'MANUFACTURER_LINK_FLAG_',\n",
    "    'NUMBER_PATIENTS_IN_EVENT',\n",
    "    'ADVERSE_EVENT_FLAG',\n",
    "    'PRODUCT_PROBLEM_FLAG',\n",
    "    'DATE_REPORT',\n",
    "    'REPROCESSED_AND_REUSED_FLAG',\n",
    "    'HEALTH_PROFESSIONAL',\n",
    "    'DATE_FACILITY_AWARE',\n",
    "    'REPORT_TO_FDA',\n",
    "    'DATE_REPORT_TO_FDA',\n",
    "    'DATE_REPORT_TO_MANUFACTURER',\n",
    "    'MANUFACTURER_CONTACT_T_NAME',\n",
    "    'MANUFACTURER_CONTACT_F_NAME',\n",
    "    'MANUFACTURER_CONTACT_L_NAME',\n",
    "    'MANUFACTURER_CONTACT_STREET_1',\n",
    "    'MANUFACTURER_CONTACT_STREET_2',\n",
    "    'MANUFACTURER_CONTACT_CITY',\n",
    "    'MANUFACTURER_CONTACT_STATE',\n",
    "    'MANUFACTURER_CONTACT_ZIP_CODE',\n",
    "    'MANUFACTURER_CONTACT_ZIP_EXT',\n",
    "    'MANUFACTURER_CONTACT_COUNTRY',\n",
    "    'MANUFACTURER_CONTACT_POSTAL',\n",
    "    'MANUFACTURER_CONTACT_AREA_CODE',\n",
    "    'MANUFACTURER_CONTACT_EXCHANGE',\n",
    "    'MANUFACTURER_CONTACT_PHONE_NO',\n",
    "    'MANUFACTURER_CONTACT_EXTENSION',\n",
    "    'MANUFACTURER_CONTACT_PCOUNTRY',\n",
    "    'MANUFACTURER_CONTACT_PCITY',\n",
    "    'MANUFACTURER_CONTACT_PLOCAL',\n",
    "    'MANUFACTURER_G1_STREET_1',\n",
    "    'MANUFACTURER_G1_STREET_2',\n",
    "    'MANUFACTURER_G1_CITY',\n",
    "    'MANUFACTURER_G1_STATE_CODE',\n",
    "    'MANUFACTURER_G1_ZIP_CODE',\n",
    "    'MANUFACTURER_G1_ZIP_CODE_EXT',\n",
    "    'MANUFACTURER_G1_COUNTRY_CODE',\n",
    "    'MANUFACTURER_G1_POSTAL_CODE',\n",
    "    'DATE_MANUFACTURER_RECEIVED',\n",
    "    'DEVICE_DATE_OF_MANUFACTURE',\n",
    "    'SINGLE_USE_FLAG',\n",
    "    'PREVIOUS_USE_CODE',\n",
    "    'REMOVAL_CORRECTION_NUMBER',\n",
    "    'DISTRIBUTOR_NAME',\n",
    "    'DISTRIBUTOR_ADDRESS_1',\n",
    "    'DISTRIBUTOR_ADDRESS_2',\n",
    "    'DISTRIBUTOR_CITY',\n",
    "    'DISTRIBUTOR_STATE_CODE',\n",
    "    'DISTRIBUTOR_ZIP_CODE',\n",
    "    'DISTRIBUTOR_ZIP_CODE_EXT',\n",
    "    'REPORT_TO_MANUFACTURER',\n",
    "    'MANUFACTURER_ADDRESS_1',\n",
    "    'MANUFACTURER_ADDRESS_2',\n",
    "    'MANUFACTURER_CITY',\n",
    "    'MANUFACTURER_STATE_CODE',\n",
    "    'MANUFACTURER_ZIP_CODE',\n",
    "    'MANUFACTURER_ZIP_CODE_EXT',\n",
    "    'MANUFACTURER_COUNTRY_CODE',\n",
    "    'MANUFACTURER_POSTAL_CODE',\n",
    "    'DATE_ADDED',\n",
    "    'DATE_CHANGED',\n",
    "    'REPORTER_COUNTRY_CODE',\n",
    "    'PMA_PMN_NUM',\n",
    "    'EXEMPTION_NUMBER'\n",
    "]\n",
    "\n",
    "# Remove the unwanted columns from the MDR dataframe\n",
    "mdrfoithru2021.drop(unwanted_columns, axis=1, inplace=True)\n",
    "\n",
    "# Replace any records that Pandas converted to 'N/A' with an empty string.  This is needed for further processing\n",
    "mdrfoithru2021.fillna('', inplace=True)\n",
    "\n",
    "print(f\"mdrfoiThru2021 data frame creation complete (rows, columns): {mdrfoithru2021.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimize the MDR data\n",
    "Goals:\n",
    "- _*(COMPLETE)*_ Minimize the MDR data by pulling out records that were received in 2020\n",
    "\n",
    "The following Python code completes these steps:\n",
    "1. Use the `DATE_RECEIVED` column in the mdrfoithru2021 dataframe to create an MDR dataframe for 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdrfoi2020 data frame creation complete (rows, columns): (1565039, 17)\n"
     ]
    }
   ],
   "source": [
    "# Use the `DATE_RECEIVED` column in the mdrfoithru2021 dataframe to create an MDR dataframe for 2020\n",
    "mdrfoi2020 = mdrfoithru2021[mdrfoithru2021['DATE_RECEIVED'].str.contains('2020')]\n",
    "\n",
    "print(f\"mdrfoi2020 data frame creation complete (rows, columns): {mdrfoi2020.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a series of passes to merge the data to build a complete dataset\n",
    "Goals:\n",
    "- _*(COMPLETE)*_ Pass 1: Combine the narrative data with the device problem descriptions\n",
    "\n",
    "The following Python code completes these steps:\n",
    "1. Merge the `foitext2020` dataframe with the `combined_foidevproblem` dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass 1 of the data merge is complete (rows, columns): (3852658, 7)\n"
     ]
    }
   ],
   "source": [
    "# Merge foitext2020 with combined_foidevproblem\n",
    "pass1 = pd.merge(\n",
    "        foitext2020, \n",
    "        combined_foidevproblem, \n",
    "        on=\"MDR_REPORT_KEY\", \n",
    "        how=\"inner\")\n",
    "\n",
    "print(f\"Pass 1 of the data merge is complete (rows, columns): {pass1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a series of passes to merge the data to build a complete dataset\n",
    "Goals:\n",
    "- _*(COMPLETE)*_ Pass 2: Combine the result of the first pass with the device data\n",
    "\n",
    "The following Python code completes these steps:\n",
    "1. Merge the `pass1` datafram with the `device2015` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass 2 of the data merge is complete (rows, columns): (3856740, 16)\n"
     ]
    }
   ],
   "source": [
    "# Merge the pass1 datafram with the device2020 dataframe\n",
    "pass2 = pd.merge(\n",
    "        pass1, \n",
    "        device2020, \n",
    "        on=\"MDR_REPORT_KEY\", \n",
    "        how=\"inner\")\n",
    "\n",
    "print(f\"Pass 2 of the data merge is complete (rows, columns): {pass2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a series of passes to merge the data to build a complete dataset\n",
    "Goals:\n",
    "- _*(COMPLETE)*_ Pass 3: Combine the result of the second pass with the 2020 MDR data\n",
    "\n",
    "The following Python code completes these steps:\n",
    "1. Merge the `pass2` dataframe with the `mdrfoi2020` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass 3 of the data merge is complete (rows, columns): (3856740, 33)\n"
     ]
    }
   ],
   "source": [
    "# Merge pass2 dataframe with the mdrfoi2020 dataframe\n",
    "pass3 = pd.merge(\n",
    "        pass2, \n",
    "        mdrfoi2020, \n",
    "        on=\"MDR_REPORT_KEY\", \n",
    "        how=\"inner\")\n",
    "\n",
    "print(f\"Pass 3 of the data merge is complete (rows, columns): {pass3.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the combined data to a CSV file\n",
    "Goals:\n",
    "- _*(COMPLETE)*_ Write the combined data dataframe to a CSV\n",
    "\n",
    "The following Python code completes these steps:\n",
    "1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created report for pass3 complete (rows, columns): (3856740, 33)\n"
     ]
    }
   ],
   "source": [
    "# Write all rows of pass 3 to a spreadsheet\n",
    "pass3.to_csv(f\"{working_directory}/2020_data_complete.csv\")\n",
    "\n",
    "print(f\"Created report for pass3 complete (rows, columns): {pass3.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample the combined data\n",
    "Goals:\n",
    "- _*(COMPLETE)*_ Use the first 1000 rows of the third and final pass on the combined data to create a sample\n",
    "\n",
    "The following Python code completes these steps:\n",
    "1. Write the first 1000 rows of pass 3 to a spreadsheet, `2020_data_sample.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created report for pass3 sample (rows, columns): (1000, 33)\n"
     ]
    }
   ],
   "source": [
    "# Write the first 1000 rows of pass 3 to a spreadsheet\n",
    "pass3_head = pass3.head(1000)\n",
    "pass3_head.to_csv(f\"{working_directory}/2020_data_sample.csv\")\n",
    "\n",
    "print(f\"Created report for pass3 sample (rows, columns): {pass3_head.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report the generic names for devices\n",
    "Create a report that lists the generic name for each device along with the number of times the device appeared in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created report for generic names (rows, columns): (15204, 2)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Count the GENERIC_NAMES column from the pass3 dataframe and write the result to a dataframe\n",
    "generic_names = pd.DataFrame(pass3['GENERIC_NAME'].value_counts(), columns=['GENERIC_NAME','COUNT'])\n",
    "\n",
    "# Write the generic names report to a spreadsheet\n",
    "generic_names.to_csv(f\"{working_directory}/device_data_generic_names.csv\")\n",
    "\n",
    "print(f\"Created report for generic names (rows, columns): {generic_names.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-03-30 01:34 AM Notebook has completed.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# datetime object containing current date and time for the US/Pacific time zone\n",
    "now = datetime.now(pytz.timezone('US/Pacific'))\n",
    "\n",
    "# Format date and time like 2022-10-31 5:49 PM\n",
    "date_time_string = now.strftime(\"%Y-%m-%d %I:%M %p\") \n",
    "\n",
    "print(f\"{date_time_string} Notebook has completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations and Research\n",
    "After running into issues with merging the MAUDE data for 2020, the available data files were reviewed in an effort to optimize data analysis.\n",
    "\n",
    "The specific columns needed to analyze the data were identified.  Unneeded columns were called out for removal.\n",
    "\n",
    "Previous analysis was completed using Pandas to read the data into an SQLite database.  However, mergeing the data in SQLlite proved to be inefficient, causing long run times.\n",
    "\n",
    "After the data set was minimized by removing unneeded columns and files, Pandas dataframes were used to merge the data.  This approach was much more efficient than using SQLite.\n",
    "\n",
    "After merging all needed columns from all data files into a single dataframe, a 1000 row sample of data was exported.  This was needed due to the number of rows in the 2020 dataset being more than what could be easily opened in Excel.  The dataset contained 1.7M rows and the Excel limit is approximately 1M rows.\n",
    "\n",
    "A report was generated for the count of generic device names present in the 2020 dataset.  This may prove useful in further limiting the dataset to one specific device type.\n",
    "\n",
    "With the dataset being in a near complete state, preparation for Machine Learning processing can begin.\n",
    "\n",
    "Several references, courses, and other resources have been identified to help with beginning code development for the ML implementation.\n",
    "\n",
    "The following LinkedIn Learning courses were reccommended:\n",
    "-  Processing Text with Python Essential Training, 33m\n",
    "-  Advanced NLP with Python for Machine Learning, 2h 14m\n",
    "-  Text Analytics and Predictions with Python Essential Training, 35m\n",
    "-  NLP with Python for Machine Learning Essential Training, 4h 14m\n",
    "-  Natural Language Processing with PyTorch, 41m\n",
    "-  TensorFlow: Working with NLP, 41m\n",
    "-  Deep Learning Foundations: Natural Language Processing with TensorFlow, 1h 47m\n",
    "\n",
    "# Next Steps\n",
    "1. Review the sample dataset for 2020\n",
    "1. Review references, courses, and other resources on Machine Learning\n",
    "1. Begin developing an ML model to process the 2020 data\n",
    "\n",
    "# Summary\n",
    "1. The MAUDE data for 2020 data was reviewed and minimized by removing columns that were not needed for further analysis.  \n",
    "1. The data was merged using Pandas dataframes which allowed for more efficient processing compared to using SQLite.  \n",
    "1. A portion of the merged 2020 data was downloaded to create a sample for viewing and analysis in Excel.\n",
    "1. An additional report was generated for the count of generic device names present in the 2020 dataset.  This report may prove useful in further limiting the dataset to one specific device type.\n",
    "1. Machine Learning resources were identified to support developing an LDA model to process the 2020 data set.\n",
    "\n",
    "# Update, 2023-01-17\n",
    "The following columns were added back to the data:\n",
    "- DATE_OF_EVENT\n",
    "- REPORTER_OCCUPATION_CODE\n",
    "- REPORT_DATE\n",
    "- EVENT_LOCATION\n",
    "- SOURCE_TYPE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p37",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "0df23e0b8587652e8947bf433502af00ecfb3462d59e71e7bba71c0280b5fb19"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
