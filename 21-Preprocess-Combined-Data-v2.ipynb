{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44abeade-5fe3-40d6-99cb-621c66badf2e",
   "metadata": {},
   "source": [
    "# Preprocess Combined Data\n",
    "Using the combined QBJ as input, perform data preprocessing including:\n",
    "\n",
    "1. Expand Contractions, Tokenize, and Convert to Lowercase\n",
    "1. Remove Punctuation\n",
    "1. Remove Stop Words\n",
    "1. Remove Words Starting with a Digit\n",
    "1. Parts of Speech (POS) Tagging\n",
    "1. Lemmatize\n",
    "1. Stemming\n",
    "1. Create Bag of Words (BOW)\n",
    "1. Calculate Term Frequency\n",
    "1. Calculate Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "1. Sentencize\n",
    "     1. Lemmatize Sentences\n",
    "     1. Stem Sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8e07559-3ae2-43c7-aeb5-a5cad8413271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "! pip install --quiet --upgrade contractions\n",
    "! pip install --quiet --upgrade nltk\n",
    "! python -m nltk.downloader --quiet 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a5cb8b5-2419-4a10-95ef-b98e00fa8fb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Identify the working directory and data files\n",
    "working_directory = './21-Preprocess-Combined-Data-v2'\n",
    "\n",
    "# Create the working directory if needed\n",
    "try:\n",
    "    os.makedirs(working_directory, exist_ok=True)\n",
    "except OSError as error:\n",
    "    print(f\"Error creating {working_directory}: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b12a20c0-f0d4-47ba-bc75-32ef8cc62c14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Readthe combined data into a dataframe\n",
    "data_file = './15-Combine-2020-2021-Stratified-Data/qbj_data_combined.csv'\n",
    "\n",
    "# Read the data into a pandas dataframe\n",
    "df = pd.read_csv(data_file,           # The data file being read, from the variable assignment above\n",
    "                 on_bad_lines='warn', # This tells Pandas to only warn on bad lines vs causing an error\n",
    "                 dtype='str')         # This tells Pandas to treat all numbers as words\n",
    "\n",
    "df.fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6021374b-7c6c-48c7-9319-47b7512b9a0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5736, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85fd9d2d-01a6-45e6-8807-9e27dc01d5d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>FOI_TEXT</th>\n",
       "      <th>DEVICE_PROBLEM_CODE</th>\n",
       "      <th>DEVICE_PROBLEM_TEXT</th>\n",
       "      <th>GENERIC_NAME</th>\n",
       "      <th>DEVICE_REPORT_PRODUCT_CODE</th>\n",
       "      <th>UDI-DI</th>\n",
       "      <th>UDI-PUBLIC</th>\n",
       "      <th>DATE_OF_EVENT</th>\n",
       "      <th>REPORTER_OCCUPATION_CODE</th>\n",
       "      <th>REPORT_DATE</th>\n",
       "      <th>EVENT_LOCATION</th>\n",
       "      <th>SOURCE_TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1969025</td>\n",
       "      <td>IT WAS REPORTED THAT THE TRANSMITTER LOST CONN...</td>\n",
       "      <td>3283</td>\n",
       "      <td>Wireless Communication Problem</td>\n",
       "      <td>CONTINUOUS GLUCOSE MONITOR</td>\n",
       "      <td>QBJ</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>07/30/2020</td>\n",
       "      <td>000</td>\n",
       "      <td></td>\n",
       "      <td>I</td>\n",
       "      <td>CONSUMER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1426265</td>\n",
       "      <td>IT WAS REPORTED THAT SIGNAL LOSS OVER ONE HOUR...</td>\n",
       "      <td>3283</td>\n",
       "      <td>Wireless Communication Problem</td>\n",
       "      <td>CONTINUOUS GLUCOSE MONITOR</td>\n",
       "      <td>QBJ</td>\n",
       "      <td>00386270000385</td>\n",
       "      <td>00386270000385</td>\n",
       "      <td>06/05/2020</td>\n",
       "      <td>000</td>\n",
       "      <td></td>\n",
       "      <td>I</td>\n",
       "      <td>CONSUMER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ROW_ID                                           FOI_TEXT  \\\n",
       "0  1969025  IT WAS REPORTED THAT THE TRANSMITTER LOST CONN...   \n",
       "1  1426265  IT WAS REPORTED THAT SIGNAL LOSS OVER ONE HOUR...   \n",
       "\n",
       "  DEVICE_PROBLEM_CODE             DEVICE_PROBLEM_TEXT  \\\n",
       "0                3283  Wireless Communication Problem   \n",
       "1                3283  Wireless Communication Problem   \n",
       "\n",
       "                 GENERIC_NAME DEVICE_REPORT_PRODUCT_CODE          UDI-DI  \\\n",
       "0  CONTINUOUS GLUCOSE MONITOR                        QBJ                   \n",
       "1  CONTINUOUS GLUCOSE MONITOR                        QBJ  00386270000385   \n",
       "\n",
       "       UDI-PUBLIC DATE_OF_EVENT REPORTER_OCCUPATION_CODE REPORT_DATE  \\\n",
       "0                    07/30/2020                      000               \n",
       "1  00386270000385    06/05/2020                      000               \n",
       "\n",
       "  EVENT_LOCATION SOURCE_TYPE  \n",
       "0              I    CONSUMER  \n",
       "1              I    CONSUMER  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fd1319-1714-4115-9bb3-07935554919f",
   "metadata": {},
   "source": [
    "## Assign a Row ID for Verification\n",
    "Assign a value to a variable that identifies a row from the dataset.  \n",
    "\n",
    "This will allow the same row to be used for verification of each preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2746674c-c24b-4986-b047-54f45d1beeab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "verification_row = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cccf441-a429-4612-bda4-eafbc2af9ef0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load the Natural Language Toolkit (NLTK) and Preprocessing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cbed954-f2f1-41da-a1e8-3718d6ce413f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the NLTK library\n",
    "import nltk # If this step fails, rerun 07-Install-NLTK.ipynb\n",
    "import string\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b62b98e-4f7e-4025-81e7-0e04f25dd328",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Expand Contractions, Tokenize, and Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b2e40a5-3775-4ff2-94db-6df8f183131c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [it, was, reported, that, the, transmitter, lo...\n",
       "1    [it, was, reported, that, signal, loss, over, ...\n",
       "2    [it, was, reported, that, transmitter, failed,...\n",
       "3    [it, was, reported, that, signal, loss, over, ...\n",
       "4    [it, was, reported, that, signal, loss, over, ...\n",
       "Name: TOKENIZED_TEXT, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This approach takes the FOI_TEXT as a string and creates a new column with tokens\n",
    "# It removes contractions _and_ tokenizes at the same time\n",
    "# No additional function is needed, x.split tokenizes the string (FOI text) at every space\n",
    "# A call to lower() converts the word to lowercase\n",
    "\n",
    "df['TOKENIZED_TEXT'] = df['FOI_TEXT'].apply(lambda x: [contractions.fix(word).lower() for word in x.split()])\n",
    "df['TOKENIZED_TEXT'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49be4fc3-c02f-468b-9dc8-82506eaaeb12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " 'was',\n",
       " 'reported',\n",
       " 'that',\n",
       " 'signal',\n",
       " 'loss',\n",
       " 'over',\n",
       " 'one',\n",
       " 'hour',\n",
       " 'occurred.',\n",
       " 'no',\n",
       " 'product',\n",
       " 'or',\n",
       " 'data',\n",
       " 'was',\n",
       " 'provided',\n",
       " 'for',\n",
       " 'evaluation.',\n",
       " 'confirmation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'allegation',\n",
       " 'and',\n",
       " 'a',\n",
       " 'probable',\n",
       " 'because',\n",
       " 'could',\n",
       " 'not',\n",
       " 'be',\n",
       " 'determined.',\n",
       " 'no',\n",
       " 'injury',\n",
       " 'or',\n",
       " 'medical',\n",
       " 'intervention',\n",
       " 'was',\n",
       " 'reported.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['TOKENIZED_TEXT'][verification_row]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b341a0-4cc5-409b-a4f9-510cff03c266",
   "metadata": {},
   "source": [
    "## 2. Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6eb411b-67a5-4f90-addc-1fe49c741838",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef62350c-3ed9-4b54-a69b-a3e93059c550",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [it, was, reported, that, the, transmitter, lo...\n",
       "1    [it, was, reported, that, signal, loss, over, ...\n",
       "2    [it, was, reported, that, transmitter, failed,...\n",
       "3    [it, was, reported, that, signal, loss, over, ...\n",
       "4    [it, was, reported, that, signal, loss, over, ...\n",
       "Name: NOPUNCT_TEXT, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to remove punctuation in the data\n",
    "def remove_punctuation(text):\n",
    "    text = \"\".join([character for character in text if character not in string.punctuation])\n",
    "    return text\n",
    "\n",
    "df['NOPUNCT_TEXT'] = df['TOKENIZED_TEXT'].apply(lambda x: [remove_punctuation(word) for word in x])\n",
    "df['NOPUNCT_TEXT'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fe886e3-fad3-4ca4-a02a-d94b796eb33a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " 'was',\n",
       " 'reported',\n",
       " 'that',\n",
       " 'signal',\n",
       " 'loss',\n",
       " 'over',\n",
       " 'one',\n",
       " 'hour',\n",
       " 'occurred',\n",
       " 'no',\n",
       " 'product',\n",
       " 'or',\n",
       " 'data',\n",
       " 'was',\n",
       " 'provided',\n",
       " 'for',\n",
       " 'evaluation',\n",
       " 'confirmation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'allegation',\n",
       " 'and',\n",
       " 'a',\n",
       " 'probable',\n",
       " 'because',\n",
       " 'could',\n",
       " 'not',\n",
       " 'be',\n",
       " 'determined',\n",
       " 'no',\n",
       " 'injury',\n",
       " 'or',\n",
       " 'medical',\n",
       " 'intervention',\n",
       " 'was',\n",
       " 'reported']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['NOPUNCT_TEXT'][verification_row]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c01067-455b-44af-95ca-93854f5cc469",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cebbe10-e09c-4aa5-917d-5159fb44be56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [reported, transmitter, lost, connection, pump...\n",
       "1    [reported, signal, loss, one, hour, occurred, ...\n",
       "2    [reported, transmitter, failed, error, occurre...\n",
       "3    [reported, signal, loss, one, hour, occurred, ...\n",
       "4    [reported, signal, loss, one, hour, occurred, ...\n",
       "Name: NOSTOPWORDS_TEXT, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Define a function to convert to lowercase and remove stopwords\n",
    "def remove_stopwords(tokenized_text):\n",
    "    text = [word for word in tokenized_text if word.lower() not in stopwords]\n",
    "    return text\n",
    "\n",
    "df['NOSTOPWORDS_TEXT'] = df['NOPUNCT_TEXT'].apply(lambda x: remove_stopwords(x))\n",
    "df['NOSTOPWORDS_TEXT'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06cf0ce2-c8fc-4376-86d3-19c9dcdb31ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reported',\n",
       " 'signal',\n",
       " 'loss',\n",
       " 'one',\n",
       " 'hour',\n",
       " 'occurred',\n",
       " 'product',\n",
       " 'data',\n",
       " 'provided',\n",
       " 'evaluation',\n",
       " 'confirmation',\n",
       " 'allegation',\n",
       " 'probable',\n",
       " 'could',\n",
       " 'determined',\n",
       " 'injury',\n",
       " 'medical',\n",
       " 'intervention',\n",
       " 'reported']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['NOSTOPWORDS_TEXT'][verification_row]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f671ef-1c12-4e60-a58e-9007e4ace32a",
   "metadata": {},
   "source": [
    "## 4. Remove Words Starting with a Digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "202dfd97-3eb9-4bf8-8cc2-3762a0bf8ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [reported, transmitter, lost, connection, pump...\n",
       "1    [reported, signal, loss, one, hour, occurred, ...\n",
       "2    [reported, transmitter, failed, error, occurre...\n",
       "3    [reported, signal, loss, one, hour, occurred, ...\n",
       "4    [reported, signal, loss, one, hour, occurred, ...\n",
       "Name: NODIGITS_TEXT, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# create a regular expression pattern to match words that start with numbers\n",
    "pattern = re.compile(r'^\\d+')\n",
    "\n",
    "# Define a function to convert to lowercase and remove stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    text = [word for word in tokens if not pattern.match(word)]\n",
    "    return text\n",
    "\n",
    "df['NODIGITS_TEXT'] = df['NOSTOPWORDS_TEXT'].apply(lambda x: remove_stopwords(x))\n",
    "df['NODIGITS_TEXT'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c663980d-2329-4dfe-ae08-5a5057fc2185",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reported',\n",
       " 'signal',\n",
       " 'loss',\n",
       " 'one',\n",
       " 'hour',\n",
       " 'occurred',\n",
       " 'product',\n",
       " 'data',\n",
       " 'provided',\n",
       " 'evaluation',\n",
       " 'confirmation',\n",
       " 'allegation',\n",
       " 'probable',\n",
       " 'could',\n",
       " 'determined',\n",
       " 'injury',\n",
       " 'medical',\n",
       " 'intervention',\n",
       " 'reported']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['NODIGITS_TEXT'][verification_row]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1fe705-642f-44fd-952b-90011bdb4a06",
   "metadata": {},
   "source": [
    "## X. Word Frequency Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36a4a4da-960f-4cfb-ac35-928c7937e443",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reported</td>\n",
       "      <td>9176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>determined</td>\n",
       "      <td>4213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>occurred</td>\n",
       "      <td>3974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>probable</td>\n",
       "      <td>3963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>allegation</td>\n",
       "      <td>3932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1060</th>\n",
       "      <td>treatment</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>burn</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1062</th>\n",
       "      <td>ointment</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>hydrocortisone</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>analysis</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1065 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Word  Frequency\n",
       "0           reported       9176\n",
       "1         determined       4213\n",
       "2           occurred       3974\n",
       "3           probable       3963\n",
       "4         allegation       3932\n",
       "...              ...        ...\n",
       "1060       treatment          1\n",
       "1061            burn          1\n",
       "1062        ointment          1\n",
       "1063  hydrocortisone          1\n",
       "1064        analysis          1\n",
       "\n",
       "[1065 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explode the lists into separate rows\n",
    "exploded_df = df.explode('NODIGITS_TEXT')\n",
    "word_freq = exploded_df['NODIGITS_TEXT'].value_counts()\n",
    "\n",
    "# Create a DataFrame from the word frequency data\n",
    "freq_df = pd.DataFrame({'Word': word_freq.index, 'Frequency': word_freq.values})\n",
    "freq_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c213aa-997b-4371-8dd9-c6b5f5dfe07e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Parts of Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e318ac0-d7d2-406b-b2df-6cb861548bd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [(reported, VBN), (transmitter, NN), (lost, VB...\n",
       "1    [(reported, VBN), (signal, JJ), (loss, NN), (o...\n",
       "2    [(reported, VBN), (transmitter, NN), (failed, ...\n",
       "3    [(reported, VBN), (signal, JJ), (loss, NN), (o...\n",
       "4    [(reported, VBN), (signal, JJ), (loss, NN), (o...\n",
       "Name: POS_TEXT, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nltk.pos_tag() function to each row of the TOKENIZED_TEXT column\n",
    "# pos_tag returns a Tuple for each word consisting of the word and its classification\n",
    "# TODO: List classifications and their abbreviations\n",
    "df['POS_TEXT'] = df['NODIGITS_TEXT'].apply(nltk.pos_tag)\n",
    "df['POS_TEXT'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12140ece-4515-4637-8efe-dfb3cfa8cf3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reported', 'VBN'),\n",
       " ('signal', 'JJ'),\n",
       " ('loss', 'NN'),\n",
       " ('one', 'CD'),\n",
       " ('hour', 'NN'),\n",
       " ('occurred', 'VBD'),\n",
       " ('product', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " ('provided', 'VBD'),\n",
       " ('evaluation', 'NN'),\n",
       " ('confirmation', 'NN'),\n",
       " ('allegation', 'NN'),\n",
       " ('probable', 'NN'),\n",
       " ('could', 'MD'),\n",
       " ('determined', 'VB'),\n",
       " ('injury', 'VB'),\n",
       " ('medical', 'JJ'),\n",
       " ('intervention', 'NN'),\n",
       " ('reported', 'VBD')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['POS_TEXT'][verification_row]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462e6d3f-d2a6-414b-ae7a-55481ba8d5fe",
   "metadata": {},
   "source": [
    "## 6. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "034f05dc-490e-4692-b1cb-2445732f80d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [report, transmitter, lose, connection, pump, ...\n",
       "1    [report, signal, loss, one, hour, occur, produ...\n",
       "2    [report, transmitter, fail, error, occur, data...\n",
       "3    [report, signal, loss, one, hour, occur, revie...\n",
       "4    [report, signal, loss, one, hour, occur, produ...\n",
       "Name: LEMMATIZED_TEXT, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# define a function to lemmatize each word in a text list based on its POS tag\n",
    "def lemmatize_text(pos_tagged_text):\n",
    "    # initialize WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # map NLTK's POS tags to WordNet's POS tags\n",
    "    # TODO: list the abbreviations for WordNet's parts of speech\n",
    "    pos_map = {'N': 'n', 'V': 'v', 'R': 'r', 'J': 'a'}\n",
    "    \n",
    "    # lemmatize each word in the text list based on its POS tag\n",
    "    lemmatized_text = []\n",
    "    \n",
    "    for word, pos in pos_tagged_text:\n",
    "        \n",
    "        # get the first character of the POS tag to use as the WordNet POS tag\n",
    "        # \n",
    "        # Set the WordNetLemmatizer default to Nouns ('n') or Verbs ('v')\n",
    "        #\n",
    "        wn_pos = pos_map.get(pos[0], 'n') \n",
    "        \n",
    "        # lemmatize the word and append it to the lemmatized text list\n",
    "        lemmatized_word = lemmatizer.lemmatize(word, pos=wn_pos)\n",
    "        lemmatized_text.append(lemmatized_word)\n",
    "    \n",
    "    # return the lemmatized text list\n",
    "    return lemmatized_text\n",
    "\n",
    "# apply the lemmatize_text function to each row of the dataframe\n",
    "df['LEMMATIZED_TEXT'] = df['POS_TEXT'].apply(lemmatize_text)\n",
    "df['LEMMATIZED_TEXT'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66e4cfe-298d-4186-ab98-74848658af20",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6db2894c-4f23-4d9c-9b0f-ef344c9db60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [report, transmitt, lost, connect, pump, great...\n",
       "1    [report, signal, loss, one, hour, occur, produ...\n",
       "2    [report, transmitt, fail, error, occur, data, ...\n",
       "3    [report, signal, loss, one, hour, occur, revie...\n",
       "4    [report, signal, loss, one, hour, occur, produ...\n",
       "Name: STEMMED_TEXT, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# define a function to stem each word in a text list\n",
    "def stem_words(pos_tagged_text):\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    stemmed_text = []\n",
    "    \n",
    "    for word, pos in pos_tagged_text:\n",
    "        # stem the word and append it to the stemmed text list\n",
    "        stemmed_word = stemmer.stem(word)\n",
    "        stemmed_text.append(stemmed_word)\n",
    "    \n",
    "    # return the stemmed text list\n",
    "    return stemmed_text\n",
    "\n",
    "df['STEMMED_TEXT'] = df['POS_TEXT'].apply(stem_words)\n",
    "df['STEMMED_TEXT'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23bed8a-188b-4b6d-ba1b-6d17911dec96",
   "metadata": {},
   "source": [
    "## Compare the results of lemmatization and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9f7eda1-d616-419f-990c-ec13f885ea56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d3b4d_ th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_d3b4d_row0_col0, #T_d3b4d_row0_col1, #T_d3b4d_row0_col2, #T_d3b4d_row1_col0, #T_d3b4d_row1_col1, #T_d3b4d_row1_col2, #T_d3b4d_row2_col0, #T_d3b4d_row2_col1, #T_d3b4d_row2_col2, #T_d3b4d_row3_col0, #T_d3b4d_row3_col1, #T_d3b4d_row3_col2, #T_d3b4d_row4_col0, #T_d3b4d_row4_col1, #T_d3b4d_row4_col2, #T_d3b4d_row5_col0, #T_d3b4d_row5_col1, #T_d3b4d_row5_col2, #T_d3b4d_row6_col0, #T_d3b4d_row6_col1, #T_d3b4d_row6_col2, #T_d3b4d_row7_col0, #T_d3b4d_row7_col1, #T_d3b4d_row7_col2, #T_d3b4d_row8_col0, #T_d3b4d_row8_col1, #T_d3b4d_row8_col2, #T_d3b4d_row9_col0, #T_d3b4d_row9_col1, #T_d3b4d_row9_col2, #T_d3b4d_row10_col0, #T_d3b4d_row10_col1, #T_d3b4d_row10_col2, #T_d3b4d_row11_col0, #T_d3b4d_row11_col1, #T_d3b4d_row11_col2, #T_d3b4d_row12_col0, #T_d3b4d_row12_col1, #T_d3b4d_row12_col2, #T_d3b4d_row13_col0, #T_d3b4d_row13_col1, #T_d3b4d_row13_col2, #T_d3b4d_row14_col0, #T_d3b4d_row14_col1, #T_d3b4d_row14_col2, #T_d3b4d_row15_col0, #T_d3b4d_row15_col1, #T_d3b4d_row15_col2, #T_d3b4d_row16_col0, #T_d3b4d_row16_col1, #T_d3b4d_row16_col2, #T_d3b4d_row17_col0, #T_d3b4d_row17_col1, #T_d3b4d_row17_col2, #T_d3b4d_row18_col0, #T_d3b4d_row18_col1, #T_d3b4d_row18_col2 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d3b4d_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >WORD, PART OF SPEECH</th>\n",
       "      <th class=\"col_heading level0 col1\" >LEMMA</th>\n",
       "      <th class=\"col_heading level0 col2\" >STEM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_d3b4d_row0_col0\" class=\"data row0 col0\" >('reported', 'VBN')</td>\n",
       "      <td id=\"T_d3b4d_row0_col1\" class=\"data row0 col1\" >report</td>\n",
       "      <td id=\"T_d3b4d_row0_col2\" class=\"data row0 col2\" >report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_d3b4d_row1_col0\" class=\"data row1 col0\" >('signal', 'JJ')</td>\n",
       "      <td id=\"T_d3b4d_row1_col1\" class=\"data row1 col1\" >signal</td>\n",
       "      <td id=\"T_d3b4d_row1_col2\" class=\"data row1 col2\" >signal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_d3b4d_row2_col0\" class=\"data row2 col0\" >('loss', 'NN')</td>\n",
       "      <td id=\"T_d3b4d_row2_col1\" class=\"data row2 col1\" >loss</td>\n",
       "      <td id=\"T_d3b4d_row2_col2\" class=\"data row2 col2\" >loss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_d3b4d_row3_col0\" class=\"data row3 col0\" >('one', 'CD')</td>\n",
       "      <td id=\"T_d3b4d_row3_col1\" class=\"data row3 col1\" >one</td>\n",
       "      <td id=\"T_d3b4d_row3_col2\" class=\"data row3 col2\" >one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_d3b4d_row4_col0\" class=\"data row4 col0\" >('hour', 'NN')</td>\n",
       "      <td id=\"T_d3b4d_row4_col1\" class=\"data row4 col1\" >hour</td>\n",
       "      <td id=\"T_d3b4d_row4_col2\" class=\"data row4 col2\" >hour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_d3b4d_row5_col0\" class=\"data row5 col0\" >('occurred', 'VBD')</td>\n",
       "      <td id=\"T_d3b4d_row5_col1\" class=\"data row5 col1\" >occur</td>\n",
       "      <td id=\"T_d3b4d_row5_col2\" class=\"data row5 col2\" >occur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_d3b4d_row6_col0\" class=\"data row6 col0\" >('product', 'NN')</td>\n",
       "      <td id=\"T_d3b4d_row6_col1\" class=\"data row6 col1\" >product</td>\n",
       "      <td id=\"T_d3b4d_row6_col2\" class=\"data row6 col2\" >product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_d3b4d_row7_col0\" class=\"data row7 col0\" >('data', 'NNS')</td>\n",
       "      <td id=\"T_d3b4d_row7_col1\" class=\"data row7 col1\" >data</td>\n",
       "      <td id=\"T_d3b4d_row7_col2\" class=\"data row7 col2\" >data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_d3b4d_row8_col0\" class=\"data row8 col0\" >('provided', 'VBD')</td>\n",
       "      <td id=\"T_d3b4d_row8_col1\" class=\"data row8 col1\" >provide</td>\n",
       "      <td id=\"T_d3b4d_row8_col2\" class=\"data row8 col2\" >provid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_d3b4d_row9_col0\" class=\"data row9 col0\" >('evaluation', 'NN')</td>\n",
       "      <td id=\"T_d3b4d_row9_col1\" class=\"data row9 col1\" >evaluation</td>\n",
       "      <td id=\"T_d3b4d_row9_col2\" class=\"data row9 col2\" >evalu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_d3b4d_row10_col0\" class=\"data row10 col0\" >('confirmation', 'NN')</td>\n",
       "      <td id=\"T_d3b4d_row10_col1\" class=\"data row10 col1\" >confirmation</td>\n",
       "      <td id=\"T_d3b4d_row10_col2\" class=\"data row10 col2\" >confirm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_d3b4d_row11_col0\" class=\"data row11 col0\" >('allegation', 'NN')</td>\n",
       "      <td id=\"T_d3b4d_row11_col1\" class=\"data row11 col1\" >allegation</td>\n",
       "      <td id=\"T_d3b4d_row11_col2\" class=\"data row11 col2\" >alleg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_d3b4d_row12_col0\" class=\"data row12 col0\" >('probable', 'NN')</td>\n",
       "      <td id=\"T_d3b4d_row12_col1\" class=\"data row12 col1\" >probable</td>\n",
       "      <td id=\"T_d3b4d_row12_col2\" class=\"data row12 col2\" >probabl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_d3b4d_row13_col0\" class=\"data row13 col0\" >('could', 'MD')</td>\n",
       "      <td id=\"T_d3b4d_row13_col1\" class=\"data row13 col1\" >could</td>\n",
       "      <td id=\"T_d3b4d_row13_col2\" class=\"data row13 col2\" >could</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_d3b4d_row14_col0\" class=\"data row14 col0\" >('determined', 'VB')</td>\n",
       "      <td id=\"T_d3b4d_row14_col1\" class=\"data row14 col1\" >determine</td>\n",
       "      <td id=\"T_d3b4d_row14_col2\" class=\"data row14 col2\" >determin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_d3b4d_row15_col0\" class=\"data row15 col0\" >('injury', 'VB')</td>\n",
       "      <td id=\"T_d3b4d_row15_col1\" class=\"data row15 col1\" >injury</td>\n",
       "      <td id=\"T_d3b4d_row15_col2\" class=\"data row15 col2\" >injuri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_d3b4d_row16_col0\" class=\"data row16 col0\" >('medical', 'JJ')</td>\n",
       "      <td id=\"T_d3b4d_row16_col1\" class=\"data row16 col1\" >medical</td>\n",
       "      <td id=\"T_d3b4d_row16_col2\" class=\"data row16 col2\" >medic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_d3b4d_row17_col0\" class=\"data row17 col0\" >('intervention', 'NN')</td>\n",
       "      <td id=\"T_d3b4d_row17_col1\" class=\"data row17 col1\" >intervention</td>\n",
       "      <td id=\"T_d3b4d_row17_col2\" class=\"data row17 col2\" >intervent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_d3b4d_row18_col0\" class=\"data row18 col0\" >('reported', 'VBD')</td>\n",
       "      <td id=\"T_d3b4d_row18_col1\" class=\"data row18 col1\" >report</td>\n",
       "      <td id=\"T_d3b4d_row18_col2\" class=\"data row18 col2\" >report</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f9f4db75650>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_lemma_stem_df = pd.DataFrame({\n",
    "    'WORD, PART OF SPEECH': df['POS_TEXT'][verification_row], \n",
    "    'LEMMA': df['LEMMATIZED_TEXT'][verification_row], \n",
    "    'STEM': df['STEMMED_TEXT'][verification_row]\n",
    "})\n",
    "\n",
    "compare_lemma_stem_df = compare_lemma_stem_df.style.set_properties(**{'text-align': 'left'})\n",
    "compare_lemma_stem_df = compare_lemma_stem_df.set_table_styles([dict(selector = 'th', props=[('text-align', 'left')])])\n",
    "compare_lemma_stem_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3080c448-ef79-4bfa-bf12-884a043b4eb2",
   "metadata": {},
   "source": [
    "## 8. Create Bag of Words (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb023fab-c436-4827-a86a-6b15bd24bfe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 7. Create Bag of Words (BOW)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create a CountVectorizer object\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# fit the vectorizer to the text data\n",
    "count_vectorizer.fit(df['LEMMATIZED_TEXT'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# create a bag of words matrix\n",
    "bow_matrix = count_vectorizer.transform(df['LEMMATIZED_TEXT'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# convert the bag of words matrix to a DataFrame\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=count_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fdabd803-b610-47a5-95dc-3d83c0be0d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5736, 922)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b63a7c3b-84bd-4c92-a79b-2d1b00ce7c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abbott</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>abdominal</th>\n",
       "      <th>aberration</th>\n",
       "      <th>able</th>\n",
       "      <th>accessory</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accurate</th>\n",
       "      <th>acetaminophen</th>\n",
       "      <th>actually</th>\n",
       "      <th>...</th>\n",
       "      <th>work</th>\n",
       "      <th>would</th>\n",
       "      <th>x2</th>\n",
       "      <th>xray</th>\n",
       "      <th>year</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>zero</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 922 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abbott  abdomen  abdominal  aberration  able  accessory  accuracy  \\\n",
       "0       0        0          0           0     0          0         0   \n",
       "1       0        0          0           0     0          0         0   \n",
       "2       0        0          0           0     0          0         0   \n",
       "3       0        0          0           0     0          0         0   \n",
       "4       0        0          0           0     0          0         0   \n",
       "\n",
       "   accurate  acetaminophen  actually  ...  work  would  x2  xray  year  \\\n",
       "0         0              0         0  ...     0      0   0     0     0   \n",
       "1         0              0         0  ...     0      0   0     0     0   \n",
       "2         0              0         0  ...     0      0   0     0     0   \n",
       "3         0              0         0  ...     0      0   0     0     0   \n",
       "4         0              0         0  ...     0      0   0     0     0   \n",
       "\n",
       "   yellow  yes  yet  zero  zone  \n",
       "0       0    0    0     0     0  \n",
       "1       0    0    0     0     0  \n",
       "2       0    0    0     0     0  \n",
       "3       0    0    0     0     0  \n",
       "4       0    0    0     0     0  \n",
       "\n",
       "[5 rows x 922 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_df.head()\n",
    "# TODO: Plot the BOW results (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a68c5c-55bc-4559-af40-e8ae3ad76d5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 9. Calculate Term Frequency-Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fc601b8-7e4a-4a15-aa80-7017721a7892",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 8. Calculate Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create a CountVectorizer object and fit it to the text data\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "X = tfidf_vectorizer.fit_transform(df['LEMMATIZED_TEXT'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# convert the sparse matrix to a DataFrame\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc7dc764-c475-4da1-9a5e-62d3a103688a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5736, 922)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ced8af5a-9c8a-4e5f-8bf6-c4d0b9a2ab68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abbott</th>\n",
       "      <th>abdomen</th>\n",
       "      <th>abdominal</th>\n",
       "      <th>aberration</th>\n",
       "      <th>able</th>\n",
       "      <th>accessory</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>accurate</th>\n",
       "      <th>acetaminophen</th>\n",
       "      <th>actually</th>\n",
       "      <th>...</th>\n",
       "      <th>work</th>\n",
       "      <th>would</th>\n",
       "      <th>x2</th>\n",
       "      <th>xray</th>\n",
       "      <th>year</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>zero</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 922 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abbott  abdomen  abdominal  aberration  able  accessory  accuracy  \\\n",
       "0     0.0      0.0        0.0         0.0   0.0        0.0       0.0   \n",
       "1     0.0      0.0        0.0         0.0   0.0        0.0       0.0   \n",
       "2     0.0      0.0        0.0         0.0   0.0        0.0       0.0   \n",
       "3     0.0      0.0        0.0         0.0   0.0        0.0       0.0   \n",
       "4     0.0      0.0        0.0         0.0   0.0        0.0       0.0   \n",
       "\n",
       "   accurate  acetaminophen  actually  ...  work  would   x2  xray  year  \\\n",
       "0       0.0            0.0       0.0  ...   0.0    0.0  0.0   0.0   0.0   \n",
       "1       0.0            0.0       0.0  ...   0.0    0.0  0.0   0.0   0.0   \n",
       "2       0.0            0.0       0.0  ...   0.0    0.0  0.0   0.0   0.0   \n",
       "3       0.0            0.0       0.0  ...   0.0    0.0  0.0   0.0   0.0   \n",
       "4       0.0            0.0       0.0  ...   0.0    0.0  0.0   0.0   0.0   \n",
       "\n",
       "   yellow  yes  yet  zero  zone  \n",
       "0     0.0  0.0  0.0   0.0   0.0  \n",
       "1     0.0  0.0  0.0   0.0   0.0  \n",
       "2     0.0  0.0  0.0   0.0   0.0  \n",
       "3     0.0  0.0  0.0   0.0   0.0  \n",
       "4     0.0  0.0  0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 922 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0e2ab0-8c85-4795-868a-603b77154cca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 10. Sentencize\n",
    "The `FOI_TEXT` can be processed as sentences.\n",
    "\n",
    "For further analysis, each sentence needs to be associated with the `FOI_TEXT` row that it came from.\n",
    "\n",
    "[This discussion from Stack Overflow](https://stackoverflow.com/a/43922444/2308522) provides a suggestion for breaking the code into a dataframe of sentences with each sentence retaining the ID of the row where it was originally located.\n",
    "\n",
    "[This page from the Pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.itertuples.html) provides details on using the `itertuples()` function to process the rows of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a577254-5ecb-4a76-8d42-3021092ca551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d3b4d_ th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_d3b4d_row0_col0, #T_d3b4d_row0_col1, #T_d3b4d_row0_col2, #T_d3b4d_row1_col0, #T_d3b4d_row1_col1, #T_d3b4d_row1_col2, #T_d3b4d_row2_col0, #T_d3b4d_row2_col1, #T_d3b4d_row2_col2, #T_d3b4d_row3_col0, #T_d3b4d_row3_col1, #T_d3b4d_row3_col2, #T_d3b4d_row4_col0, #T_d3b4d_row4_col1, #T_d3b4d_row4_col2, #T_d3b4d_row5_col0, #T_d3b4d_row5_col1, #T_d3b4d_row5_col2, #T_d3b4d_row6_col0, #T_d3b4d_row6_col1, #T_d3b4d_row6_col2, #T_d3b4d_row7_col0, #T_d3b4d_row7_col1, #T_d3b4d_row7_col2, #T_d3b4d_row8_col0, #T_d3b4d_row8_col1, #T_d3b4d_row8_col2, #T_d3b4d_row9_col0, #T_d3b4d_row9_col1, #T_d3b4d_row9_col2, #T_d3b4d_row10_col0, #T_d3b4d_row10_col1, #T_d3b4d_row10_col2, #T_d3b4d_row11_col0, #T_d3b4d_row11_col1, #T_d3b4d_row11_col2, #T_d3b4d_row12_col0, #T_d3b4d_row12_col1, #T_d3b4d_row12_col2, #T_d3b4d_row13_col0, #T_d3b4d_row13_col1, #T_d3b4d_row13_col2, #T_d3b4d_row14_col0, #T_d3b4d_row14_col1, #T_d3b4d_row14_col2, #T_d3b4d_row15_col0, #T_d3b4d_row15_col1, #T_d3b4d_row15_col2, #T_d3b4d_row16_col0, #T_d3b4d_row16_col1, #T_d3b4d_row16_col2, #T_d3b4d_row17_col0, #T_d3b4d_row17_col1, #T_d3b4d_row17_col2, #T_d3b4d_row18_col0, #T_d3b4d_row18_col1, #T_d3b4d_row18_col2 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d3b4d_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >WORD, PART OF SPEECH</th>\n",
       "      <th class=\"col_heading level0 col1\" >LEMMA</th>\n",
       "      <th class=\"col_heading level0 col2\" >STEM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_d3b4d_row0_col0\" class=\"data row0 col0\" >('reported', 'VBN')</td>\n",
       "      <td id=\"T_d3b4d_row0_col1\" class=\"data row0 col1\" >report</td>\n",
       "      <td id=\"T_d3b4d_row0_col2\" class=\"data row0 col2\" >report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_d3b4d_row1_col0\" class=\"data row1 col0\" >('signal', 'JJ')</td>\n",
       "      <td id=\"T_d3b4d_row1_col1\" class=\"data row1 col1\" >signal</td>\n",
       "      <td id=\"T_d3b4d_row1_col2\" class=\"data row1 col2\" >signal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_d3b4d_row2_col0\" class=\"data row2 col0\" >('loss', 'NN')</td>\n",
       "      <td id=\"T_d3b4d_row2_col1\" class=\"data row2 col1\" >loss</td>\n",
       "      <td id=\"T_d3b4d_row2_col2\" class=\"data row2 col2\" >loss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_d3b4d_row3_col0\" class=\"data row3 col0\" >('one', 'CD')</td>\n",
       "      <td id=\"T_d3b4d_row3_col1\" class=\"data row3 col1\" >one</td>\n",
       "      <td id=\"T_d3b4d_row3_col2\" class=\"data row3 col2\" >one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_d3b4d_row4_col0\" class=\"data row4 col0\" >('hour', 'NN')</td>\n",
       "      <td id=\"T_d3b4d_row4_col1\" class=\"data row4 col1\" >hour</td>\n",
       "      <td id=\"T_d3b4d_row4_col2\" class=\"data row4 col2\" >hour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_d3b4d_row5_col0\" class=\"data row5 col0\" >('occurred', 'VBD')</td>\n",
       "      <td id=\"T_d3b4d_row5_col1\" class=\"data row5 col1\" >occur</td>\n",
       "      <td id=\"T_d3b4d_row5_col2\" class=\"data row5 col2\" >occur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_d3b4d_row6_col0\" class=\"data row6 col0\" >('product', 'NN')</td>\n",
       "      <td id=\"T_d3b4d_row6_col1\" class=\"data row6 col1\" >product</td>\n",
       "      <td id=\"T_d3b4d_row6_col2\" class=\"data row6 col2\" >product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_d3b4d_row7_col0\" class=\"data row7 col0\" >('data', 'NNS')</td>\n",
       "      <td id=\"T_d3b4d_row7_col1\" class=\"data row7 col1\" >data</td>\n",
       "      <td id=\"T_d3b4d_row7_col2\" class=\"data row7 col2\" >data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_d3b4d_row8_col0\" class=\"data row8 col0\" >('provided', 'VBD')</td>\n",
       "      <td id=\"T_d3b4d_row8_col1\" class=\"data row8 col1\" >provide</td>\n",
       "      <td id=\"T_d3b4d_row8_col2\" class=\"data row8 col2\" >provid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_d3b4d_row9_col0\" class=\"data row9 col0\" >('evaluation', 'NN')</td>\n",
       "      <td id=\"T_d3b4d_row9_col1\" class=\"data row9 col1\" >evaluation</td>\n",
       "      <td id=\"T_d3b4d_row9_col2\" class=\"data row9 col2\" >evalu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_d3b4d_row10_col0\" class=\"data row10 col0\" >('confirmation', 'NN')</td>\n",
       "      <td id=\"T_d3b4d_row10_col1\" class=\"data row10 col1\" >confirmation</td>\n",
       "      <td id=\"T_d3b4d_row10_col2\" class=\"data row10 col2\" >confirm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_d3b4d_row11_col0\" class=\"data row11 col0\" >('allegation', 'NN')</td>\n",
       "      <td id=\"T_d3b4d_row11_col1\" class=\"data row11 col1\" >allegation</td>\n",
       "      <td id=\"T_d3b4d_row11_col2\" class=\"data row11 col2\" >alleg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_d3b4d_row12_col0\" class=\"data row12 col0\" >('probable', 'NN')</td>\n",
       "      <td id=\"T_d3b4d_row12_col1\" class=\"data row12 col1\" >probable</td>\n",
       "      <td id=\"T_d3b4d_row12_col2\" class=\"data row12 col2\" >probabl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_d3b4d_row13_col0\" class=\"data row13 col0\" >('could', 'MD')</td>\n",
       "      <td id=\"T_d3b4d_row13_col1\" class=\"data row13 col1\" >could</td>\n",
       "      <td id=\"T_d3b4d_row13_col2\" class=\"data row13 col2\" >could</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_d3b4d_row14_col0\" class=\"data row14 col0\" >('determined', 'VB')</td>\n",
       "      <td id=\"T_d3b4d_row14_col1\" class=\"data row14 col1\" >determine</td>\n",
       "      <td id=\"T_d3b4d_row14_col2\" class=\"data row14 col2\" >determin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_d3b4d_row15_col0\" class=\"data row15 col0\" >('injury', 'VB')</td>\n",
       "      <td id=\"T_d3b4d_row15_col1\" class=\"data row15 col1\" >injury</td>\n",
       "      <td id=\"T_d3b4d_row15_col2\" class=\"data row15 col2\" >injuri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_d3b4d_row16_col0\" class=\"data row16 col0\" >('medical', 'JJ')</td>\n",
       "      <td id=\"T_d3b4d_row16_col1\" class=\"data row16 col1\" >medical</td>\n",
       "      <td id=\"T_d3b4d_row16_col2\" class=\"data row16 col2\" >medic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_d3b4d_row17_col0\" class=\"data row17 col0\" >('intervention', 'NN')</td>\n",
       "      <td id=\"T_d3b4d_row17_col1\" class=\"data row17 col1\" >intervention</td>\n",
       "      <td id=\"T_d3b4d_row17_col2\" class=\"data row17 col2\" >intervent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d3b4d_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_d3b4d_row18_col0\" class=\"data row18 col0\" >('reported', 'VBD')</td>\n",
       "      <td id=\"T_d3b4d_row18_col1\" class=\"data row18 col1\" >report</td>\n",
       "      <td id=\"T_d3b4d_row18_col2\" class=\"data row18 col2\" >report</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f9f4db75650>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "# Using itertuples(), the columns must be addressed using thier position.\n",
    "# Here's a map of position to name:\n",
    "# row[1]: ROW_ID\n",
    "# row[2]: FOI_TEXT\n",
    "# row[3]: DEVICE_PROBLEM_CODE\n",
    "# row[4]: DEVICE_PROBLEM_TEXT\n",
    "for row in df.itertuples():\n",
    "    for sentence in row[2].split('.'):\n",
    "        if sentence != '':\n",
    "            sentences.append([row[1], row[3], row[4], sentence])\n",
    "\n",
    "sentences_df = pd.DataFrame(sentences, columns=['ROW_ID', 'DEVICE_PROBLEM_CODE', 'DEVICE_PROBLEM_TEXT','SENTENCIZED_FOI_TEXT'])\n",
    "\n",
    "compare_lemma_stem_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62a27c3d-d6de-47cc-bca1-32e3a716f168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25765, 4)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f4a1a46-8619-400b-84f6-c4ed8003017e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>DEVICE_PROBLEM_CODE</th>\n",
       "      <th>DEVICE_PROBLEM_TEXT</th>\n",
       "      <th>SENTENCIZED_FOI_TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1969025</td>\n",
       "      <td>3283</td>\n",
       "      <td>Wireless Communication Problem</td>\n",
       "      <td>IT WAS REPORTED THAT THE TRANSMITTER LOST CONN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1969025</td>\n",
       "      <td>3283</td>\n",
       "      <td>Wireless Communication Problem</td>\n",
       "      <td>THE TRANSMITTER ULTIMATELY REGAINED CONNECTIO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1969025</td>\n",
       "      <td>3283</td>\n",
       "      <td>Wireless Communication Problem</td>\n",
       "      <td>NO ADDITIONAL PATIENT OR EVENT INFORMATION WA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ROW_ID DEVICE_PROBLEM_CODE             DEVICE_PROBLEM_TEXT  \\\n",
       "0  1969025                3283  Wireless Communication Problem   \n",
       "1  1969025                3283  Wireless Communication Problem   \n",
       "2  1969025                3283  Wireless Communication Problem   \n",
       "\n",
       "                                SENTENCIZED_FOI_TEXT  \n",
       "0  IT WAS REPORTED THAT THE TRANSMITTER LOST CONN...  \n",
       "1   THE TRANSMITTER ULTIMATELY REGAINED CONNECTIO...  \n",
       "2   NO ADDITIONAL PATIENT OR EVENT INFORMATION WA...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6aa46d7-6b15-4539-89c3-acc16c1c6c2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IT WAS REPORTED THAT THE TRANSMITTER LOST CONNECTION WITH THE PUMP FOR GREATER THAN 1 HOUR'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df['SENTENCIZED_FOI_TEXT'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cea1af87-cd7e-4400-bc31-fcc8f3fea5cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand Contractions, Tokenize, and Convert to Lowercase\n",
    "sentences_df['TOKENIZED_SENTENCES'] = sentences_df['SENTENCIZED_FOI_TEXT'].apply(lambda x: [contractions.fix(word).lower() for word in x.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "25d9b7de-2e44-4ecd-b63a-8ade98fe7a3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " 'was',\n",
       " 'reported',\n",
       " 'that',\n",
       " 'the',\n",
       " 'transmitter',\n",
       " 'lost',\n",
       " 'connection',\n",
       " 'with',\n",
       " 'the',\n",
       " 'pump',\n",
       " 'for',\n",
       " 'greater',\n",
       " 'than',\n",
       " '1',\n",
       " 'hour']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df['TOKENIZED_SENTENCES'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "daad689d-e1e4-48ee-aeaa-ef244a0773a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " 'was',\n",
       " 'reported',\n",
       " 'that',\n",
       " 'the',\n",
       " 'transmitter',\n",
       " 'lost',\n",
       " 'connection',\n",
       " 'with',\n",
       " 'the',\n",
       " 'pump',\n",
       " 'for',\n",
       " 'greater',\n",
       " 'than',\n",
       " '1',\n",
       " 'hour']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "sentences_df['NOPUNCT_SENTENCES'] = sentences_df['TOKENIZED_SENTENCES'].apply(lambda x: [remove_punctuation(word) for word in x])\n",
    "sentences_df['NOPUNCT_SENTENCES'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3f2f22a-6f55-4ea6-8382-e97993d5815c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " 'was',\n",
       " 'reported',\n",
       " 'that',\n",
       " 'the',\n",
       " 'transmitter',\n",
       " 'lost',\n",
       " 'connection',\n",
       " 'with',\n",
       " 'the',\n",
       " 'pump',\n",
       " 'for',\n",
       " 'greater',\n",
       " 'than',\n",
       " 'hour']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stop words\n",
    "sentences_df['NOSTOPWORDS_SENTENCES'] = sentences_df['NOPUNCT_SENTENCES'].apply(lambda x: remove_stopwords(x))\n",
    "sentences_df['NOSTOPWORDS_SENTENCES'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e96ddcd3-6711-4a56-ad2c-cd4bf5e84988",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('it', 'PRP'),\n",
       " ('was', 'VBD'),\n",
       " ('reported', 'VBN'),\n",
       " ('that', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('transmitter', 'NN'),\n",
       " ('lost', 'VBD'),\n",
       " ('connection', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('pump', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('greater', 'JJR'),\n",
       " ('than', 'IN'),\n",
       " ('hour', 'NN')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply POS Tagging\n",
    "sentences_df['POS_SENTENCES'] = sentences_df['NOSTOPWORDS_SENTENCES'].apply(nltk.pos_tag)\n",
    "sentences_df['POS_SENTENCES'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f36e88a-a2c6-4e68-98d9-ad4d9d8339ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to join tokens that have been lemmatized and stemmed\n",
    "def join_tokenized_sentence(tokens):\n",
    "    joined_words = []\n",
    "    \n",
    "    for word in tokens:\n",
    "        joined_words.append(word)\n",
    "    \n",
    "    # Join the stemmed words back into a sentence\n",
    "    return ' '.join(joined_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880f8066-5d31-4abf-996d-269eef7fd804",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 10.A Lemmatize Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "538fbb13-a592-4502-abb6-8cb05deeccc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " 'be',\n",
       " 'report',\n",
       " 'that',\n",
       " 'the',\n",
       " 'transmitter',\n",
       " 'lose',\n",
       " 'connection',\n",
       " 'with',\n",
       " 'the',\n",
       " 'pump',\n",
       " 'for',\n",
       " 'great',\n",
       " 'than',\n",
       " 'hour']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df['TOKEN_LEMMATIZED_SENTENCES'] = sentences_df['POS_SENTENCES'].apply(lemmatize_text)\n",
    "sentences_df['TOKEN_LEMMATIZED_SENTENCES'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "589301ef-dcce-434f-829b-23f4ca684dc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it be report that the transmitter lose connection with the pump for great than hour'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df['LEMMATIZED_SENTENCES'] = sentences_df['TOKEN_LEMMATIZED_SENTENCES'].apply(join_tokenized_sentence)\n",
    "sentences_df['LEMMATIZED_SENTENCES'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa224d6e-72ca-4548-8091-82cdb05591d3",
   "metadata": {},
   "source": [
    "### 10.B Stem Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a8b31b8f-b03f-4db2-8f8f-ef2d89c4e450",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " 'wa',\n",
       " 'report',\n",
       " 'that',\n",
       " 'the',\n",
       " 'transmitt',\n",
       " 'lost',\n",
       " 'connect',\n",
       " 'with',\n",
       " 'the',\n",
       " 'pump',\n",
       " 'for',\n",
       " 'greater',\n",
       " 'than',\n",
       " 'hour']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a new column called 'STEMMED_SENTENCES'\n",
    "sentences_df['TOKEN_STEMMED_SENTENCES'] = sentences_df['POS_SENTENCES'].apply(stem_words)\n",
    "sentences_df['TOKEN_STEMMED_SENTENCES'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "13731b1c-2b14-4295-8bc0-132c74d1bac4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it wa report that the transmitt lost connect with the pump for greater than hour'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df['STEMMED_SENTENCES'] = sentences_df['TOKEN_STEMMED_SENTENCES'].apply(join_tokenized_sentence)\n",
    "sentences_df['STEMMED_SENTENCES'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74f580b-740d-4631-9816-37ce859dde28",
   "metadata": {},
   "source": [
    "## Review the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "66c4f4aa-b714-40d5-a097-1b1a4ea51c5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DF COLUMN NAMES</th>\n",
       "      <th>EXAMPLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ROW_ID</td>\n",
       "      <td>1969025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FOI_TEXT</td>\n",
       "      <td>IT WAS REPORTED THAT THE TRANSMITTER LOST CONN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DEVICE_PROBLEM_CODE</td>\n",
       "      <td>3283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DEVICE_PROBLEM_TEXT</td>\n",
       "      <td>Wireless Communication Problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GENERIC_NAME</td>\n",
       "      <td>CONTINUOUS GLUCOSE MONITOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>DEVICE_REPORT_PRODUCT_CODE</td>\n",
       "      <td>QBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UDI-DI</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>UDI-PUBLIC</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DATE_OF_EVENT</td>\n",
       "      <td>07/30/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>REPORTER_OCCUPATION_CODE</td>\n",
       "      <td>000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>REPORT_DATE</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>EVENT_LOCATION</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SOURCE_TYPE</td>\n",
       "      <td>CONSUMER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TOKENIZED_TEXT</td>\n",
       "      <td>[it, was, reported, that, the, transmitter, lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NOPUNCT_TEXT</td>\n",
       "      <td>[it, was, reported, that, the, transmitter, lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NOSTOPWORDS_TEXT</td>\n",
       "      <td>[reported, transmitter, lost, connection, pump...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NODIGITS_TEXT</td>\n",
       "      <td>[reported, transmitter, lost, connection, pump...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>POS_TEXT</td>\n",
       "      <td>[(reported, VBN), (transmitter, NN), (lost, VB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LEMMATIZED_TEXT</td>\n",
       "      <td>[report, transmitter, lose, connection, pump, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>STEMMED_TEXT</td>\n",
       "      <td>[report, transmitt, lost, connect, pump, great...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               DF COLUMN NAMES  \\\n",
       "0                       ROW_ID   \n",
       "1                     FOI_TEXT   \n",
       "2          DEVICE_PROBLEM_CODE   \n",
       "3          DEVICE_PROBLEM_TEXT   \n",
       "4                 GENERIC_NAME   \n",
       "5   DEVICE_REPORT_PRODUCT_CODE   \n",
       "6                       UDI-DI   \n",
       "7                   UDI-PUBLIC   \n",
       "8                DATE_OF_EVENT   \n",
       "9     REPORTER_OCCUPATION_CODE   \n",
       "10                 REPORT_DATE   \n",
       "11              EVENT_LOCATION   \n",
       "12                 SOURCE_TYPE   \n",
       "13              TOKENIZED_TEXT   \n",
       "14                NOPUNCT_TEXT   \n",
       "15            NOSTOPWORDS_TEXT   \n",
       "16               NODIGITS_TEXT   \n",
       "17                    POS_TEXT   \n",
       "18             LEMMATIZED_TEXT   \n",
       "19                STEMMED_TEXT   \n",
       "\n",
       "                                              EXAMPLE  \n",
       "0                                             1969025  \n",
       "1   IT WAS REPORTED THAT THE TRANSMITTER LOST CONN...  \n",
       "2                                                3283  \n",
       "3                      Wireless Communication Problem  \n",
       "4                          CONTINUOUS GLUCOSE MONITOR  \n",
       "5                                                 QBJ  \n",
       "6                                                      \n",
       "7                                                      \n",
       "8                                          07/30/2020  \n",
       "9                                                 000  \n",
       "10                                                     \n",
       "11                                                  I  \n",
       "12                                           CONSUMER  \n",
       "13  [it, was, reported, that, the, transmitter, lo...  \n",
       "14  [it, was, reported, that, the, transmitter, lo...  \n",
       "15  [reported, transmitter, lost, connection, pump...  \n",
       "16  [reported, transmitter, lost, connection, pump...  \n",
       "17  [(reported, VBN), (transmitter, NN), (lost, VB...  \n",
       "18  [report, transmitter, lose, connection, pump, ...  \n",
       "19  [report, transmitt, lost, connect, pump, great...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new dataframe with just one row containing the column names\n",
    "column_names_df = pd.DataFrame({\n",
    "    'DF COLUMN NAMES': df.columns,\n",
    "})\n",
    "\n",
    "example = []\n",
    "\n",
    "for col in df.columns:\n",
    "    example.append(df[col][0])\n",
    "        \n",
    "column_names_df['EXAMPLE'] = example\n",
    "column_names_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "95aba4e3-0454-46c6-a73a-f4f644d567d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SENTENCES DF COLUMN NAMES</th>\n",
       "      <th>EXAMPLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ROW_ID</td>\n",
       "      <td>1969025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DEVICE_PROBLEM_CODE</td>\n",
       "      <td>3283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DEVICE_PROBLEM_TEXT</td>\n",
       "      <td>Wireless Communication Problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SENTENCIZED_FOI_TEXT</td>\n",
       "      <td>IT WAS REPORTED THAT THE TRANSMITTER LOST CONN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TOKENIZED_SENTENCES</td>\n",
       "      <td>[it, was, reported, that, the, transmitter, lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NOPUNCT_SENTENCES</td>\n",
       "      <td>[it, was, reported, that, the, transmitter, lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NOSTOPWORDS_SENTENCES</td>\n",
       "      <td>[it, was, reported, that, the, transmitter, lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>POS_SENTENCES</td>\n",
       "      <td>[(it, PRP), (was, VBD), (reported, VBN), (that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>TOKEN_LEMMATIZED_SENTENCES</td>\n",
       "      <td>[it, be, report, that, the, transmitter, lose,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LEMMATIZED_SENTENCES</td>\n",
       "      <td>it be report that the transmitter lose connect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TOKEN_STEMMED_SENTENCES</td>\n",
       "      <td>[it, wa, report, that, the, transmitt, lost, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>STEMMED_SENTENCES</td>\n",
       "      <td>it wa report that the transmitt lost connect w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     SENTENCES DF COLUMN NAMES  \\\n",
       "0                       ROW_ID   \n",
       "1          DEVICE_PROBLEM_CODE   \n",
       "2          DEVICE_PROBLEM_TEXT   \n",
       "3         SENTENCIZED_FOI_TEXT   \n",
       "4          TOKENIZED_SENTENCES   \n",
       "5            NOPUNCT_SENTENCES   \n",
       "6        NOSTOPWORDS_SENTENCES   \n",
       "7                POS_SENTENCES   \n",
       "8   TOKEN_LEMMATIZED_SENTENCES   \n",
       "9         LEMMATIZED_SENTENCES   \n",
       "10     TOKEN_STEMMED_SENTENCES   \n",
       "11           STEMMED_SENTENCES   \n",
       "\n",
       "                                              EXAMPLE  \n",
       "0                                             1969025  \n",
       "1                                                3283  \n",
       "2                      Wireless Communication Problem  \n",
       "3   IT WAS REPORTED THAT THE TRANSMITTER LOST CONN...  \n",
       "4   [it, was, reported, that, the, transmitter, lo...  \n",
       "5   [it, was, reported, that, the, transmitter, lo...  \n",
       "6   [it, was, reported, that, the, transmitter, lo...  \n",
       "7   [(it, PRP), (was, VBD), (reported, VBN), (that...  \n",
       "8   [it, be, report, that, the, transmitter, lose,...  \n",
       "9   it be report that the transmitter lose connect...  \n",
       "10  [it, wa, report, that, the, transmitt, lost, c...  \n",
       "11  it wa report that the transmitt lost connect w...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new dataframe with just one row containing the column names\n",
    "column_names_df = pd.DataFrame({\n",
    "    'SENTENCES DF COLUMN NAMES': sentences_df.columns,\n",
    "})\n",
    "\n",
    "example = []\n",
    "\n",
    "for col in sentences_df.columns:\n",
    "    example.append(sentences_df[col][0])\n",
    "\n",
    "column_names_df['EXAMPLE'] = example\n",
    "column_names_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13143c09-f9cd-4c01-b7b7-5e3994aa21e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save the preproecssed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "98e853ae-ef27-4d92-a71a-b09efd9f101b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv(f\"{working_directory}/preprocessed_data.csv\", index=False)\n",
    "\n",
    "bow_df.to_csv(f\"{working_directory}/bag_of_words_data.csv\", index=False)\n",
    "\n",
    "tfidf_df.to_csv(f\"{working_directory}/tfidf_data.csv\", index=False)\n",
    "\n",
    "sentences_df.to_csv(f\"{working_directory}/sentences_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc2654d-14e8-4709-8289-9922127f9f88",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Upload All Output to an S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9b03bb42-336b-406d-84d2-dbda67a93f9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: 21-Preprocess-Combined-Data-v2/preprocessed_data.csv to s3://praxis-2023-html-output/preprocessed_data.csv\n",
      "upload: 21-Preprocess-Combined-Data-v2/dataframe.pickle to s3://praxis-2023-html-output/dataframe.pickle\n",
      "upload: 21-Preprocess-Combined-Data-v2/sentences_data.csv to s3://praxis-2023-html-output/sentences_data.csv\n",
      "upload: 21-Preprocess-Combined-Data-v2/tfidf_data.csv to s3://praxis-2023-html-output/tfidf_data.csv\n",
      "upload: 21-Preprocess-Combined-Data-v2/bag_of_words_data.csv to s3://praxis-2023-html-output/bag_of_words_data.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Create the upload command using the AWS command line interface\n",
    "command = [\"aws\", \"s3\", \"sync\", working_directory, \n",
    "           f\"s3://praxis-2023-html-output/\", \"--exclude\", f\"*/.ipynb_checkpoints/*\", \"--no-progress\"]\n",
    "\n",
    "# Run the command and wait for it to complete\n",
    "output = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "# Print the command's output\n",
    "print(output.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd1d2b88-dc2a-4829-acdb-d270073299d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Assume `df` is the dataframe you want to save\n",
    "with open(f\"{working_directory}/dataframe.pickle\", 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cfe24156-2f48-4ede-bb5f-1b0a19e59ae2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fin\n"
     ]
    }
   ],
   "source": [
    "print('fin')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p37",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
