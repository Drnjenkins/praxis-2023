{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44abeade-5fe3-40d6-99cb-621c66badf2e",
   "metadata": {},
   "source": [
    "# Preprocess Combined Data\n",
    "Using the combined QBJ as input, perform data preprocessing including:\n",
    "\n",
    "1. Expand Contractions, Tokenize, and Convert to Lowercase\n",
    "1. Remove Punctuation\n",
    "1. Remove Stop Words\n",
    "1. Parts of Speech (POS) Tagging\n",
    "1. Lemmatize\n",
    "1. Stemming\n",
    "1. Sentencize\n",
    "1. Create Bag of Words (BOW)\n",
    "1. Calculate Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "_Run [Notebook 07-Install-NLTK](./07-Install-NLTK.ipynb) as needed to set up NLTK._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a5cb8b5-2419-4a10-95ef-b98e00fa8fb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Identify the working directory and data files\n",
    "working_directory = './16-data_preprocessing'\n",
    "\n",
    "# Create the working directory if needed\n",
    "try:\n",
    "    os.makedirs(working_directory, exist_ok=True)\n",
    "except OSError as error:\n",
    "    print(f\"Error creating {working_directory}: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b12a20c0-f0d4-47ba-bc75-32ef8cc62c14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Readthe combined data into a dataframe\n",
    "data_file = './15-data_combination/qbj_data_combined.csv'\n",
    "\n",
    "# Read the data into a pandas dataframe\n",
    "df = pd.read_csv(data_file,           # The data file being read, from the variable assignment above\n",
    "                 on_bad_lines='warn', # This tells Pandas to only warn on bad lines vs causing an error\n",
    "                 dtype='str')         # This tells Pandas to treat all numbers as words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6021374b-7c6c-48c7-9319-47b7512b9a0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5736, 13)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85fd9d2d-01a6-45e6-8807-9e27dc01d5d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>FOI_TEXT</th>\n",
       "      <th>DEVICE_PROBLEM_CODE</th>\n",
       "      <th>DEVICE_PROBLEM_TEXT</th>\n",
       "      <th>GENERIC_NAME</th>\n",
       "      <th>DEVICE_REPORT_PRODUCT_CODE</th>\n",
       "      <th>UDI-DI</th>\n",
       "      <th>UDI-PUBLIC</th>\n",
       "      <th>DATE_OF_EVENT</th>\n",
       "      <th>REPORTER_OCCUPATION_CODE</th>\n",
       "      <th>REPORT_DATE</th>\n",
       "      <th>EVENT_LOCATION</th>\n",
       "      <th>SOURCE_TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1969025</td>\n",
       "      <td>IT WAS REPORTED THAT THE TRANSMITTER LOST CONN...</td>\n",
       "      <td>3283</td>\n",
       "      <td>Wireless Communication Problem</td>\n",
       "      <td>CONTINUOUS GLUCOSE MONITOR</td>\n",
       "      <td>QBJ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>07/30/2020</td>\n",
       "      <td>000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I</td>\n",
       "      <td>CONSUMER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1426265</td>\n",
       "      <td>IT WAS REPORTED THAT SIGNAL LOSS OVER ONE HOUR...</td>\n",
       "      <td>3283</td>\n",
       "      <td>Wireless Communication Problem</td>\n",
       "      <td>CONTINUOUS GLUCOSE MONITOR</td>\n",
       "      <td>QBJ</td>\n",
       "      <td>00386270000385</td>\n",
       "      <td>00386270000385</td>\n",
       "      <td>06/05/2020</td>\n",
       "      <td>000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I</td>\n",
       "      <td>CONSUMER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ROW_ID                                           FOI_TEXT  \\\n",
       "0  1969025  IT WAS REPORTED THAT THE TRANSMITTER LOST CONN...   \n",
       "1  1426265  IT WAS REPORTED THAT SIGNAL LOSS OVER ONE HOUR...   \n",
       "\n",
       "  DEVICE_PROBLEM_CODE             DEVICE_PROBLEM_TEXT  \\\n",
       "0                3283  Wireless Communication Problem   \n",
       "1                3283  Wireless Communication Problem   \n",
       "\n",
       "                 GENERIC_NAME DEVICE_REPORT_PRODUCT_CODE          UDI-DI  \\\n",
       "0  CONTINUOUS GLUCOSE MONITOR                        QBJ             NaN   \n",
       "1  CONTINUOUS GLUCOSE MONITOR                        QBJ  00386270000385   \n",
       "\n",
       "       UDI-PUBLIC DATE_OF_EVENT REPORTER_OCCUPATION_CODE REPORT_DATE  \\\n",
       "0             NaN    07/30/2020                      000         NaN   \n",
       "1  00386270000385    06/05/2020                      000         NaN   \n",
       "\n",
       "  EVENT_LOCATION SOURCE_TYPE  \n",
       "0              I    CONSUMER  \n",
       "1              I    CONSUMER  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fd1319-1714-4115-9bb3-07935554919f",
   "metadata": {},
   "source": [
    "## Assign a Row ID for Verification\n",
    "Assign a value to a variable that identifies a row from the dataset.  \n",
    "\n",
    "This will allow the same row to be used for verification of each preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2746674c-c24b-4986-b047-54f45d1beeab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "verification_row = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cccf441-a429-4612-bda4-eafbc2af9ef0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load the Natural Language Toolkit (NLTK) and Preprocessing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cbed954-f2f1-41da-a1e8-3718d6ce413f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the NLTK library\n",
    "import nltk # If this step fails, rerun 07-Install-NLTK.ipynb\n",
    "import string\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b62b98e-4f7e-4025-81e7-0e04f25dd328",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Expand Contractions, Tokenize, and Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b2e40a5-3775-4ff2-94db-6df8f183131c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [it, was, reported, that, the, transmitter, lo...\n",
       "1    [it, was, reported, that, signal, loss, over, ...\n",
       "2    [it, was, reported, that, transmitter, failed,...\n",
       "3    [it, was, reported, that, signal, loss, over, ...\n",
       "4    [it, was, reported, that, signal, loss, over, ...\n",
       "Name: TOKENIZED_TEXT, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This approach takes the FOI_TEXT as a string and creates a new column with tokens\n",
    "# It removes contractions _and_ tokenizes at the same time\n",
    "# No additional function is needed, x.split tokenizes the string (FOI text) at every space\n",
    "# A call to lower() converts the word to lowercase\n",
    "\n",
    "df['TOKENIZED_TEXT'] = df['FOI_TEXT'].apply(lambda x: [contractions.fix(word).lower() for word in x.split()])\n",
    "df['TOKENIZED_TEXT'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49be4fc3-c02f-468b-9dc8-82506eaaeb12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " 'was',\n",
       " 'reported',\n",
       " 'that',\n",
       " 'signal',\n",
       " 'loss',\n",
       " 'over',\n",
       " 'one',\n",
       " 'hour',\n",
       " 'occurred.',\n",
       " 'no',\n",
       " 'product',\n",
       " 'or',\n",
       " 'data',\n",
       " 'was',\n",
       " 'provided',\n",
       " 'for',\n",
       " 'evaluation.',\n",
       " 'confirmation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'allegation',\n",
       " 'and',\n",
       " 'a',\n",
       " 'probable',\n",
       " 'because',\n",
       " 'could',\n",
       " 'not',\n",
       " 'be',\n",
       " 'determined.',\n",
       " 'no',\n",
       " 'injury',\n",
       " 'or',\n",
       " 'medical',\n",
       " 'intervention',\n",
       " 'was',\n",
       " 'reported.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['TOKENIZED_TEXT'][verification_row]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b341a0-4cc5-409b-a4f9-510cff03c266",
   "metadata": {},
   "source": [
    "## 2. Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6eb411b-67a5-4f90-addc-1fe49c741838",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef62350c-3ed9-4b54-a69b-a3e93059c550",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [it, was, reported, that, the, transmitter, lo...\n",
       "1    [it, was, reported, that, signal, loss, over, ...\n",
       "2    [it, was, reported, that, transmitter, failed,...\n",
       "3    [it, was, reported, that, signal, loss, over, ...\n",
       "4    [it, was, reported, that, signal, loss, over, ...\n",
       "Name: NOPUNCT_TEXT, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to remove punctuation in the data\n",
    "def remove_punctuation(text):\n",
    "    text = \"\".join([character for character in text if character not in string.punctuation])\n",
    "    return text\n",
    "\n",
    "df['NOPUNCT_TEXT'] = df['TOKENIZED_TEXT'].apply(lambda x: [remove_punctuation(word) for word in x])\n",
    "df['NOPUNCT_TEXT'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fe886e3-fad3-4ca4-a02a-d94b796eb33a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " 'was',\n",
       " 'reported',\n",
       " 'that',\n",
       " 'signal',\n",
       " 'loss',\n",
       " 'over',\n",
       " 'one',\n",
       " 'hour',\n",
       " 'occurred',\n",
       " 'no',\n",
       " 'product',\n",
       " 'or',\n",
       " 'data',\n",
       " 'was',\n",
       " 'provided',\n",
       " 'for',\n",
       " 'evaluation',\n",
       " 'confirmation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'allegation',\n",
       " 'and',\n",
       " 'a',\n",
       " 'probable',\n",
       " 'because',\n",
       " 'could',\n",
       " 'not',\n",
       " 'be',\n",
       " 'determined',\n",
       " 'no',\n",
       " 'injury',\n",
       " 'or',\n",
       " 'medical',\n",
       " 'intervention',\n",
       " 'was',\n",
       " 'reported']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['NOPUNCT_TEXT'][verification_row]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c01067-455b-44af-95ca-93854f5cc469",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cebbe10-e09c-4aa5-917d-5159fb44be56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [reported, transmitter, lost, connection, pump...\n",
       "1    [reported, signal, loss, one, hour, occurred, ...\n",
       "2    [reported, transmitter, failed, error, occurre...\n",
       "3    [reported, signal, loss, one, hour, occurred, ...\n",
       "4    [reported, signal, loss, one, hour, occurred, ...\n",
       "Name: NOSTOPWORD_TEXT, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Define a function to convert to lowercase and remove stopwords\n",
    "def remove_stopwords(tokenized_text):\n",
    "    text = [word for word in tokenized_text if word.lower() not in stopwords]\n",
    "    return text\n",
    "\n",
    "df['NOSTOPWORD_TEXT'] = df['NOPUNCT_TEXT'].apply(lambda x: remove_stopwords(x))\n",
    "df['NOSTOPWORD_TEXT'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06cf0ce2-c8fc-4376-86d3-19c9dcdb31ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reported',\n",
       " 'signal',\n",
       " 'loss',\n",
       " 'one',\n",
       " 'hour',\n",
       " 'occurred',\n",
       " 'product',\n",
       " 'data',\n",
       " 'provided',\n",
       " 'evaluation',\n",
       " 'confirmation',\n",
       " 'allegation',\n",
       " 'probable',\n",
       " 'could',\n",
       " 'determined',\n",
       " 'injury',\n",
       " 'medical',\n",
       " 'intervention',\n",
       " 'reported']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['NOSTOPWORD_TEXT'][verification_row]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c213aa-997b-4371-8dd9-c6b5f5dfe07e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Parts of Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e318ac0-d7d2-406b-b2df-6cb861548bd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [(reported, VBN), (transmitter, NN), (lost, VB...\n",
       "1    [(reported, VBN), (signal, JJ), (loss, NN), (o...\n",
       "2    [(reported, VBN), (transmitter, NN), (failed, ...\n",
       "3    [(reported, VBN), (signal, JJ), (loss, NN), (o...\n",
       "4    [(reported, VBN), (signal, JJ), (loss, NN), (o...\n",
       "Name: POS_TEXT, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nltk.pos_tag() function to each row of the TOKENIZED_TEXT column\n",
    "# pos_tag returns a Tuple for each word consisting of the word and its classification\n",
    "# TODO: List classifications and their abbreviations\n",
    "df['POS_TEXT'] = df['NOSTOPWORD_TEXT'].apply(nltk.pos_tag)\n",
    "df['POS_TEXT'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12140ece-4515-4637-8efe-dfb3cfa8cf3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reported', 'VBN'),\n",
       " ('signal', 'JJ'),\n",
       " ('loss', 'NN'),\n",
       " ('one', 'CD'),\n",
       " ('hour', 'NN'),\n",
       " ('occurred', 'VBD'),\n",
       " ('product', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " ('provided', 'VBD'),\n",
       " ('evaluation', 'NN'),\n",
       " ('confirmation', 'NN'),\n",
       " ('allegation', 'NN'),\n",
       " ('probable', 'NN'),\n",
       " ('could', 'MD'),\n",
       " ('determined', 'VB'),\n",
       " ('injury', 'VB'),\n",
       " ('medical', 'JJ'),\n",
       " ('intervention', 'NN'),\n",
       " ('reported', 'VBD')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['POS_TEXT'][verification_row]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462e6d3f-d2a6-414b-ae7a-55481ba8d5fe",
   "metadata": {},
   "source": [
    "## 5. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "034f05dc-490e-4692-b1cb-2445732f80d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [report, transmitter, lose, connection, pump, ...\n",
       "1    [report, signal, loss, one, hour, occur, produ...\n",
       "2    [report, transmitter, fail, error, occur, data...\n",
       "3    [report, signal, loss, one, hour, occur, revie...\n",
       "4    [report, signal, loss, one, hour, occur, produ...\n",
       "Name: LEMMATIZED_TEXT, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# define a function to lemmatize each word in a text list based on its POS tag\n",
    "def lemmatize_text(pos_tagged_text):\n",
    "    # initialize WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # map NLTK's POS tags to WordNet's POS tags\n",
    "    # TODO: list the abbreviations for WordNet's parts of speech\n",
    "    pos_map = {'N': 'n', 'V': 'v', 'R': 'r', 'J': 'a'}\n",
    "    \n",
    "    # lemmatize each word in the text list based on its POS tag\n",
    "    lemmatized_text = []\n",
    "    \n",
    "    for word, pos in pos_tagged_text:\n",
    "        \n",
    "        # get the first character of the POS tag to use as the WordNet POS tag\n",
    "        # \n",
    "        # Set the WordNetLemmatizer default to Nouns ('n') or Verbs ('v')\n",
    "        #\n",
    "        wn_pos = pos_map.get(pos[0], 'n') \n",
    "        \n",
    "        # lemmatize the word and append it to the lemmatized text list\n",
    "        lemmatized_word = lemmatizer.lemmatize(word, pos=wn_pos)\n",
    "        lemmatized_text.append(lemmatized_word)\n",
    "    \n",
    "    # return the lemmatized text list\n",
    "    return lemmatized_text\n",
    "\n",
    "# apply the lemmatize_text function to each row of the dataframe\n",
    "df['LEMMATIZED_TEXT'] = df['POS_TEXT'].apply(lemmatize_text)\n",
    "df['LEMMATIZED_TEXT'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66e4cfe-298d-4186-ab98-74848658af20",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6db2894c-4f23-4d9c-9b0f-ef344c9db60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [report, transmitt, lost, connect, pump, great...\n",
       "1    [report, signal, loss, one, hour, occur, produ...\n",
       "2    [report, transmitt, fail, error, occur, data, ...\n",
       "3    [report, signal, loss, one, hour, occur, revie...\n",
       "4    [report, signal, loss, one, hour, occur, produ...\n",
       "Name: STEMMED_TEXT, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# define a function to stem each word in a text list based on its POS tag\n",
    "def stem_words(pos_tagged_text):\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    stemmed_text = []\n",
    "    \n",
    "    for word, pos in pos_tagged_text:\n",
    "        # stem the word and append it to the stemmed text list\n",
    "        stemmed_word = stemmer.stem(word)\n",
    "        stemmed_text.append(stemmed_word)\n",
    "    \n",
    "    # return the stemmed text list\n",
    "    return stemmed_text\n",
    "\n",
    "df['STEMMED_TEXT'] = df['POS_TEXT'].apply(stem_words)\n",
    "df['STEMMED_TEXT'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23bed8a-188b-4b6d-ba1b-6d17911dec96",
   "metadata": {},
   "source": [
    "## Compare the results of lemmatization and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91b1fbf4-91a8-4ffc-a90f-27a75fc95636",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word            POS Tag Lemma           Stem            \n",
      "--------------- ------- ---------------- ----------------\n",
      "reported        VBN     report          report\n",
      "signal          JJ      signal          signal\n",
      "loss            NN      loss            loss\n",
      "one             CD      one             one\n",
      "hour            NN      hour            hour\n",
      "occurred        VBD     occur           occur\n",
      "product         NN      product         product\n",
      "data            NNS     data            data\n",
      "provided        VBD     provide         provid\n",
      "evaluation      NN      evaluation      evalu\n",
      "confirmation    NN      confirmation    confirm\n",
      "allegation      NN      allegation      alleg\n",
      "probable        NN      probable        probabl\n",
      "could           MD      could           could\n",
      "determined      VB      determine       determin\n",
      "injury          VB      injury          injuri\n",
      "medical         JJ      medical         medic\n",
      "intervention    NN      intervention    intervent\n",
      "reported        VBD     report          report\n"
     ]
    }
   ],
   "source": [
    "# For verification, print a table showing the word, POS tag, the lemmatization result, and the stemming result\n",
    "# TODO: Is the lemmatizer correctly lemmatizing words that end in 'ion', ie: evalutation -> evaluate\n",
    "word_header = \"Word\"\n",
    "pos_header = \"POS Tag\"\n",
    "lemma_header = \"Lemma\"\n",
    "stem_header = \"Stem\"\n",
    "print(f\"{word_header:<{16}}{pos_header:<{8}}{lemma_header:<{16}}{stem_header:<{16}}\")\n",
    "print(\"-\"*15, \"-\"*7, \"-\"*16, \"-\"*16)\n",
    "\n",
    "for i in range(len(df['POS_TEXT'][verification_row])):\n",
    "    original_word = df['POS_TEXT'][verification_row][i][0]\n",
    "    pos_tag = df['POS_TEXT'][verification_row][i][1]\n",
    "    lemma = df['LEMMATIZED_TEXT'][verification_row][i]\n",
    "    stem = df['STEMMED_TEXT'][verification_row][i]\n",
    "    \n",
    "    print(f\"{original_word:<{16}}{pos_tag:<{8}}{lemma:<{16}}{stem}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0e2ab0-8c85-4795-868a-603b77154cca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7. Sentencize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57d03e1c-12a5-4888-98f8-7fa48fecbe91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [IT WAS REPORTED THAT THE TRANSMITTER LOST CON...\n",
       "1    [IT WAS REPORTED THAT SIGNAL LOSS OVER ONE HOU...\n",
       "2    [IT WAS REPORTED THAT TRANSMITTER FAILED ERROR...\n",
       "3    [IT WAS REPORTED THAT SIGNAL LOSS OVER ONE HOU...\n",
       "4    [IT WAS REPORTED THAT SIGNAL LOSS OVER ONE HOU...\n",
       "Name: SENTENCIZED_TEXT, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Define a function to tokenize the sentences\n",
    "def split_sentences(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "df['SENTENCIZED_TEXT'] = df['FOI_TEXT'].apply(split_sentences)\n",
    "df['SENTENCIZED_TEXT'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce01689d-e927-497a-bca5-d79f426d4ba2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IT WAS REPORTED THAT SIGNAL LOSS OVER ONE HOUR OCCURRED.',\n",
       " 'NO PRODUCT OR DATA WAS PROVIDED FOR EVALUATION.',\n",
       " 'CONFIRMATION OF THE ALLEGATION AND A PROBABLE CAUSE COULD NOT BE DETERMINED.',\n",
       " 'NO INJURY OR MEDICAL INTERVENTION WAS REPORTED.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['SENTENCIZED_TEXT'][verification_row]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3080c448-ef79-4bfa-bf12-884a043b4eb2",
   "metadata": {},
   "source": [
    "## 8. Create Bag of Words (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb023fab-c436-4827-a86a-6b15bd24bfe1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5736, 1197)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create a CountVectorizer object\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# fit the vectorizer to the text data\n",
    "count_vectorizer.fit(df['LEMMATIZED_TEXT'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# create a bag of words matrix\n",
    "bow_matrix = count_vectorizer.transform(df['LEMMATIZED_TEXT'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# convert the bag of words matrix to a DataFrame\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "bow_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e787b477-89c3-434e-8909-1b8fce6418bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>021</th>\n",
       "      <th>021vdc</th>\n",
       "      <th>03142020</th>\n",
       "      <th>045</th>\n",
       "      <th>06142020</th>\n",
       "      <th>07032020</th>\n",
       "      <th>07312020</th>\n",
       "      <th>0v</th>\n",
       "      <th>0vdc</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>work</th>\n",
       "      <th>would</th>\n",
       "      <th>x2</th>\n",
       "      <th>xray</th>\n",
       "      <th>year</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>zero</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1197 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   021  021vdc  03142020  045  06142020  07032020  07312020  0v  0vdc  10  \\\n",
       "0    0       0         0    0         0         0         0   0     0   0   \n",
       "1    0       0         0    0         0         0         0   0     0   0   \n",
       "2    0       0         0    0         0         0         0   0     0   0   \n",
       "3    0       0         0    0         0         0         0   0     0   0   \n",
       "4    0       0         0    0         0         0         0   0     0   0   \n",
       "\n",
       "   ...  work  would  x2  xray  year  yellow  yes  yet  zero  zone  \n",
       "0  ...     0      0   0     0     0       0    0    0     0     0  \n",
       "1  ...     0      0   0     0     0       0    0    0     0     0  \n",
       "2  ...     0      0   0     0     0       0    0    0     0     0  \n",
       "3  ...     0      0   0     0     0       0    0    0     0     0  \n",
       "4  ...     0      0   0     0     0       0    0    0     0     0  \n",
       "\n",
       "[5 rows x 1197 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a792a6c7-7813-44bc-b1d8-8db3a8f58f73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Plot the BOW results (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a68c5c-55bc-4559-af40-e8ae3ad76d5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 9. Calculate Term Frequency-Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5fc601b8-7e4a-4a15-aa80-7017721a7892",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5736, 1197)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create a CountVectorizer object and fit it to the text data\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "X = tfidf_vectorizer.fit_transform(df['LEMMATIZED_TEXT'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# convert the sparse matrix to a DataFrame\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=tfidf_vectorizer.get_feature_names())\n",
    "tfidf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9816a9be-d507-447e-85cc-24b370efdb8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>021</th>\n",
       "      <th>021vdc</th>\n",
       "      <th>03142020</th>\n",
       "      <th>045</th>\n",
       "      <th>06142020</th>\n",
       "      <th>07032020</th>\n",
       "      <th>07312020</th>\n",
       "      <th>0v</th>\n",
       "      <th>0vdc</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>work</th>\n",
       "      <th>would</th>\n",
       "      <th>x2</th>\n",
       "      <th>xray</th>\n",
       "      <th>year</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>zero</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1197 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   021  021vdc  03142020  045  06142020  07032020  07312020   0v  0vdc   10  \\\n",
       "0  0.0     0.0       0.0  0.0       0.0       0.0       0.0  0.0   0.0  0.0   \n",
       "1  0.0     0.0       0.0  0.0       0.0       0.0       0.0  0.0   0.0  0.0   \n",
       "2  0.0     0.0       0.0  0.0       0.0       0.0       0.0  0.0   0.0  0.0   \n",
       "3  0.0     0.0       0.0  0.0       0.0       0.0       0.0  0.0   0.0  0.0   \n",
       "4  0.0     0.0       0.0  0.0       0.0       0.0       0.0  0.0   0.0  0.0   \n",
       "\n",
       "   ...  work  would   x2  xray  year  yellow  yes  yet  zero  zone  \n",
       "0  ...   0.0    0.0  0.0   0.0   0.0     0.0  0.0  0.0   0.0   0.0  \n",
       "1  ...   0.0    0.0  0.0   0.0   0.0     0.0  0.0  0.0   0.0   0.0  \n",
       "2  ...   0.0    0.0  0.0   0.0   0.0     0.0  0.0  0.0   0.0   0.0  \n",
       "3  ...   0.0    0.0  0.0   0.0   0.0     0.0  0.0  0.0   0.0   0.0  \n",
       "4  ...   0.0    0.0  0.0   0.0   0.0     0.0  0.0  0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 1197 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74f580b-740d-4631-9816-37ce859dde28",
   "metadata": {},
   "source": [
    "## Review the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "486e4bb1-150b-4b9e-bc2e-c06ad0073a3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe columns:\n",
      "\tROW_ID\n",
      "\tFOI_TEXT\n",
      "\tDEVICE_PROBLEM_CODE\n",
      "\tDEVICE_PROBLEM_TEXT\n",
      "\tGENERIC_NAME\n",
      "\tDEVICE_REPORT_PRODUCT_CODE\n",
      "\tUDI-DI\n",
      "\tUDI-PUBLIC\n",
      "\tDATE_OF_EVENT\n",
      "\tREPORTER_OCCUPATION_CODE\n",
      "\tREPORT_DATE\n",
      "\tEVENT_LOCATION\n",
      "\tSOURCE_TYPE\n",
      "\tTOKENIZED_TEXT\n",
      "\tNOPUNCT_TEXT\n",
      "\tNOSTOPWORD_TEXT\n",
      "\tPOS_TEXT\n",
      "\tLEMMATIZED_TEXT\n",
      "\tSTEMMED_TEXT\n",
      "\tSENTENCIZED_TEXT\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataframe columns:\")\n",
    "for col in df.columns:\n",
    "    print(f\"\\t{col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13143c09-f9cd-4c01-b7b7-5e3994aa21e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save the preproecssed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98e853ae-ef27-4d92-a71a-b09efd9f101b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv(f\"{working_directory}/preprocessed_data.csv\", index=False)\n",
    "bow_df.to_csv(f\"{working_directory}/bag_of_words_data.csv\", index=False)\n",
    "tfidf_df.to_csv(f\"{working_directory}/tfidf_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc2654d-14e8-4709-8289-9922127f9f88",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Upload All Output to an S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9b03bb42-336b-406d-84d2-dbda67a93f9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: 16-data_preprocessing/bag_of_words_data.csv to s3://praxis-2023-html-output/bag_of_words_data.csv\n",
      "upload: 16-data_preprocessing/tfidf_data.csv to s3://praxis-2023-html-output/tfidf_data.csv\n",
      "upload: 16-data_preprocessing/preprocessed_data.csv to s3://praxis-2023-html-output/preprocessed_data.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Create the upload command using the AWS command line interface\n",
    "command = [\"aws\", \"s3\", \"sync\", working_directory, f\"s3://praxis-2023-html-output\", \"--exclude\", f\"*/.ipynb_checkpoints/*\", \"--no-progress\"]\n",
    "\n",
    "# Run the command and wait for it to complete\n",
    "output = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "# Print the command's output\n",
    "print(output.stdout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p37",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
