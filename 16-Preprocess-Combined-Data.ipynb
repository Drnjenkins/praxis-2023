{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44abeade-5fe3-40d6-99cb-621c66badf2e",
   "metadata": {},
   "source": [
    "# Preprocess Combined Data\n",
    "Using the combined QBJ as input, perform data preprocessing including:\n",
    "\n",
    "- Expand Contractions, Tokenize, and Convert to Lowercase\n",
    "- Remove Punctuation\n",
    "- Remove Stop Words\n",
    "- Parts of Speech (POS) Tagging\n",
    "- Lemmatize\n",
    "- Create Bag of Words (BOW)\n",
    "- Calculate Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "_Run [Notebook 07-Install-NLTK](./07-Install-NLTK.ipynb) as needed to set up NLTK._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a5cb8b5-2419-4a10-95ef-b98e00fa8fb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Identify the working directory and data files\n",
    "working_directory = './16-data_preprocessing'\n",
    "\n",
    "# Create the working directory if needed\n",
    "try:\n",
    "    os.makedirs(working_directory, exist_ok=True)\n",
    "except OSError as error:\n",
    "    print(f\"Error creating {working_directory}: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b12a20c0-f0d4-47ba-bc75-32ef8cc62c14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Readthe combined data into a dataframe\n",
    "data_file = './15-data_combination/qbj_data_combined.csv'\n",
    "\n",
    "# Read the data into a pandas dataframe\n",
    "df = pd.read_csv(data_file,           # The data file being read, from the variable assignment above\n",
    "                 on_bad_lines='warn', # This tells Pandas to only warn on bad lines vs causing an error\n",
    "                 dtype='str')         # This tells Pandas to treat all numbers as words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6021374b-7c6c-48c7-9319-47b7512b9a0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5736, 13)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85fd9d2d-01a6-45e6-8807-9e27dc01d5d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>FOI_TEXT</th>\n",
       "      <th>DEVICE_PROBLEM_CODE</th>\n",
       "      <th>DEVICE_PROBLEM_TEXT</th>\n",
       "      <th>GENERIC_NAME</th>\n",
       "      <th>DEVICE_REPORT_PRODUCT_CODE</th>\n",
       "      <th>UDI-DI</th>\n",
       "      <th>UDI-PUBLIC</th>\n",
       "      <th>DATE_OF_EVENT</th>\n",
       "      <th>REPORTER_OCCUPATION_CODE</th>\n",
       "      <th>REPORT_DATE</th>\n",
       "      <th>EVENT_LOCATION</th>\n",
       "      <th>SOURCE_TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1969025</td>\n",
       "      <td>IT WAS REPORTED THAT THE TRANSMITTER LOST CONN...</td>\n",
       "      <td>3283</td>\n",
       "      <td>Wireless Communication Problem</td>\n",
       "      <td>CONTINUOUS GLUCOSE MONITOR</td>\n",
       "      <td>QBJ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>07/30/2020</td>\n",
       "      <td>000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I</td>\n",
       "      <td>CONSUMER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1426265</td>\n",
       "      <td>IT WAS REPORTED THAT SIGNAL LOSS OVER ONE HOUR...</td>\n",
       "      <td>3283</td>\n",
       "      <td>Wireless Communication Problem</td>\n",
       "      <td>CONTINUOUS GLUCOSE MONITOR</td>\n",
       "      <td>QBJ</td>\n",
       "      <td>00386270000385</td>\n",
       "      <td>00386270000385</td>\n",
       "      <td>06/05/2020</td>\n",
       "      <td>000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I</td>\n",
       "      <td>CONSUMER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ROW_ID                                           FOI_TEXT  \\\n",
       "0  1969025  IT WAS REPORTED THAT THE TRANSMITTER LOST CONN...   \n",
       "1  1426265  IT WAS REPORTED THAT SIGNAL LOSS OVER ONE HOUR...   \n",
       "\n",
       "  DEVICE_PROBLEM_CODE             DEVICE_PROBLEM_TEXT  \\\n",
       "0                3283  Wireless Communication Problem   \n",
       "1                3283  Wireless Communication Problem   \n",
       "\n",
       "                 GENERIC_NAME DEVICE_REPORT_PRODUCT_CODE          UDI-DI  \\\n",
       "0  CONTINUOUS GLUCOSE MONITOR                        QBJ             NaN   \n",
       "1  CONTINUOUS GLUCOSE MONITOR                        QBJ  00386270000385   \n",
       "\n",
       "       UDI-PUBLIC DATE_OF_EVENT REPORTER_OCCUPATION_CODE REPORT_DATE  \\\n",
       "0             NaN    07/30/2020                      000         NaN   \n",
       "1  00386270000385    06/05/2020                      000         NaN   \n",
       "\n",
       "  EVENT_LOCATION SOURCE_TYPE  \n",
       "0              I    CONSUMER  \n",
       "1              I    CONSUMER  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cccf441-a429-4612-bda4-eafbc2af9ef0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load the Natural Language Toolkit (NLTK) and Preprocessing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cbed954-f2f1-41da-a1e8-3718d6ce413f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the NLTK library; If this step fails, rerun 07-Install-NLTK.ipynb\n",
    "import nltk\n",
    "import string\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b62b98e-4f7e-4025-81e7-0e04f25dd328",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Expand Contractions, Tokenize, and Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b2e40a5-3775-4ff2-94db-6df8f183131c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [it, was, reported, that, the, transmitter, lo...\n",
       "1       [it, was, reported, that, signal, loss, over, ...\n",
       "2       [it, was, reported, that, transmitter, failed,...\n",
       "3       [it, was, reported, that, signal, loss, over, ...\n",
       "4       [it, was, reported, that, signal, loss, over, ...\n",
       "                              ...                        \n",
       "5731    [it, was, reported, that, a, transmitter, fail...\n",
       "5732    [it, was, reported, that, signal, loss, over, ...\n",
       "5733    [it, was, reported, that, transmitter, failed,...\n",
       "5734    [it, was, reported, that, a, transmitter, fail...\n",
       "5735    [it, was, reported, that, signal, loss, over, ...\n",
       "Name: TOKENIZED_TEXT, Length: 5736, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This approach takes the FOI_TEXT as a string and creates a new column with tokens\n",
    "# That is, it removes the contraction _and_ tokenizes at the same time\n",
    "# no additional function is needed, x.split tokenizes the string (FOI text) at every space\n",
    "# a call to lower() converts the word to lowercase\n",
    "\n",
    "df['TOKENIZED_TEXT'] = df['FOI_TEXT'].apply(lambda x: [contractions.fix(word).lower() for word in x.split()])\n",
    "df['TOKENIZED_TEXT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49be4fc3-c02f-468b-9dc8-82506eaaeb12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " 'was',\n",
       " 'reported',\n",
       " 'that',\n",
       " 'the',\n",
       " 'transmitter',\n",
       " 'lost',\n",
       " 'connection',\n",
       " 'with',\n",
       " 'the',\n",
       " 'pump',\n",
       " 'for',\n",
       " 'greater',\n",
       " 'than',\n",
       " '1',\n",
       " 'hour.',\n",
       " 'the',\n",
       " 'transmitter',\n",
       " 'ultimately',\n",
       " 'regained',\n",
       " 'connection',\n",
       " 'with',\n",
       " 'the',\n",
       " 'pump.',\n",
       " 'no',\n",
       " 'additional',\n",
       " 'patient',\n",
       " 'or',\n",
       " 'event',\n",
       " 'information',\n",
       " 'was',\n",
       " 'available.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['TOKENIZED_TEXT'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b341a0-4cc5-409b-a4f9-510cff03c266",
   "metadata": {},
   "source": [
    "## Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6eb411b-67a5-4f90-addc-1fe49c741838",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef62350c-3ed9-4b54-a69b-a3e93059c550",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [it, was, reported, that, the, transmitter, lo...\n",
       "1       [it, was, reported, that, signal, loss, over, ...\n",
       "2       [it, was, reported, that, transmitter, failed,...\n",
       "3       [it, was, reported, that, signal, loss, over, ...\n",
       "4       [it, was, reported, that, signal, loss, over, ...\n",
       "                              ...                        \n",
       "5731    [it, was, reported, that, a, transmitter, fail...\n",
       "5732    [it, was, reported, that, signal, loss, over, ...\n",
       "5733    [it, was, reported, that, transmitter, failed,...\n",
       "5734    [it, was, reported, that, a, transmitter, fail...\n",
       "5735    [it, was, reported, that, signal, loss, over, ...\n",
       "Name: CLEAN_TEXT, Length: 5736, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to remove punctuation in the data\n",
    "def remove_punctuation(text):\n",
    "    text = \"\".join([character for character in text if character not in string.punctuation])\n",
    "    return text\n",
    "\n",
    "df['CLEAN_TEXT'] = df['TOKENIZED_TEXT'].apply(lambda x: [remove_punctuation(word) for word in x])\n",
    "df['CLEAN_TEXT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fe886e3-fad3-4ca4-a02a-d94b796eb33a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " 'was',\n",
       " 'reported',\n",
       " 'that',\n",
       " 'the',\n",
       " 'transmitter',\n",
       " 'lost',\n",
       " 'connection',\n",
       " 'with',\n",
       " 'the',\n",
       " 'pump',\n",
       " 'for',\n",
       " 'greater',\n",
       " 'than',\n",
       " '1',\n",
       " 'hour',\n",
       " 'the',\n",
       " 'transmitter',\n",
       " 'ultimately',\n",
       " 'regained',\n",
       " 'connection',\n",
       " 'with',\n",
       " 'the',\n",
       " 'pump',\n",
       " 'no',\n",
       " 'additional',\n",
       " 'patient',\n",
       " 'or',\n",
       " 'event',\n",
       " 'information',\n",
       " 'was',\n",
       " 'available']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['CLEAN_TEXT'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c01067-455b-44af-95ca-93854f5cc469",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cebbe10-e09c-4aa5-917d-5159fb44be56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [reported, transmitter, lost, connection, pump...\n",
       "1       [reported, signal, loss, one, hour, occurred, ...\n",
       "2       [reported, transmitter, failed, error, occurre...\n",
       "3       [reported, signal, loss, one, hour, occurred, ...\n",
       "4       [reported, signal, loss, one, hour, occurred, ...\n",
       "                              ...                        \n",
       "5731    [reported, transmitter, failed, error, occurre...\n",
       "5732    [reported, signal, loss, one, hour, occurred, ...\n",
       "5733    [reported, transmitter, failed, error, occurre...\n",
       "5734    [reported, transmitter, failed, error, occurre...\n",
       "5735    [reported, signal, loss, one, hour, occurred, ...\n",
       "Name: NOSTOPWORD_TEXT, Length: 5736, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Define a function to convert to lowercase and remove stopwords\n",
    "def remove_stopwords(tokenized_text):\n",
    "    text = [word for word in tokenized_text if word.lower() not in stopwords]\n",
    "    return text\n",
    "\n",
    "df['NOSTOPWORD_TEXT'] = df['CLEAN_TEXT'].apply(lambda x: remove_stopwords(x))\n",
    "df['NOSTOPWORD_TEXT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06cf0ce2-c8fc-4376-86d3-19c9dcdb31ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reported',\n",
       " 'transmitter',\n",
       " 'lost',\n",
       " 'connection',\n",
       " 'pump',\n",
       " 'greater',\n",
       " '1',\n",
       " 'hour',\n",
       " 'transmitter',\n",
       " 'ultimately',\n",
       " 'regained',\n",
       " 'connection',\n",
       " 'pump',\n",
       " 'additional',\n",
       " 'patient',\n",
       " 'event',\n",
       " 'information',\n",
       " 'available']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['NOSTOPWORD_TEXT'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c213aa-997b-4371-8dd9-c6b5f5dfe07e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Parts of Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e318ac0-d7d2-406b-b2df-6cb861548bd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [(reported, VBN), (transmitter, NN), (lost, VB...\n",
       "1       [(reported, VBN), (signal, JJ), (loss, NN), (o...\n",
       "2       [(reported, VBN), (transmitter, NN), (failed, ...\n",
       "3       [(reported, VBN), (signal, JJ), (loss, NN), (o...\n",
       "4       [(reported, VBN), (signal, JJ), (loss, NN), (o...\n",
       "                              ...                        \n",
       "5731    [(reported, VBN), (transmitter, NN), (failed, ...\n",
       "5732    [(reported, VBN), (signal, JJ), (loss, NN), (o...\n",
       "5733    [(reported, VBN), (transmitter, NN), (failed, ...\n",
       "5734    [(reported, VBN), (transmitter, NN), (failed, ...\n",
       "5735    [(reported, VBN), (signal, JJ), (loss, NN), (o...\n",
       "Name: POS_TEXT, Length: 5736, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nltk.pos_tag() function to each row of the TOKENIZED_TEXT column\n",
    "df['POS_TEXT'] = df['NOSTOPWORD_TEXT'].apply(nltk.pos_tag)\n",
    "df['POS_TEXT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12140ece-4515-4637-8efe-dfb3cfa8cf3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reported', 'VBN'),\n",
       " ('transmitter', 'NN'),\n",
       " ('lost', 'VBN'),\n",
       " ('connection', 'NN'),\n",
       " ('pump', 'NN'),\n",
       " ('greater', 'JJR'),\n",
       " ('1', 'CD'),\n",
       " ('hour', 'NN'),\n",
       " ('transmitter', 'NN'),\n",
       " ('ultimately', 'RB'),\n",
       " ('regained', 'VBD'),\n",
       " ('connection', 'NN'),\n",
       " ('pump', 'NN'),\n",
       " ('additional', 'JJ'),\n",
       " ('patient', 'NN'),\n",
       " ('event', 'NN'),\n",
       " ('information', 'NN'),\n",
       " ('available', 'JJ')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['POS_TEXT'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462e6d3f-d2a6-414b-ae7a-55481ba8d5fe",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "034f05dc-490e-4692-b1cb-2445732f80d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [report, transmitter, lose, connection, pump, ...\n",
       "1       [report, signal, loss, one, hour, occur, produ...\n",
       "2       [report, transmitter, fail, error, occur, data...\n",
       "3       [report, signal, loss, one, hour, occur, revie...\n",
       "4       [report, signal, loss, one, hour, occur, produ...\n",
       "                              ...                        \n",
       "5731    [report, transmitter, fail, error, occur, data...\n",
       "5732    [report, signal, loss, one, hour, occur, indic...\n",
       "5733    [report, transmitter, fail, error, occur, prod...\n",
       "5734    [report, transmitter, fail, error, occur, data...\n",
       "5735    [report, signal, loss, one, hour, occur, data,...\n",
       "Name: LEMMATIZED_TEXT, Length: 5736, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Set the WordNetLemmatizer preference to Nouns ('n') or Verbs ('v')\n",
    "# This will affect the lemmatization\n",
    "wordnet_preference = \"v\"\n",
    "\n",
    "# define a function to lemmatize each word in a text list based on its POS tag\n",
    "def lemmatize_text(pos_tagged_text):\n",
    "    # initialize WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # map NLTK's POS tags to WordNet's POS tags\n",
    "    pos_map = {'N': 'n', 'V': 'v', 'R': 'r', 'J': 'a'}\n",
    "    \n",
    "    # lemmatize each word in the text list based on its POS tag\n",
    "    lemmatized_text = []\n",
    "    \n",
    "    for word, pos in pos_tagged_text:\n",
    "        \n",
    "        # get the first character of the POS tag to use as the WordNet POS tag\n",
    "        # Set the WordNetLemmatizer default to Nouns ('n') or Verbs ('v')\n",
    "        wn_pos = pos_map.get(pos[0], 'n') \n",
    "        \n",
    "        # lemmatize the word and append it to the lemmatized text list\n",
    "        lemmatized_word = lemmatizer.lemmatize(word, pos=wn_pos)\n",
    "        lemmatized_text.append(lemmatized_word)\n",
    "    \n",
    "    # return the lemmatized text list as a string\n",
    "    return lemmatized_text\n",
    "\n",
    "# apply the lemmatize_text function to each row of the dataframe\n",
    "df['LEMMATIZED_TEXT'] = df['POS_TEXT'].apply(lemmatize_text)\n",
    "df['LEMMATIZED_TEXT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91b1fbf4-91a8-4ffc-a90f-27a75fc95636",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word            POS Tag Lemma\n",
      "--------------- ------- ----------------\n",
      "reported        VBN     report\n",
      "signal          JJ      signal\n",
      "loss            NN      loss\n",
      "one             CD      one\n",
      "hour            NN      hour\n",
      "occurred        VBD     occur\n",
      "product         NN      product\n",
      "data            NNS     data\n",
      "provided        VBD     provide\n",
      "evaluation      NN      evaluation\n",
      "confirmation    NN      confirmation\n",
      "allegation      NN      allegation\n",
      "probable        NN      probable\n",
      "could           MD      could\n",
      "determined      VB      determine\n",
      "injury          VB      injury\n",
      "medical         JJ      medical\n",
      "intervention    NN      intervention\n",
      "reported        VBD     report\n"
     ]
    }
   ],
   "source": [
    "row_check = 9\n",
    "\n",
    "word_header = \"Word\"\n",
    "pos_header = \"POS Tag\"\n",
    "lemma_header = \"Lemma\"\n",
    "print(f\"{word_header:<{16}}{pos_header:<{8}}{lemma_header}\")\n",
    "print(\"-\"*15, \"-\"*7, \"-\"*16)\n",
    "\n",
    "for i in range(len(df['POS_TEXT'][row_check])):\n",
    "    original_word = df['POS_TEXT'][row_check][i][0]\n",
    "    pos_tag = df['POS_TEXT'][row_check][i][1]\n",
    "    lemma = df['LEMMATIZED_TEXT'][row_check][i]\n",
    "    \n",
    "    print(f\"{original_word:<{16}}{pos_tag:<{8}}{lemma}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3080c448-ef79-4bfa-bf12-884a043b4eb2",
   "metadata": {},
   "source": [
    "## Create Bag of Words (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb023fab-c436-4827-a86a-6b15bd24bfe1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5736, 1197)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create a CountVectorizer object\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# fit the vectorizer to the text data\n",
    "count_vectorizer.fit(df['LEMMATIZED_TEXT'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# create a bag of words matrix\n",
    "bow_matrix = count_vectorizer.transform(df['LEMMATIZED_TEXT'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# convert the bag of words matrix to a DataFrame\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=count_vectorizer.get_feature_names())\n",
    "\n",
    "bow_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e787b477-89c3-434e-8909-1b8fce6418bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>021</th>\n",
       "      <th>021vdc</th>\n",
       "      <th>03142020</th>\n",
       "      <th>045</th>\n",
       "      <th>06142020</th>\n",
       "      <th>07032020</th>\n",
       "      <th>07312020</th>\n",
       "      <th>0v</th>\n",
       "      <th>0vdc</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>work</th>\n",
       "      <th>would</th>\n",
       "      <th>x2</th>\n",
       "      <th>xray</th>\n",
       "      <th>year</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>zero</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1197 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   021  021vdc  03142020  045  06142020  07032020  07312020  0v  0vdc  10  \\\n",
       "0    0       0         0    0         0         0         0   0     0   0   \n",
       "1    0       0         0    0         0         0         0   0     0   0   \n",
       "2    0       0         0    0         0         0         0   0     0   0   \n",
       "3    0       0         0    0         0         0         0   0     0   0   \n",
       "4    0       0         0    0         0         0         0   0     0   0   \n",
       "\n",
       "   ...  work  would  x2  xray  year  yellow  yes  yet  zero  zone  \n",
       "0  ...     0      0   0     0     0       0    0    0     0     0  \n",
       "1  ...     0      0   0     0     0       0    0    0     0     0  \n",
       "2  ...     0      0   0     0     0       0    0    0     0     0  \n",
       "3  ...     0      0   0     0     0       0    0    0     0     0  \n",
       "4  ...     0      0   0     0     0       0    0    0     0     0  \n",
       "\n",
       "[5 rows x 1197 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a68c5c-55bc-4559-af40-e8ae3ad76d5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Calculate Term Frequency-Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5fc601b8-7e4a-4a15-aa80-7017721a7892",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5736, 1197)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create a CountVectorizer object and fit it to the text data\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "X = tfidf_vectorizer.fit_transform(df['LEMMATIZED_TEXT'].apply(lambda x: ' '.join(x)))\n",
    "\n",
    "# convert the sparse matrix to a DataFrame\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "tfidf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9816a9be-d507-447e-85cc-24b370efdb8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>021</th>\n",
       "      <th>021vdc</th>\n",
       "      <th>03142020</th>\n",
       "      <th>045</th>\n",
       "      <th>06142020</th>\n",
       "      <th>07032020</th>\n",
       "      <th>07312020</th>\n",
       "      <th>0v</th>\n",
       "      <th>0vdc</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>work</th>\n",
       "      <th>would</th>\n",
       "      <th>x2</th>\n",
       "      <th>xray</th>\n",
       "      <th>year</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>zero</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1197 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   021  021vdc  03142020  045  06142020  07032020  07312020   0v  0vdc   10  \\\n",
       "0  0.0     0.0       0.0  0.0       0.0       0.0       0.0  0.0   0.0  0.0   \n",
       "1  0.0     0.0       0.0  0.0       0.0       0.0       0.0  0.0   0.0  0.0   \n",
       "2  0.0     0.0       0.0  0.0       0.0       0.0       0.0  0.0   0.0  0.0   \n",
       "3  0.0     0.0       0.0  0.0       0.0       0.0       0.0  0.0   0.0  0.0   \n",
       "4  0.0     0.0       0.0  0.0       0.0       0.0       0.0  0.0   0.0  0.0   \n",
       "\n",
       "   ...  work  would   x2  xray  year  yellow  yes  yet  zero  zone  \n",
       "0  ...   0.0    0.0  0.0   0.0   0.0     0.0  0.0  0.0   0.0   0.0  \n",
       "1  ...   0.0    0.0  0.0   0.0   0.0     0.0  0.0  0.0   0.0   0.0  \n",
       "2  ...   0.0    0.0  0.0   0.0   0.0     0.0  0.0  0.0   0.0   0.0  \n",
       "3  ...   0.0    0.0  0.0   0.0   0.0     0.0  0.0  0.0   0.0   0.0  \n",
       "4  ...   0.0    0.0  0.0   0.0   0.0     0.0  0.0  0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 1197 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13143c09-f9cd-4c01-b7b7-5e3994aa21e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save the preproecssed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "98e853ae-ef27-4d92-a71a-b09efd9f101b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv(f\"{working_directory}/preprocessed_data.csv\", index=False)\n",
    "bow_df.to_csv(f\"{working_directory}/bag_of_words_data.csv\", index=False)\n",
    "tfidf_df.to_csv(f\"{working_directory}/tfidf_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc2654d-14e8-4709-8289-9922127f9f88",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Upload All Output to an S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9b03bb42-336b-406d-84d2-dbda67a93f9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Create the upload command using the AWS command line interface\n",
    "command = [\"aws\", \"s3\", \"sync\", working_directory, f\"s3://praxis-2023-html-output\", \"--exclude\", f\"*/.ipynb_checkpoints/*\", \"--no-progress\"]\n",
    "\n",
    "# Run the command and wait for it to complete\n",
    "output = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "# Print the command's output\n",
    "print(output.stdout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p37",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
