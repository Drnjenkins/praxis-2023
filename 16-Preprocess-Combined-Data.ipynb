{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44abeade-5fe3-40d6-99cb-621c66badf2e",
   "metadata": {},
   "source": [
    "# Preprocess Combined Data\n",
    "Using the combined QBJ as input, perform data preprocessing including:\n",
    "\n",
    "1. Expand Contractions, Tokenize, and Convert to Lowercase\n",
    "1. Remove Punctuation\n",
    "1. Remove Stop Words\n",
    "1. Parts of Speech (POS) Tagging\n",
    "1. Lemmatize\n",
    "1. Stemming\n",
    "1. Create Bag of Words (BOW)\n",
    "1. Calculate Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "1. Sentencize\n",
    "     1. Lemmatize Sentences\n",
    "     1. Stem Sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8e07559-3ae2-43c7-aeb5-a5cad8413271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_pytorch_latest_p37/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "! pip install --quiet --upgrade contractions\n",
    "! pip install --quiet --upgrade nltk\n",
    "! python -m nltk.downloader --quiet 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a5cb8b5-2419-4a10-95ef-b98e00fa8fb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Identify the working directory and data files\n",
    "working_directory = \"./16-Preprocess-Combined-Data\"\n",
    "\n",
    "# Create the working directory if needed\n",
    "try:\n",
    "    os.makedirs(working_directory, exist_ok=True)\n",
    "except OSError as error:\n",
    "    print(f\"Error creating {working_directory}: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b12a20c0-f0d4-47ba-bc75-32ef8cc62c14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Readthe combined data into a dataframe\n",
    "data_file = \"./15-Combine-2020-2021-Stratified-Data/qbj_data_combined.csv\"\n",
    "\n",
    "# Read the data into a pandas dataframe\n",
    "df = pd.read_csv(\n",
    "    data_file,  # The data file being read, from the variable assignment above\n",
    "    on_bad_lines=\"warn\",  # This tells Pandas to only warn on bad lines vs causing an error\n",
    "    dtype=\"str\",\n",
    ")  # This tells Pandas to treat all numbers as words\n",
    "\n",
    "df.fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6021374b-7c6c-48c7-9319-47b7512b9a0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5736, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85fd9d2d-01a6-45e6-8807-9e27dc01d5d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>FOI_TEXT</th>\n",
       "      <th>DEVICE_PROBLEM_CODE</th>\n",
       "      <th>DEVICE_PROBLEM_TEXT</th>\n",
       "      <th>GENERIC_NAME</th>\n",
       "      <th>DEVICE_REPORT_PRODUCT_CODE</th>\n",
       "      <th>UDI-DI</th>\n",
       "      <th>UDI-PUBLIC</th>\n",
       "      <th>DATE_OF_EVENT</th>\n",
       "      <th>REPORTER_OCCUPATION_CODE</th>\n",
       "      <th>REPORT_DATE</th>\n",
       "      <th>EVENT_LOCATION</th>\n",
       "      <th>SOURCE_TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1969025</td>\n",
       "      <td>IT WAS REPORTED THAT THE TRANSMITTER LOST CONN...</td>\n",
       "      <td>3283</td>\n",
       "      <td>Wireless Communication Problem</td>\n",
       "      <td>CONTINUOUS GLUCOSE MONITOR</td>\n",
       "      <td>QBJ</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>07/30/2020</td>\n",
       "      <td>000</td>\n",
       "      <td></td>\n",
       "      <td>I</td>\n",
       "      <td>CONSUMER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1426265</td>\n",
       "      <td>IT WAS REPORTED THAT SIGNAL LOSS OVER ONE HOUR...</td>\n",
       "      <td>3283</td>\n",
       "      <td>Wireless Communication Problem</td>\n",
       "      <td>CONTINUOUS GLUCOSE MONITOR</td>\n",
       "      <td>QBJ</td>\n",
       "      <td>00386270000385</td>\n",
       "      <td>00386270000385</td>\n",
       "      <td>06/05/2020</td>\n",
       "      <td>000</td>\n",
       "      <td></td>\n",
       "      <td>I</td>\n",
       "      <td>CONSUMER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ROW_ID                                           FOI_TEXT  \\\n",
       "0  1969025  IT WAS REPORTED THAT THE TRANSMITTER LOST CONN...   \n",
       "1  1426265  IT WAS REPORTED THAT SIGNAL LOSS OVER ONE HOUR...   \n",
       "\n",
       "  DEVICE_PROBLEM_CODE             DEVICE_PROBLEM_TEXT  \\\n",
       "0                3283  Wireless Communication Problem   \n",
       "1                3283  Wireless Communication Problem   \n",
       "\n",
       "                 GENERIC_NAME DEVICE_REPORT_PRODUCT_CODE          UDI-DI  \\\n",
       "0  CONTINUOUS GLUCOSE MONITOR                        QBJ                   \n",
       "1  CONTINUOUS GLUCOSE MONITOR                        QBJ  00386270000385   \n",
       "\n",
       "       UDI-PUBLIC DATE_OF_EVENT REPORTER_OCCUPATION_CODE REPORT_DATE  \\\n",
       "0                    07/30/2020                      000               \n",
       "1  00386270000385    06/05/2020                      000               \n",
       "\n",
       "  EVENT_LOCATION SOURCE_TYPE  \n",
       "0              I    CONSUMER  \n",
       "1              I    CONSUMER  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fd1319-1714-4115-9bb3-07935554919f",
   "metadata": {},
   "source": [
    "## Assign a Row ID for Verification\n",
    "Assign a value to a variable that identifies a row from the dataset.  \n",
    "\n",
    "This will allow the same row to be used for verification of each preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2746674c-c24b-4986-b047-54f45d1beeab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "verification_row = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cccf441-a429-4612-bda4-eafbc2af9ef0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load the Natural Language Toolkit (NLTK) and Preprocessing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cbed954-f2f1-41da-a1e8-3718d6ce413f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the NLTK library\n",
    "import nltk  # If this step fails, rerun 07-Install-NLTK.ipynb\n",
    "import string\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b62b98e-4f7e-4025-81e7-0e04f25dd328",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Expand Contractions, Tokenize, and Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b2e40a5-3775-4ff2-94db-6df8f183131c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [it, was, reported, that, the, transmitter, lo...\n",
       "1    [it, was, reported, that, signal, loss, over, ...\n",
       "2    [it, was, reported, that, transmitter, failed,...\n",
       "3    [it, was, reported, that, signal, loss, over, ...\n",
       "4    [it, was, reported, that, signal, loss, over, ...\n",
       "Name: TOKENIZED_TEXT, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This approach takes the FOI_TEXT as a string and creates a new column with tokens\n",
    "# It removes contractions _and_ tokenizes at the same time\n",
    "# No additional function is needed, x.split tokenizes the string (FOI text) at every space\n",
    "# A call to lower() converts the word to lowercase\n",
    "\n",
    "df[\"TOKENIZED_TEXT\"] = df[\"FOI_TEXT\"].apply(\n",
    "    lambda x: [contractions.fix(word).lower() for word in x.split()]\n",
    ")\n",
    "df[\"TOKENIZED_TEXT\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49be4fc3-c02f-468b-9dc8-82506eaaeb12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " 'was',\n",
       " 'reported',\n",
       " 'that',\n",
       " 'signal',\n",
       " 'loss',\n",
       " 'over',\n",
       " 'one',\n",
       " 'hour',\n",
       " 'occurred.',\n",
       " 'no',\n",
       " 'product',\n",
       " 'or',\n",
       " 'data',\n",
       " 'was',\n",
       " 'provided',\n",
       " 'for',\n",
       " 'evaluation.',\n",
       " 'confirmation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'allegation',\n",
       " 'and',\n",
       " 'a',\n",
       " 'probable',\n",
       " 'because',\n",
       " 'could',\n",
       " 'not',\n",
       " 'be',\n",
       " 'determined.',\n",
       " 'no',\n",
       " 'injury',\n",
       " 'or',\n",
       " 'medical',\n",
       " 'intervention',\n",
       " 'was',\n",
       " 'reported.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"TOKENIZED_TEXT\"][verification_row]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b341a0-4cc5-409b-a4f9-510cff03c266",
   "metadata": {},
   "source": [
    "## 2. Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6eb411b-67a5-4f90-addc-1fe49c741838",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef62350c-3ed9-4b54-a69b-a3e93059c550",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [it, was, reported, that, the, transmitter, lo...\n",
       "1    [it, was, reported, that, signal, loss, over, ...\n",
       "2    [it, was, reported, that, transmitter, failed,...\n",
       "3    [it, was, reported, that, signal, loss, over, ...\n",
       "4    [it, was, reported, that, signal, loss, over, ...\n",
       "Name: NOPUNCT_TEXT, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to remove punctuation in the data\n",
    "def remove_punctuation(text):\n",
    "    text = \"\".join(\n",
    "        [character for character in text if character not in string.punctuation]\n",
    "    )\n",
    "    return text\n",
    "\n",
    "\n",
    "df[\"NOPUNCT_TEXT\"] = df[\"TOKENIZED_TEXT\"].apply(\n",
    "    lambda x: [remove_punctuation(word) for word in x]\n",
    ")\n",
    "df[\"NOPUNCT_TEXT\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fe886e3-fad3-4ca4-a02a-d94b796eb33a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " 'was',\n",
       " 'reported',\n",
       " 'that',\n",
       " 'signal',\n",
       " 'loss',\n",
       " 'over',\n",
       " 'one',\n",
       " 'hour',\n",
       " 'occurred',\n",
       " 'no',\n",
       " 'product',\n",
       " 'or',\n",
       " 'data',\n",
       " 'was',\n",
       " 'provided',\n",
       " 'for',\n",
       " 'evaluation',\n",
       " 'confirmation',\n",
       " 'of',\n",
       " 'the',\n",
       " 'allegation',\n",
       " 'and',\n",
       " 'a',\n",
       " 'probable',\n",
       " 'because',\n",
       " 'could',\n",
       " 'not',\n",
       " 'be',\n",
       " 'determined',\n",
       " 'no',\n",
       " 'injury',\n",
       " 'or',\n",
       " 'medical',\n",
       " 'intervention',\n",
       " 'was',\n",
       " 'reported']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"NOPUNCT_TEXT\"][verification_row]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c01067-455b-44af-95ca-93854f5cc469",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cebbe10-e09c-4aa5-917d-5159fb44be56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [reported, transmitter, lost, connection, pump...\n",
       "1    [reported, signal, loss, one, hour, occurred, ...\n",
       "2    [reported, transmitter, failed, error, occurre...\n",
       "3    [reported, signal, loss, one, hour, occurred, ...\n",
       "4    [reported, signal, loss, one, hour, occurred, ...\n",
       "Name: NOSTOPWORDS_TEXT, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "\n",
    "# Define a function to convert to lowercase and remove stopwords\n",
    "def remove_stopwords(tokenized_text):\n",
    "    text = [word for word in tokenized_text if word.lower() not in stopwords]\n",
    "    return text\n",
    "\n",
    "\n",
    "df[\"NOSTOPWORDS_TEXT\"] = df[\"NOPUNCT_TEXT\"].apply(lambda x: remove_stopwords(x))\n",
    "df[\"NOSTOPWORDS_TEXT\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06cf0ce2-c8fc-4376-86d3-19c9dcdb31ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reported',\n",
       " 'signal',\n",
       " 'loss',\n",
       " 'one',\n",
       " 'hour',\n",
       " 'occurred',\n",
       " 'product',\n",
       " 'data',\n",
       " 'provided',\n",
       " 'evaluation',\n",
       " 'confirmation',\n",
       " 'allegation',\n",
       " 'probable',\n",
       " 'could',\n",
       " 'determined',\n",
       " 'injury',\n",
       " 'medical',\n",
       " 'intervention',\n",
       " 'reported']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"NOSTOPWORDS_TEXT\"][verification_row]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c213aa-997b-4371-8dd9-c6b5f5dfe07e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Parts of Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e318ac0-d7d2-406b-b2df-6cb861548bd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [(reported, VBN), (transmitter, NN), (lost, VB...\n",
       "1    [(reported, VBN), (signal, JJ), (loss, NN), (o...\n",
       "2    [(reported, VBN), (transmitter, NN), (failed, ...\n",
       "3    [(reported, VBN), (signal, JJ), (loss, NN), (o...\n",
       "4    [(reported, VBN), (signal, JJ), (loss, NN), (o...\n",
       "Name: POS_TEXT, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the nltk.pos_tag() function to each row of the TOKENIZED_TEXT column\n",
    "# pos_tag returns a Tuple for each word consisting of the word and its classification\n",
    "# TODO: List classifications and their abbreviations\n",
    "df[\"POS_TEXT\"] = df[\"NOSTOPWORDS_TEXT\"].apply(nltk.pos_tag)\n",
    "df[\"POS_TEXT\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12140ece-4515-4637-8efe-dfb3cfa8cf3d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reported', 'VBN'),\n",
       " ('signal', 'JJ'),\n",
       " ('loss', 'NN'),\n",
       " ('one', 'CD'),\n",
       " ('hour', 'NN'),\n",
       " ('occurred', 'VBD'),\n",
       " ('product', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " ('provided', 'VBD'),\n",
       " ('evaluation', 'NN'),\n",
       " ('confirmation', 'NN'),\n",
       " ('allegation', 'NN'),\n",
       " ('probable', 'NN'),\n",
       " ('could', 'MD'),\n",
       " ('determined', 'VB'),\n",
       " ('injury', 'VB'),\n",
       " ('medical', 'JJ'),\n",
       " ('intervention', 'NN'),\n",
       " ('reported', 'VBD')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"POS_TEXT\"][verification_row]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462e6d3f-d2a6-414b-ae7a-55481ba8d5fe",
   "metadata": {},
   "source": [
    "## 5. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "034f05dc-490e-4692-b1cb-2445732f80d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [report, transmitter, lose, connection, pump, ...\n",
       "1    [report, signal, loss, one, hour, occur, produ...\n",
       "2    [report, transmitter, fail, error, occur, data...\n",
       "3    [report, signal, loss, one, hour, occur, revie...\n",
       "4    [report, signal, loss, one, hour, occur, produ...\n",
       "Name: LEMMATIZED_TEXT, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# define a function to lemmatize each word in a text list based on its POS tag\n",
    "def lemmatize_text(pos_tagged_text):\n",
    "    # initialize WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # map NLTK's POS tags to WordNet's POS tags\n",
    "    # TODO: list the abbreviations for WordNet's parts of speech\n",
    "    pos_map = {\"N\": \"n\", \"V\": \"v\", \"R\": \"r\", \"J\": \"a\"}\n",
    "\n",
    "    # lemmatize each word in the text list based on its POS tag\n",
    "    lemmatized_text = []\n",
    "\n",
    "    for word, pos in pos_tagged_text:\n",
    "        # get the first character of the POS tag to use as the WordNet POS tag\n",
    "        #\n",
    "        # Set the WordNetLemmatizer default to Nouns ('n') or Verbs ('v')\n",
    "        #\n",
    "        wn_pos = pos_map.get(pos[0], \"n\")\n",
    "\n",
    "        # lemmatize the word and append it to the lemmatized text list\n",
    "        lemmatized_word = lemmatizer.lemmatize(word, pos=wn_pos)\n",
    "        lemmatized_text.append(lemmatized_word)\n",
    "\n",
    "    # return the lemmatized text list\n",
    "    return lemmatized_text\n",
    "\n",
    "\n",
    "# apply the lemmatize_text function to each row of the dataframe\n",
    "df[\"LEMMATIZED_TEXT\"] = df[\"POS_TEXT\"].apply(lemmatize_text)\n",
    "df[\"LEMMATIZED_TEXT\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66e4cfe-298d-4186-ab98-74848658af20",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6db2894c-4f23-4d9c-9b0f-ef344c9db60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [report, transmitt, lost, connect, pump, great...\n",
       "1    [report, signal, loss, one, hour, occur, produ...\n",
       "2    [report, transmitt, fail, error, occur, data, ...\n",
       "3    [report, signal, loss, one, hour, occur, revie...\n",
       "4    [report, signal, loss, one, hour, occur, produ...\n",
       "Name: STEMMED_TEXT, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "# define a function to stem each word in a text list\n",
    "def stem_words(pos_tagged_text):\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    stemmed_text = []\n",
    "\n",
    "    for word, pos in pos_tagged_text:\n",
    "        # stem the word and append it to the stemmed text list\n",
    "        stemmed_word = stemmer.stem(word)\n",
    "        stemmed_text.append(stemmed_word)\n",
    "\n",
    "    # return the stemmed text list\n",
    "    return stemmed_text\n",
    "\n",
    "\n",
    "df[\"STEMMED_TEXT\"] = df[\"POS_TEXT\"].apply(stem_words)\n",
    "df[\"STEMMED_TEXT\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23bed8a-188b-4b6d-ba1b-6d17911dec96",
   "metadata": {},
   "source": [
    "## Compare the results of lemmatization and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9f7eda1-d616-419f-990c-ec13f885ea56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_5a38f_ th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_5a38f_row0_col0, #T_5a38f_row0_col1, #T_5a38f_row0_col2, #T_5a38f_row1_col0, #T_5a38f_row1_col1, #T_5a38f_row1_col2, #T_5a38f_row2_col0, #T_5a38f_row2_col1, #T_5a38f_row2_col2, #T_5a38f_row3_col0, #T_5a38f_row3_col1, #T_5a38f_row3_col2, #T_5a38f_row4_col0, #T_5a38f_row4_col1, #T_5a38f_row4_col2, #T_5a38f_row5_col0, #T_5a38f_row5_col1, #T_5a38f_row5_col2, #T_5a38f_row6_col0, #T_5a38f_row6_col1, #T_5a38f_row6_col2, #T_5a38f_row7_col0, #T_5a38f_row7_col1, #T_5a38f_row7_col2, #T_5a38f_row8_col0, #T_5a38f_row8_col1, #T_5a38f_row8_col2, #T_5a38f_row9_col0, #T_5a38f_row9_col1, #T_5a38f_row9_col2, #T_5a38f_row10_col0, #T_5a38f_row10_col1, #T_5a38f_row10_col2, #T_5a38f_row11_col0, #T_5a38f_row11_col1, #T_5a38f_row11_col2, #T_5a38f_row12_col0, #T_5a38f_row12_col1, #T_5a38f_row12_col2, #T_5a38f_row13_col0, #T_5a38f_row13_col1, #T_5a38f_row13_col2, #T_5a38f_row14_col0, #T_5a38f_row14_col1, #T_5a38f_row14_col2, #T_5a38f_row15_col0, #T_5a38f_row15_col1, #T_5a38f_row15_col2, #T_5a38f_row16_col0, #T_5a38f_row16_col1, #T_5a38f_row16_col2, #T_5a38f_row17_col0, #T_5a38f_row17_col1, #T_5a38f_row17_col2, #T_5a38f_row18_col0, #T_5a38f_row18_col1, #T_5a38f_row18_col2 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_5a38f_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >WORD, PART OF SPEECH</th>\n",
       "      <th class=\"col_heading level0 col1\" >LEMMA</th>\n",
       "      <th class=\"col_heading level0 col2\" >STEM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_5a38f_row0_col0\" class=\"data row0 col0\" >('reported', 'VBN')</td>\n",
       "      <td id=\"T_5a38f_row0_col1\" class=\"data row0 col1\" >report</td>\n",
       "      <td id=\"T_5a38f_row0_col2\" class=\"data row0 col2\" >report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_5a38f_row1_col0\" class=\"data row1 col0\" >('signal', 'JJ')</td>\n",
       "      <td id=\"T_5a38f_row1_col1\" class=\"data row1 col1\" >signal</td>\n",
       "      <td id=\"T_5a38f_row1_col2\" class=\"data row1 col2\" >signal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_5a38f_row2_col0\" class=\"data row2 col0\" >('loss', 'NN')</td>\n",
       "      <td id=\"T_5a38f_row2_col1\" class=\"data row2 col1\" >loss</td>\n",
       "      <td id=\"T_5a38f_row2_col2\" class=\"data row2 col2\" >loss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_5a38f_row3_col0\" class=\"data row3 col0\" >('one', 'CD')</td>\n",
       "      <td id=\"T_5a38f_row3_col1\" class=\"data row3 col1\" >one</td>\n",
       "      <td id=\"T_5a38f_row3_col2\" class=\"data row3 col2\" >one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_5a38f_row4_col0\" class=\"data row4 col0\" >('hour', 'NN')</td>\n",
       "      <td id=\"T_5a38f_row4_col1\" class=\"data row4 col1\" >hour</td>\n",
       "      <td id=\"T_5a38f_row4_col2\" class=\"data row4 col2\" >hour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_5a38f_row5_col0\" class=\"data row5 col0\" >('occurred', 'VBD')</td>\n",
       "      <td id=\"T_5a38f_row5_col1\" class=\"data row5 col1\" >occur</td>\n",
       "      <td id=\"T_5a38f_row5_col2\" class=\"data row5 col2\" >occur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_5a38f_row6_col0\" class=\"data row6 col0\" >('product', 'NN')</td>\n",
       "      <td id=\"T_5a38f_row6_col1\" class=\"data row6 col1\" >product</td>\n",
       "      <td id=\"T_5a38f_row6_col2\" class=\"data row6 col2\" >product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_5a38f_row7_col0\" class=\"data row7 col0\" >('data', 'NNS')</td>\n",
       "      <td id=\"T_5a38f_row7_col1\" class=\"data row7 col1\" >data</td>\n",
       "      <td id=\"T_5a38f_row7_col2\" class=\"data row7 col2\" >data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_5a38f_row8_col0\" class=\"data row8 col0\" >('provided', 'VBD')</td>\n",
       "      <td id=\"T_5a38f_row8_col1\" class=\"data row8 col1\" >provide</td>\n",
       "      <td id=\"T_5a38f_row8_col2\" class=\"data row8 col2\" >provid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_5a38f_row9_col0\" class=\"data row9 col0\" >('evaluation', 'NN')</td>\n",
       "      <td id=\"T_5a38f_row9_col1\" class=\"data row9 col1\" >evaluation</td>\n",
       "      <td id=\"T_5a38f_row9_col2\" class=\"data row9 col2\" >evalu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_5a38f_row10_col0\" class=\"data row10 col0\" >('confirmation', 'NN')</td>\n",
       "      <td id=\"T_5a38f_row10_col1\" class=\"data row10 col1\" >confirmation</td>\n",
       "      <td id=\"T_5a38f_row10_col2\" class=\"data row10 col2\" >confirm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_5a38f_row11_col0\" class=\"data row11 col0\" >('allegation', 'NN')</td>\n",
       "      <td id=\"T_5a38f_row11_col1\" class=\"data row11 col1\" >allegation</td>\n",
       "      <td id=\"T_5a38f_row11_col2\" class=\"data row11 col2\" >alleg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_5a38f_row12_col0\" class=\"data row12 col0\" >('probable', 'NN')</td>\n",
       "      <td id=\"T_5a38f_row12_col1\" class=\"data row12 col1\" >probable</td>\n",
       "      <td id=\"T_5a38f_row12_col2\" class=\"data row12 col2\" >probabl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_5a38f_row13_col0\" class=\"data row13 col0\" >('could', 'MD')</td>\n",
       "      <td id=\"T_5a38f_row13_col1\" class=\"data row13 col1\" >could</td>\n",
       "      <td id=\"T_5a38f_row13_col2\" class=\"data row13 col2\" >could</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_5a38f_row14_col0\" class=\"data row14 col0\" >('determined', 'VB')</td>\n",
       "      <td id=\"T_5a38f_row14_col1\" class=\"data row14 col1\" >determine</td>\n",
       "      <td id=\"T_5a38f_row14_col2\" class=\"data row14 col2\" >determin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_5a38f_row15_col0\" class=\"data row15 col0\" >('injury', 'VB')</td>\n",
       "      <td id=\"T_5a38f_row15_col1\" class=\"data row15 col1\" >injury</td>\n",
       "      <td id=\"T_5a38f_row15_col2\" class=\"data row15 col2\" >injuri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_5a38f_row16_col0\" class=\"data row16 col0\" >('medical', 'JJ')</td>\n",
       "      <td id=\"T_5a38f_row16_col1\" class=\"data row16 col1\" >medical</td>\n",
       "      <td id=\"T_5a38f_row16_col2\" class=\"data row16 col2\" >medic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_5a38f_row17_col0\" class=\"data row17 col0\" >('intervention', 'NN')</td>\n",
       "      <td id=\"T_5a38f_row17_col1\" class=\"data row17 col1\" >intervention</td>\n",
       "      <td id=\"T_5a38f_row17_col2\" class=\"data row17 col2\" >intervent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_5a38f_row18_col0\" class=\"data row18 col0\" >('reported', 'VBD')</td>\n",
       "      <td id=\"T_5a38f_row18_col1\" class=\"data row18 col1\" >report</td>\n",
       "      <td id=\"T_5a38f_row18_col2\" class=\"data row18 col2\" >report</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f58ee576610>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_lemma_stem_df = pd.DataFrame(\n",
    "    {\n",
    "        \"WORD, PART OF SPEECH\": df[\"POS_TEXT\"][verification_row],\n",
    "        \"LEMMA\": df[\"LEMMATIZED_TEXT\"][verification_row],\n",
    "        \"STEM\": df[\"STEMMED_TEXT\"][verification_row],\n",
    "    }\n",
    ")\n",
    "\n",
    "compare_lemma_stem_df = compare_lemma_stem_df.style.set_properties(\n",
    "    **{\"text-align\": \"left\"}\n",
    ")\n",
    "compare_lemma_stem_df = compare_lemma_stem_df.set_table_styles(\n",
    "    [dict(selector=\"th\", props=[(\"text-align\", \"left\")])]\n",
    ")\n",
    "compare_lemma_stem_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3080c448-ef79-4bfa-bf12-884a043b4eb2",
   "metadata": {},
   "source": [
    "## 7. Create Bag of Words (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb023fab-c436-4827-a86a-6b15bd24bfe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 7. Create Bag of Words (BOW)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create a CountVectorizer object\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# fit the vectorizer to the text data\n",
    "count_vectorizer.fit(df[\"LEMMATIZED_TEXT\"].apply(lambda x: \" \".join(x)))\n",
    "\n",
    "# create a bag of words matrix\n",
    "bow_matrix = count_vectorizer.transform(\n",
    "    df[\"LEMMATIZED_TEXT\"].apply(lambda x: \" \".join(x))\n",
    ")\n",
    "\n",
    "# convert the bag of words matrix to a DataFrame\n",
    "bow_df = pd.DataFrame(\n",
    "    bow_matrix.toarray(), columns=count_vectorizer.get_feature_names_out()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fdabd803-b610-47a5-95dc-3d83c0be0d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5736, 1197)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b63a7c3b-84bd-4c92-a79b-2d1b00ce7c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>021</th>\n",
       "      <th>021vdc</th>\n",
       "      <th>03142020</th>\n",
       "      <th>045</th>\n",
       "      <th>06142020</th>\n",
       "      <th>07032020</th>\n",
       "      <th>07312020</th>\n",
       "      <th>0v</th>\n",
       "      <th>0vdc</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>work</th>\n",
       "      <th>would</th>\n",
       "      <th>x2</th>\n",
       "      <th>xray</th>\n",
       "      <th>year</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>zero</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1197 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   021  021vdc  03142020  045  06142020  07032020  07312020  0v  0vdc  10  \\\n",
       "0    0       0         0    0         0         0         0   0     0   0   \n",
       "1    0       0         0    0         0         0         0   0     0   0   \n",
       "2    0       0         0    0         0         0         0   0     0   0   \n",
       "3    0       0         0    0         0         0         0   0     0   0   \n",
       "4    0       0         0    0         0         0         0   0     0   0   \n",
       "\n",
       "   ...  work  would  x2  xray  year  yellow  yes  yet  zero  zone  \n",
       "0  ...     0      0   0     0     0       0    0    0     0     0  \n",
       "1  ...     0      0   0     0     0       0    0    0     0     0  \n",
       "2  ...     0      0   0     0     0       0    0    0     0     0  \n",
       "3  ...     0      0   0     0     0       0    0    0     0     0  \n",
       "4  ...     0      0   0     0     0       0    0    0     0     0  \n",
       "\n",
       "[5 rows x 1197 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_df.head()\n",
    "# TODO: Plot the BOW results (?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a68c5c-55bc-4559-af40-e8ae3ad76d5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 8. Calculate Term Frequency-Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5fc601b8-7e4a-4a15-aa80-7017721a7892",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 8. Calculate Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create a CountVectorizer object and fit it to the text data\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "X = tfidf_vectorizer.fit_transform(df[\"LEMMATIZED_TEXT\"].apply(lambda x: \" \".join(x)))\n",
    "\n",
    "# convert the sparse matrix to a DataFrame\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc7dc764-c475-4da1-9a5e-62d3a103688a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5736, 1197)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ced8af5a-9c8a-4e5f-8bf6-c4d0b9a2ab68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>021</th>\n",
       "      <th>021vdc</th>\n",
       "      <th>03142020</th>\n",
       "      <th>045</th>\n",
       "      <th>06142020</th>\n",
       "      <th>07032020</th>\n",
       "      <th>07312020</th>\n",
       "      <th>0v</th>\n",
       "      <th>0vdc</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>work</th>\n",
       "      <th>would</th>\n",
       "      <th>x2</th>\n",
       "      <th>xray</th>\n",
       "      <th>year</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>zero</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1197 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   021  021vdc  03142020  045  06142020  07032020  07312020   0v  0vdc   10  \\\n",
       "0  0.0     0.0       0.0  0.0       0.0       0.0       0.0  0.0   0.0  0.0   \n",
       "1  0.0     0.0       0.0  0.0       0.0       0.0       0.0  0.0   0.0  0.0   \n",
       "2  0.0     0.0       0.0  0.0       0.0       0.0       0.0  0.0   0.0  0.0   \n",
       "3  0.0     0.0       0.0  0.0       0.0       0.0       0.0  0.0   0.0  0.0   \n",
       "4  0.0     0.0       0.0  0.0       0.0       0.0       0.0  0.0   0.0  0.0   \n",
       "\n",
       "   ...  work  would   x2  xray  year  yellow  yes  yet  zero  zone  \n",
       "0  ...   0.0    0.0  0.0   0.0   0.0     0.0  0.0  0.0   0.0   0.0  \n",
       "1  ...   0.0    0.0  0.0   0.0   0.0     0.0  0.0  0.0   0.0   0.0  \n",
       "2  ...   0.0    0.0  0.0   0.0   0.0     0.0  0.0  0.0   0.0   0.0  \n",
       "3  ...   0.0    0.0  0.0   0.0   0.0     0.0  0.0  0.0   0.0   0.0  \n",
       "4  ...   0.0    0.0  0.0   0.0   0.0     0.0  0.0  0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 1197 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0e2ab0-8c85-4795-868a-603b77154cca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 9. Sentencize\n",
    "The `FOI_TEXT` can be processed as sentences.\n",
    "\n",
    "For further analysis, each sentence needs to be associated with the `FOI_TEXT` row that it came from.\n",
    "\n",
    "[This discussion from Stack Overflow](https://stackoverflow.com/a/43922444/2308522) provides a suggestion for breaking the code into a dataframe of sentences with each sentence retaining the ID of the row where it was originally located.\n",
    "\n",
    "[This page from the Pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.itertuples.html) provides details on using the `itertuples()` function to process the rows of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a577254-5ecb-4a76-8d42-3021092ca551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_5a38f_ th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_5a38f_row0_col0, #T_5a38f_row0_col1, #T_5a38f_row0_col2, #T_5a38f_row1_col0, #T_5a38f_row1_col1, #T_5a38f_row1_col2, #T_5a38f_row2_col0, #T_5a38f_row2_col1, #T_5a38f_row2_col2, #T_5a38f_row3_col0, #T_5a38f_row3_col1, #T_5a38f_row3_col2, #T_5a38f_row4_col0, #T_5a38f_row4_col1, #T_5a38f_row4_col2, #T_5a38f_row5_col0, #T_5a38f_row5_col1, #T_5a38f_row5_col2, #T_5a38f_row6_col0, #T_5a38f_row6_col1, #T_5a38f_row6_col2, #T_5a38f_row7_col0, #T_5a38f_row7_col1, #T_5a38f_row7_col2, #T_5a38f_row8_col0, #T_5a38f_row8_col1, #T_5a38f_row8_col2, #T_5a38f_row9_col0, #T_5a38f_row9_col1, #T_5a38f_row9_col2, #T_5a38f_row10_col0, #T_5a38f_row10_col1, #T_5a38f_row10_col2, #T_5a38f_row11_col0, #T_5a38f_row11_col1, #T_5a38f_row11_col2, #T_5a38f_row12_col0, #T_5a38f_row12_col1, #T_5a38f_row12_col2, #T_5a38f_row13_col0, #T_5a38f_row13_col1, #T_5a38f_row13_col2, #T_5a38f_row14_col0, #T_5a38f_row14_col1, #T_5a38f_row14_col2, #T_5a38f_row15_col0, #T_5a38f_row15_col1, #T_5a38f_row15_col2, #T_5a38f_row16_col0, #T_5a38f_row16_col1, #T_5a38f_row16_col2, #T_5a38f_row17_col0, #T_5a38f_row17_col1, #T_5a38f_row17_col2, #T_5a38f_row18_col0, #T_5a38f_row18_col1, #T_5a38f_row18_col2 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_5a38f_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >WORD, PART OF SPEECH</th>\n",
       "      <th class=\"col_heading level0 col1\" >LEMMA</th>\n",
       "      <th class=\"col_heading level0 col2\" >STEM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_5a38f_row0_col0\" class=\"data row0 col0\" >('reported', 'VBN')</td>\n",
       "      <td id=\"T_5a38f_row0_col1\" class=\"data row0 col1\" >report</td>\n",
       "      <td id=\"T_5a38f_row0_col2\" class=\"data row0 col2\" >report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_5a38f_row1_col0\" class=\"data row1 col0\" >('signal', 'JJ')</td>\n",
       "      <td id=\"T_5a38f_row1_col1\" class=\"data row1 col1\" >signal</td>\n",
       "      <td id=\"T_5a38f_row1_col2\" class=\"data row1 col2\" >signal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_5a38f_row2_col0\" class=\"data row2 col0\" >('loss', 'NN')</td>\n",
       "      <td id=\"T_5a38f_row2_col1\" class=\"data row2 col1\" >loss</td>\n",
       "      <td id=\"T_5a38f_row2_col2\" class=\"data row2 col2\" >loss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_5a38f_row3_col0\" class=\"data row3 col0\" >('one', 'CD')</td>\n",
       "      <td id=\"T_5a38f_row3_col1\" class=\"data row3 col1\" >one</td>\n",
       "      <td id=\"T_5a38f_row3_col2\" class=\"data row3 col2\" >one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_5a38f_row4_col0\" class=\"data row4 col0\" >('hour', 'NN')</td>\n",
       "      <td id=\"T_5a38f_row4_col1\" class=\"data row4 col1\" >hour</td>\n",
       "      <td id=\"T_5a38f_row4_col2\" class=\"data row4 col2\" >hour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_5a38f_row5_col0\" class=\"data row5 col0\" >('occurred', 'VBD')</td>\n",
       "      <td id=\"T_5a38f_row5_col1\" class=\"data row5 col1\" >occur</td>\n",
       "      <td id=\"T_5a38f_row5_col2\" class=\"data row5 col2\" >occur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_5a38f_row6_col0\" class=\"data row6 col0\" >('product', 'NN')</td>\n",
       "      <td id=\"T_5a38f_row6_col1\" class=\"data row6 col1\" >product</td>\n",
       "      <td id=\"T_5a38f_row6_col2\" class=\"data row6 col2\" >product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_5a38f_row7_col0\" class=\"data row7 col0\" >('data', 'NNS')</td>\n",
       "      <td id=\"T_5a38f_row7_col1\" class=\"data row7 col1\" >data</td>\n",
       "      <td id=\"T_5a38f_row7_col2\" class=\"data row7 col2\" >data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_5a38f_row8_col0\" class=\"data row8 col0\" >('provided', 'VBD')</td>\n",
       "      <td id=\"T_5a38f_row8_col1\" class=\"data row8 col1\" >provide</td>\n",
       "      <td id=\"T_5a38f_row8_col2\" class=\"data row8 col2\" >provid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_5a38f_row9_col0\" class=\"data row9 col0\" >('evaluation', 'NN')</td>\n",
       "      <td id=\"T_5a38f_row9_col1\" class=\"data row9 col1\" >evaluation</td>\n",
       "      <td id=\"T_5a38f_row9_col2\" class=\"data row9 col2\" >evalu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_5a38f_row10_col0\" class=\"data row10 col0\" >('confirmation', 'NN')</td>\n",
       "      <td id=\"T_5a38f_row10_col1\" class=\"data row10 col1\" >confirmation</td>\n",
       "      <td id=\"T_5a38f_row10_col2\" class=\"data row10 col2\" >confirm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_5a38f_row11_col0\" class=\"data row11 col0\" >('allegation', 'NN')</td>\n",
       "      <td id=\"T_5a38f_row11_col1\" class=\"data row11 col1\" >allegation</td>\n",
       "      <td id=\"T_5a38f_row11_col2\" class=\"data row11 col2\" >alleg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_5a38f_row12_col0\" class=\"data row12 col0\" >('probable', 'NN')</td>\n",
       "      <td id=\"T_5a38f_row12_col1\" class=\"data row12 col1\" >probable</td>\n",
       "      <td id=\"T_5a38f_row12_col2\" class=\"data row12 col2\" >probabl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_5a38f_row13_col0\" class=\"data row13 col0\" >('could', 'MD')</td>\n",
       "      <td id=\"T_5a38f_row13_col1\" class=\"data row13 col1\" >could</td>\n",
       "      <td id=\"T_5a38f_row13_col2\" class=\"data row13 col2\" >could</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_5a38f_row14_col0\" class=\"data row14 col0\" >('determined', 'VB')</td>\n",
       "      <td id=\"T_5a38f_row14_col1\" class=\"data row14 col1\" >determine</td>\n",
       "      <td id=\"T_5a38f_row14_col2\" class=\"data row14 col2\" >determin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_5a38f_row15_col0\" class=\"data row15 col0\" >('injury', 'VB')</td>\n",
       "      <td id=\"T_5a38f_row15_col1\" class=\"data row15 col1\" >injury</td>\n",
       "      <td id=\"T_5a38f_row15_col2\" class=\"data row15 col2\" >injuri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_5a38f_row16_col0\" class=\"data row16 col0\" >('medical', 'JJ')</td>\n",
       "      <td id=\"T_5a38f_row16_col1\" class=\"data row16 col1\" >medical</td>\n",
       "      <td id=\"T_5a38f_row16_col2\" class=\"data row16 col2\" >medic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_5a38f_row17_col0\" class=\"data row17 col0\" >('intervention', 'NN')</td>\n",
       "      <td id=\"T_5a38f_row17_col1\" class=\"data row17 col1\" >intervention</td>\n",
       "      <td id=\"T_5a38f_row17_col2\" class=\"data row17 col2\" >intervent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5a38f_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_5a38f_row18_col0\" class=\"data row18 col0\" >('reported', 'VBD')</td>\n",
       "      <td id=\"T_5a38f_row18_col1\" class=\"data row18 col1\" >report</td>\n",
       "      <td id=\"T_5a38f_row18_col2\" class=\"data row18 col2\" >report</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f58ee576610>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = []\n",
    "\n",
    "# Using itertuples(), the columns must be addressed using thier position.\n",
    "# Here's a map of position to name:\n",
    "# row[1]: ROW_ID\n",
    "# row[2]: FOI_TEXT\n",
    "# row[3]: DEVICE_PROBLEM_CODE\n",
    "# row[4]: DEVICE_PROBLEM_TEXT\n",
    "for row in df.itertuples():\n",
    "    for sentence in row[2].split(\".\"):\n",
    "        if sentence != \"\":\n",
    "            sentences.append([row[1], row[3], row[4], sentence])\n",
    "\n",
    "sentences_df = pd.DataFrame(\n",
    "    sentences,\n",
    "    columns=[\n",
    "        \"ROW_ID\",\n",
    "        \"DEVICE_PROBLEM_CODE\",\n",
    "        \"DEVICE_PROBLEM_TEXT\",\n",
    "        \"SENTENCIZED_FOI_TEXT\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "compare_lemma_stem_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62a27c3d-d6de-47cc-bca1-32e3a716f168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25765, 4)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f4a1a46-8619-400b-84f6-c4ed8003017e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ROW_ID</th>\n",
       "      <th>DEVICE_PROBLEM_CODE</th>\n",
       "      <th>DEVICE_PROBLEM_TEXT</th>\n",
       "      <th>SENTENCIZED_FOI_TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1969025</td>\n",
       "      <td>3283</td>\n",
       "      <td>Wireless Communication Problem</td>\n",
       "      <td>IT WAS REPORTED THAT THE TRANSMITTER LOST CONN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1969025</td>\n",
       "      <td>3283</td>\n",
       "      <td>Wireless Communication Problem</td>\n",
       "      <td>THE TRANSMITTER ULTIMATELY REGAINED CONNECTIO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1969025</td>\n",
       "      <td>3283</td>\n",
       "      <td>Wireless Communication Problem</td>\n",
       "      <td>NO ADDITIONAL PATIENT OR EVENT INFORMATION WA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ROW_ID DEVICE_PROBLEM_CODE             DEVICE_PROBLEM_TEXT  \\\n",
       "0  1969025                3283  Wireless Communication Problem   \n",
       "1  1969025                3283  Wireless Communication Problem   \n",
       "2  1969025                3283  Wireless Communication Problem   \n",
       "\n",
       "                                SENTENCIZED_FOI_TEXT  \n",
       "0  IT WAS REPORTED THAT THE TRANSMITTER LOST CONN...  \n",
       "1   THE TRANSMITTER ULTIMATELY REGAINED CONNECTIO...  \n",
       "2   NO ADDITIONAL PATIENT OR EVENT INFORMATION WA...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6aa46d7-6b15-4539-89c3-acc16c1c6c2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IT WAS REPORTED THAT THE TRANSMITTER LOST CONNECTION WITH THE PUMP FOR GREATER THAN 1 HOUR'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df[\"SENTENCIZED_FOI_TEXT\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cea1af87-cd7e-4400-bc31-fcc8f3fea5cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand Contractions, Tokenize, and Convert to Lowercase\n",
    "sentences_df[\"TOKENIZED_SENTENCES\"] = sentences_df[\"SENTENCIZED_FOI_TEXT\"].apply(\n",
    "    lambda x: [contractions.fix(word).lower() for word in x.split()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25d9b7de-2e44-4ecd-b63a-8ade98fe7a3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " 'was',\n",
       " 'reported',\n",
       " 'that',\n",
       " 'the',\n",
       " 'transmitter',\n",
       " 'lost',\n",
       " 'connection',\n",
       " 'with',\n",
       " 'the',\n",
       " 'pump',\n",
       " 'for',\n",
       " 'greater',\n",
       " 'than',\n",
       " '1',\n",
       " 'hour']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df[\"TOKENIZED_SENTENCES\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "daad689d-e1e4-48ee-aeaa-ef244a0773a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " 'was',\n",
       " 'reported',\n",
       " 'that',\n",
       " 'the',\n",
       " 'transmitter',\n",
       " 'lost',\n",
       " 'connection',\n",
       " 'with',\n",
       " 'the',\n",
       " 'pump',\n",
       " 'for',\n",
       " 'greater',\n",
       " 'than',\n",
       " '1',\n",
       " 'hour']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "sentences_df[\"NOPUNCT_SENTENCES\"] = sentences_df[\"TOKENIZED_SENTENCES\"].apply(\n",
    "    lambda x: [remove_punctuation(word) for word in x]\n",
    ")\n",
    "sentences_df[\"NOPUNCT_SENTENCES\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3f2f22a-6f55-4ea6-8382-e97993d5815c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reported',\n",
       " 'transmitter',\n",
       " 'lost',\n",
       " 'connection',\n",
       " 'pump',\n",
       " 'greater',\n",
       " '1',\n",
       " 'hour']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stop words\n",
    "sentences_df[\"NOSTOPWORDS_SENTENCES\"] = sentences_df[\"NOPUNCT_SENTENCES\"].apply(\n",
    "    lambda x: remove_stopwords(x)\n",
    ")\n",
    "sentences_df[\"NOSTOPWORDS_SENTENCES\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e96ddcd3-6711-4a56-ad2c-cd4bf5e84988",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reported', 'VBN'),\n",
       " ('transmitter', 'NN'),\n",
       " ('lost', 'VBN'),\n",
       " ('connection', 'NN'),\n",
       " ('pump', 'NN'),\n",
       " ('greater', 'JJR'),\n",
       " ('1', 'CD'),\n",
       " ('hour', 'NN')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply POS Tagging\n",
    "sentences_df[\"POS_SENTENCES\"] = sentences_df[\"NOSTOPWORDS_SENTENCES\"].apply(\n",
    "    nltk.pos_tag\n",
    ")\n",
    "sentences_df[\"POS_SENTENCES\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f36e88a-a2c6-4e68-98d9-ad4d9d8339ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to join tokens that have been lemmatized and stemmed\n",
    "def join_tokenized_sentence(tokens):\n",
    "    joined_words = []\n",
    "\n",
    "    for word in tokens:\n",
    "        joined_words.append(word)\n",
    "\n",
    "    # Join the stemmed words back into a sentence\n",
    "    return \" \".join(joined_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880f8066-5d31-4abf-996d-269eef7fd804",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 9.A Lemmatize Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "538fbb13-a592-4502-abb6-8cb05deeccc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['report', 'transmitter', 'lose', 'connection', 'pump', 'great', '1', 'hour']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df[\"TOKEN_LEMMATIZED_SENTENCES\"] = sentences_df[\"POS_SENTENCES\"].apply(\n",
    "    lemmatize_text\n",
    ")\n",
    "sentences_df[\"TOKEN_LEMMATIZED_SENTENCES\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "589301ef-dcce-434f-829b-23f4ca684dc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'report transmitter lose connection pump great 1 hour'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df[\"LEMMATIZED_SENTENCES\"] = sentences_df[\"TOKEN_LEMMATIZED_SENTENCES\"].apply(\n",
    "    join_tokenized_sentence\n",
    ")\n",
    "sentences_df[\"LEMMATIZED_SENTENCES\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa224d6e-72ca-4548-8091-82cdb05591d3",
   "metadata": {},
   "source": [
    "## 9.B Stem Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a8b31b8f-b03f-4db2-8f8f-ef2d89c4e450",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['report', 'transmitt', 'lost', 'connect', 'pump', 'greater', '1', 'hour']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a new column called 'STEMMED_SENTENCES'\n",
    "sentences_df[\"TOKEN_STEMMED_SENTENCES\"] = sentences_df[\"POS_SENTENCES\"].apply(\n",
    "    stem_words\n",
    ")\n",
    "sentences_df[\"TOKEN_STEMMED_SENTENCES\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "13731b1c-2b14-4295-8bc0-132c74d1bac4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'report transmitt lost connect pump greater 1 hour'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df[\"STEMMED_SENTENCES\"] = sentences_df[\"TOKEN_STEMMED_SENTENCES\"].apply(\n",
    "    join_tokenized_sentence\n",
    ")\n",
    "sentences_df[\"STEMMED_SENTENCES\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74f580b-740d-4631-9816-37ce859dde28",
   "metadata": {},
   "source": [
    "## Review the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "66c4f4aa-b714-40d5-a097-1b1a4ea51c5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_fa69e_ th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_fa69e_row0_col0, #T_fa69e_row0_col1, #T_fa69e_row1_col0, #T_fa69e_row1_col1, #T_fa69e_row2_col0, #T_fa69e_row2_col1, #T_fa69e_row3_col0, #T_fa69e_row3_col1, #T_fa69e_row4_col0, #T_fa69e_row4_col1, #T_fa69e_row5_col0, #T_fa69e_row5_col1, #T_fa69e_row6_col0, #T_fa69e_row6_col1, #T_fa69e_row7_col0, #T_fa69e_row7_col1, #T_fa69e_row8_col0, #T_fa69e_row8_col1, #T_fa69e_row9_col0, #T_fa69e_row9_col1, #T_fa69e_row10_col0, #T_fa69e_row10_col1, #T_fa69e_row11_col0, #T_fa69e_row11_col1, #T_fa69e_row12_col0, #T_fa69e_row12_col1, #T_fa69e_row13_col0, #T_fa69e_row13_col1, #T_fa69e_row14_col0, #T_fa69e_row14_col1, #T_fa69e_row15_col0, #T_fa69e_row15_col1, #T_fa69e_row16_col0, #T_fa69e_row16_col1, #T_fa69e_row17_col0, #T_fa69e_row17_col1, #T_fa69e_row18_col0, #T_fa69e_row18_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_fa69e_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >DF COLUMN NAMES</th>\n",
       "      <th class=\"col_heading level0 col1\" >EXAMPLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_fa69e_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_fa69e_row0_col0\" class=\"data row0 col0\" >ROW_ID</td>\n",
       "      <td id=\"T_fa69e_row0_col1\" class=\"data row0 col1\" >1969025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa69e_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_fa69e_row1_col0\" class=\"data row1 col0\" >FOI_TEXT</td>\n",
       "      <td id=\"T_fa69e_row1_col1\" class=\"data row1 col1\" >IT WAS REPORTED THAT THE TRANSMITTER LOST CONNECTION WITH THE PUMP FOR GREATER THAN 1 HOUR. THE TRANSMITTER ULTIMATELY REGAINED CONNECTION WITH THE PUMP. NO ADDITIONAL PATIENT OR EVENT INFORMATION WAS AVAILABLE.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa69e_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_fa69e_row2_col0\" class=\"data row2 col0\" >DEVICE_PROBLEM_CODE</td>\n",
       "      <td id=\"T_fa69e_row2_col1\" class=\"data row2 col1\" >3283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa69e_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_fa69e_row3_col0\" class=\"data row3 col0\" >DEVICE_PROBLEM_TEXT</td>\n",
       "      <td id=\"T_fa69e_row3_col1\" class=\"data row3 col1\" >Wireless Communication Problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa69e_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_fa69e_row4_col0\" class=\"data row4 col0\" >GENERIC_NAME</td>\n",
       "      <td id=\"T_fa69e_row4_col1\" class=\"data row4 col1\" >CONTINUOUS GLUCOSE MONITOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa69e_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_fa69e_row5_col0\" class=\"data row5 col0\" >DEVICE_REPORT_PRODUCT_CODE</td>\n",
       "      <td id=\"T_fa69e_row5_col1\" class=\"data row5 col1\" >QBJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa69e_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_fa69e_row6_col0\" class=\"data row6 col0\" >UDI-DI</td>\n",
       "      <td id=\"T_fa69e_row6_col1\" class=\"data row6 col1\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa69e_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_fa69e_row7_col0\" class=\"data row7 col0\" >UDI-PUBLIC</td>\n",
       "      <td id=\"T_fa69e_row7_col1\" class=\"data row7 col1\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa69e_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_fa69e_row8_col0\" class=\"data row8 col0\" >DATE_OF_EVENT</td>\n",
       "      <td id=\"T_fa69e_row8_col1\" class=\"data row8 col1\" >07/30/2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa69e_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_fa69e_row9_col0\" class=\"data row9 col0\" >REPORTER_OCCUPATION_CODE</td>\n",
       "      <td id=\"T_fa69e_row9_col1\" class=\"data row9 col1\" >000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa69e_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_fa69e_row10_col0\" class=\"data row10 col0\" >REPORT_DATE</td>\n",
       "      <td id=\"T_fa69e_row10_col1\" class=\"data row10 col1\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa69e_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_fa69e_row11_col0\" class=\"data row11 col0\" >EVENT_LOCATION</td>\n",
       "      <td id=\"T_fa69e_row11_col1\" class=\"data row11 col1\" >I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa69e_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_fa69e_row12_col0\" class=\"data row12 col0\" >SOURCE_TYPE</td>\n",
       "      <td id=\"T_fa69e_row12_col1\" class=\"data row12 col1\" >CONSUMER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa69e_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_fa69e_row13_col0\" class=\"data row13 col0\" >TOKENIZED_TEXT</td>\n",
       "      <td id=\"T_fa69e_row13_col1\" class=\"data row13 col1\" >['it', 'was', 'reported', 'that', 'the', 'transmitter', 'lost', 'connection', 'with', 'the', 'pump', 'for', 'greater', 'than', '1', 'hour.', 'the', 'transmitter', 'ultimately', 'regained', 'connection', 'with', 'the', 'pump.', 'no', 'additional', 'patient', 'or', 'event', 'information', 'was', 'available.']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa69e_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_fa69e_row14_col0\" class=\"data row14 col0\" >NOPUNCT_TEXT</td>\n",
       "      <td id=\"T_fa69e_row14_col1\" class=\"data row14 col1\" >['it', 'was', 'reported', 'that', 'the', 'transmitter', 'lost', 'connection', 'with', 'the', 'pump', 'for', 'greater', 'than', '1', 'hour', 'the', 'transmitter', 'ultimately', 'regained', 'connection', 'with', 'the', 'pump', 'no', 'additional', 'patient', 'or', 'event', 'information', 'was', 'available']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa69e_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_fa69e_row15_col0\" class=\"data row15 col0\" >NOSTOPWORDS_TEXT</td>\n",
       "      <td id=\"T_fa69e_row15_col1\" class=\"data row15 col1\" >['reported', 'transmitter', 'lost', 'connection', 'pump', 'greater', '1', 'hour', 'transmitter', 'ultimately', 'regained', 'connection', 'pump', 'additional', 'patient', 'event', 'information', 'available']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa69e_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_fa69e_row16_col0\" class=\"data row16 col0\" >POS_TEXT</td>\n",
       "      <td id=\"T_fa69e_row16_col1\" class=\"data row16 col1\" >[('reported', 'VBN'), ('transmitter', 'NN'), ('lost', 'VBN'), ('connection', 'NN'), ('pump', 'NN'), ('greater', 'JJR'), ('1', 'CD'), ('hour', 'NN'), ('transmitter', 'NN'), ('ultimately', 'RB'), ('regained', 'VBD'), ('connection', 'NN'), ('pump', 'NN'), ('additional', 'JJ'), ('patient', 'NN'), ('event', 'NN'), ('information', 'NN'), ('available', 'JJ')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa69e_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_fa69e_row17_col0\" class=\"data row17 col0\" >LEMMATIZED_TEXT</td>\n",
       "      <td id=\"T_fa69e_row17_col1\" class=\"data row17 col1\" >['report', 'transmitter', 'lose', 'connection', 'pump', 'great', '1', 'hour', 'transmitter', 'ultimately', 'regain', 'connection', 'pump', 'additional', 'patient', 'event', 'information', 'available']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fa69e_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_fa69e_row18_col0\" class=\"data row18 col0\" >STEMMED_TEXT</td>\n",
       "      <td id=\"T_fa69e_row18_col1\" class=\"data row18 col1\" >['report', 'transmitt', 'lost', 'connect', 'pump', 'greater', '1', 'hour', 'transmitt', 'ultim', 'regain', 'connect', 'pump', 'addit', 'patient', 'event', 'inform', 'avail']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f58e0682e90>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new dataframe with just one row containing the column names\n",
    "column_names_df = pd.DataFrame(\n",
    "    {\n",
    "        \"DF COLUMN NAMES\": df.columns,\n",
    "    }\n",
    ")\n",
    "\n",
    "example = []\n",
    "\n",
    "for col in df.columns:\n",
    "    example.append(df[col][0])\n",
    "\n",
    "column_names_df[\"EXAMPLE\"] = example\n",
    "\n",
    "# Format output so that the column headers and data are aligned to the left\n",
    "column_names_df = column_names_df.style.set_properties(**{\"text-align\": \"left\"})\n",
    "column_names_df = column_names_df.set_table_styles(\n",
    "    [dict(selector=\"th\", props=[(\"text-align\", \"left\")])]\n",
    ")\n",
    "column_names_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "95aba4e3-0454-46c6-a73a-f4f644d567d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_7332a_ th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_7332a_row0_col0, #T_7332a_row0_col1, #T_7332a_row1_col0, #T_7332a_row1_col1, #T_7332a_row2_col0, #T_7332a_row2_col1, #T_7332a_row3_col0, #T_7332a_row3_col1, #T_7332a_row4_col0, #T_7332a_row4_col1, #T_7332a_row5_col0, #T_7332a_row5_col1, #T_7332a_row6_col0, #T_7332a_row6_col1, #T_7332a_row7_col0, #T_7332a_row7_col1, #T_7332a_row8_col0, #T_7332a_row8_col1, #T_7332a_row9_col0, #T_7332a_row9_col1, #T_7332a_row10_col0, #T_7332a_row10_col1, #T_7332a_row11_col0, #T_7332a_row11_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_7332a_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th class=\"col_heading level0 col0\" >SENTENCES DF COLUMN NAMES</th>\n",
       "      <th class=\"col_heading level0 col1\" >EXAMPLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7332a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_7332a_row0_col0\" class=\"data row0 col0\" >ROW_ID</td>\n",
       "      <td id=\"T_7332a_row0_col1\" class=\"data row0 col1\" >1969025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7332a_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_7332a_row1_col0\" class=\"data row1 col0\" >DEVICE_PROBLEM_CODE</td>\n",
       "      <td id=\"T_7332a_row1_col1\" class=\"data row1 col1\" >3283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7332a_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_7332a_row2_col0\" class=\"data row2 col0\" >DEVICE_PROBLEM_TEXT</td>\n",
       "      <td id=\"T_7332a_row2_col1\" class=\"data row2 col1\" >Wireless Communication Problem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7332a_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_7332a_row3_col0\" class=\"data row3 col0\" >SENTENCIZED_FOI_TEXT</td>\n",
       "      <td id=\"T_7332a_row3_col1\" class=\"data row3 col1\" >IT WAS REPORTED THAT THE TRANSMITTER LOST CONNECTION WITH THE PUMP FOR GREATER THAN 1 HOUR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7332a_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_7332a_row4_col0\" class=\"data row4 col0\" >TOKENIZED_SENTENCES</td>\n",
       "      <td id=\"T_7332a_row4_col1\" class=\"data row4 col1\" >['it', 'was', 'reported', 'that', 'the', 'transmitter', 'lost', 'connection', 'with', 'the', 'pump', 'for', 'greater', 'than', '1', 'hour']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7332a_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_7332a_row5_col0\" class=\"data row5 col0\" >NOPUNCT_SENTENCES</td>\n",
       "      <td id=\"T_7332a_row5_col1\" class=\"data row5 col1\" >['it', 'was', 'reported', 'that', 'the', 'transmitter', 'lost', 'connection', 'with', 'the', 'pump', 'for', 'greater', 'than', '1', 'hour']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7332a_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_7332a_row6_col0\" class=\"data row6 col0\" >NOSTOPWORDS_SENTENCES</td>\n",
       "      <td id=\"T_7332a_row6_col1\" class=\"data row6 col1\" >['reported', 'transmitter', 'lost', 'connection', 'pump', 'greater', '1', 'hour']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7332a_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_7332a_row7_col0\" class=\"data row7 col0\" >POS_SENTENCES</td>\n",
       "      <td id=\"T_7332a_row7_col1\" class=\"data row7 col1\" >[('reported', 'VBN'), ('transmitter', 'NN'), ('lost', 'VBN'), ('connection', 'NN'), ('pump', 'NN'), ('greater', 'JJR'), ('1', 'CD'), ('hour', 'NN')]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7332a_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_7332a_row8_col0\" class=\"data row8 col0\" >TOKEN_LEMMATIZED_SENTENCES</td>\n",
       "      <td id=\"T_7332a_row8_col1\" class=\"data row8 col1\" >['report', 'transmitter', 'lose', 'connection', 'pump', 'great', '1', 'hour']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7332a_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_7332a_row9_col0\" class=\"data row9 col0\" >LEMMATIZED_SENTENCES</td>\n",
       "      <td id=\"T_7332a_row9_col1\" class=\"data row9 col1\" >report transmitter lose connection pump great 1 hour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7332a_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_7332a_row10_col0\" class=\"data row10 col0\" >TOKEN_STEMMED_SENTENCES</td>\n",
       "      <td id=\"T_7332a_row10_col1\" class=\"data row10 col1\" >['report', 'transmitt', 'lost', 'connect', 'pump', 'greater', '1', 'hour']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7332a_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_7332a_row11_col0\" class=\"data row11 col0\" >STEMMED_SENTENCES</td>\n",
       "      <td id=\"T_7332a_row11_col1\" class=\"data row11 col1\" >report transmitt lost connect pump greater 1 hour</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f58e0690c90>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new dataframe with just one row containing the column names\n",
    "column_names_df = pd.DataFrame(\n",
    "    {\n",
    "        \"SENTENCES DF COLUMN NAMES\": sentences_df.columns,\n",
    "    }\n",
    ")\n",
    "\n",
    "example = []\n",
    "\n",
    "for col in sentences_df.columns:\n",
    "    example.append(sentences_df[col][0])\n",
    "\n",
    "column_names_df[\"EXAMPLE\"] = example\n",
    "\n",
    "# Format output so that the column headers and data are aligned to the left\n",
    "column_names_df = column_names_df.style.set_properties(**{\"text-align\": \"left\"})\n",
    "column_names_df = column_names_df.set_table_styles(\n",
    "    [dict(selector=\"th\", props=[(\"text-align\", \"left\")])]\n",
    ")\n",
    "column_names_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13143c09-f9cd-4c01-b7b7-5e3994aa21e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save the preproecssed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "98e853ae-ef27-4d92-a71a-b09efd9f101b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv(f\"{working_directory}/preprocessed_data.csv\", index=False)\n",
    "\n",
    "bow_df.to_csv(f\"{working_directory}/bag_of_words_data.csv\", index=False)\n",
    "\n",
    "tfidf_df.to_csv(f\"{working_directory}/tfidf_data.csv\", index=False)\n",
    "\n",
    "sentences_df.to_csv(f\"{working_directory}/sentences_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc2654d-14e8-4709-8289-9922127f9f88",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Upload All Output to an S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b03bb42-336b-406d-84d2-dbda67a93f9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Create the upload command using the AWS command line interface\n",
    "command = [\n",
    "    \"aws\",\n",
    "    \"s3\",\n",
    "    \"sync\",\n",
    "    working_directory,\n",
    "    f\"s3://praxis-2023-html-output\",\n",
    "    \"--exclude\",\n",
    "    f\"*/.ipynb_checkpoints/*\",\n",
    "    \"--no-progress\",\n",
    "]\n",
    "\n",
    "# Run the command and wait for it to complete\n",
    "output = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "# Print the command's output\n",
    "print(output.stdout)\n",
    "print(\"fin\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p37",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
